<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="googleb849f8ce9353f945.html" />













  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="欢迎戳进" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="一些摘要paper的笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="Summarization">
<meta property="og:url" content="http://yoursite.com/2018/10/03/Summarization/index.html">
<meta property="og:site_name" content="唐相儒的博客">
<meta property="og:description" content="一些摘要paper的笔记">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-47ce21ac6d194d9639a246b6fe963ba1_hd.jpg">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2017/11/7/654216544ab85a143fefd2eab6363135?imageView2/0/w/1280/h/960/ignore-error/1">
<meta property="og:image" content="http://oqnrd919g.bkt.clouddn.com/18-10-11/66945790.jpg">
<meta property="og:image" content="http://oqnrd919g.bkt.clouddn.com/18-10-11/27868842.jpg">
<meta property="og:updated_time" content="2018-10-12T00:42:32.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Summarization">
<meta name="twitter:description" content="一些摘要paper的笔记">
<meta name="twitter:image" content="https://pic3.zhimg.com/80/v2-47ce21ac6d194d9639a246b6fe963ba1_hd.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/03/Summarization/"/>





  <title> Summarization | 唐相儒的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">唐相儒的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/首页" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-ban"></i> <br />
            
            博客首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-hand-peace-o"></i> <br />
            
            文章分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/目录" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            文章归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/标签" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sign-language"></i> <br />
            
            文章标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-map-o"></i> <br />
            
            导航
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-asl-interpreting"></i> <br />
            
            team
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/03/Summarization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="唐相儒">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ooo.0o0.ooo/2017/07/01/59575381605d5.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="唐相儒的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Summarization
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-03T14:54:15+08:00">
                2018-10-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/03/Summarization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/10/03/Summarization/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>一些摘要paper的笔记<br><a id="more"></a></p>
<h1 id="start"><a href="#start" class="headerlink" title="start"></a>start</h1><h2 id="Sequence-to-Sequence-Learning-with-Neural-Networks"><a href="#Sequence-to-Sequence-Learning-with-Neural-Networks" class="headerlink" title="Sequence to Sequence Learning with Neural Networks"></a>Sequence to Sequence Learning with Neural Networks</h2><p>1.使用4层LSTM，每层1000个单元–&gt;长句表现好</p>
<p>2.第一个在encode将输入句子用lstm编码成一个固定的维度的向量(cnn无序列化顺序)</p>
<p>3.将input sentence逆序输入可以明显改善LSTM模型，说是减小“minimal time lag”</p>
<h2 id="RNN-based-Encoder-decoder-Approach-with-Word-Frequency-Estimation"><a href="#RNN-based-Encoder-decoder-Approach-with-Word-Frequency-Estimation" class="headerlink" title="RNN-based Encoder-decoder Approach with Word Frequency Estimation"></a>RNN-based Encoder-decoder Approach with Word Frequency Estimation</h2><p>多余的重复生成%</p>
<h2 id="Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"><a href="#Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate" class="headerlink" title="Neural Machine Translation by Jointly Learning to Align and Translate"></a>Neural Machine Translation by Jointly Learning to Align and Translate</h2><h2 id="Empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling"><a href="#Empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling" class="headerlink" title="Empirical evaluation of gated recurrent neural networks on sequence modeling"></a>Empirical evaluation of gated recurrent neural networks on sequence modeling</h2><h2 id="Sequence-to-sequence-rnns-for-text-summarization"><a href="#Sequence-to-sequence-rnns-for-text-summarization" class="headerlink" title="Sequence-to-sequence rnns for text summarization"></a>Sequence-to-sequence rnns for text summarization</h2><h2 id="A-neural-attention-model-for-abstractive-sentence-summarization"><a href="#A-neural-attention-model-for-abstractive-sentence-summarization" class="headerlink" title="A neural attention model for abstractive sentence summarization"></a>A neural attention model for abstractive sentence summarization</h2><h2 id="Neural-headline-generation-on-abstract-meaning-representation"><a href="#Neural-headline-generation-on-abstract-meaning-representation" class="headerlink" title="Neural headline generation on abstract meaning representation"></a>Neural headline generation on abstract meaning representation</h2><h2 id="Pointer-networks"><a href="#Pointer-networks" class="headerlink" title="Pointer networks"></a>Pointer networks</h2><p>是attention (decode生成y时，需要计算X1到Xn对生成y的贡献) 的一个变体</p>
<p>之前的翻译模型无法解决某类问题：在decode时，若source sentence长度是变化的(增多)，则target classes也是变化的（增多)。也可以描述为seq2seq输出严重依赖输入</p>
<p>而Ptr-Nets在输入序列中就挑出一个member作为输出，避免了这个问题。不像attetion将source通过encoder变成context vector，而是将attention转化为一个pointer，来选择原来输入序列中的元素</p>
<p>经典的attention：<br><img src="https://pic3.zhimg.com/80/v2-47ce21ac6d194d9639a246b6fe963ba1_hd.jpg" alt="经典的attention"></p>
<p>Ptr-Net中的attention：<br><img src="https://user-gold-cdn.xitu.io/2017/11/7/654216544ab85a143fefd2eab6363135?imageView2/0/w/1280/h/960/ignore-error/1" alt=""><br>Pointer Net没有上面传统attention的最后一个公式(将权重关系a和隐式状态整合为context vector)，而是直接进行通过softmax，指向输入序列选择中最有可能是输出的元素。</p>
<p>然后使用得到的softmax结果去拷贝encoder对应的输入元素作为的decoder输入的向量。</p>
<p>（本身是解决旅行商、凸包等，nips2015），显而易见，如果是抽取式摘要直接sentence labeling就行，如果是生成式，就是下面的pointer generator这篇，这种混合的模型能够从原文中直接复制词语，因而可以提高摘要的准确率并处理OOV词语，同时还保留下了生成新词语的能力</p>
<p>总而言之，Ptr-Net解决了seq2seq模型中output dictionary大小固定的问题，并且是一种用attention把copy机制带入生成模型中的方法</p>
<h2 id="copynet"><a href="#copynet" class="headerlink" title="copynet"></a>copynet</h2><p>copy机制是对于seq2seq(attention)的decoder时候的，具体来说有两点改变：1.decoder的状态更新会加一个位置信息进去，因为copy的时候需要知道那个词在source里面的位置(hidden state)，当然如果不在source里面，位置信息=0</p>
<p>2.另一个是，输出是个generate-code和copy-mode(从input sentence 中选词)的混合模型,即最终词的概率p是两种算p的方法的和，分别对应两种处理表示source sentence的向量来算p的方法：attentive read 和 selective read。</p>
<p>attentive read 和 selective read都是根据词在不同的集合(vocab或者source sentence的交、补集)有不同的打分函数，其中有参数需要学习。比如target不在source里面p(c)就是0，若target仅在source里面p(g)就是0<br><img src="http://oqnrd919g.bkt.clouddn.com/18-10-11/66945790.jpg" alt=""></p>
<h2 id="Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><a href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks" class="headerlink" title="Get To The Point: Summarization with Pointer-Generator Networks"></a>Get To The Point: Summarization with Pointer-Generator Networks</h2><p>解决三个问题：：产生不准确的事实类细节、生成重复的词，以及不能很好的处理OOV</p>
<p>既可以通过pointing直接从原文中copy单词又保留了通过generator生成新词的能力；另外，用coverage来记录已经总结出的内容，防止重复</p>
<p>即是模型=pointergenerator+coverage=seq2seq+attention+copy+coverage</p>
<ol>
<li>Sequence-to-sequence attentional model</li>
</ol>
<p>biLSTM作为encoder，decoder hidden state被用来计算注意力分布(source中的词的概率分布),用来计算hidden state的加权和作为新的t时间的context vector，和decoder state 过两个线性变化之后softmax来计算target 词的概率分布</p>
<ol>
<li>pointer-generator=Sequence-to-sequence attentional model+copy</li>
</ol>
<p><img src="http://oqnrd919g.bkt.clouddn.com/18-10-11/27868842.jpg" alt=""></p>
<p>重要的是，每一步还计算了一个概率值P_gen，这代表从词表中生成单词的概率。这个P_gen用来给seq2seq 模型softmax后的结果和Ptr-Net产生的来自source词的概率分布进行加权求和</p>
<p>两点说明，一个是，这里copy-mode是直接从attention权重计算的词的分布中去采样。另一个是，感觉p_gen是一种soft的形式，另外它计算方法是(h_t*是source向量+s_t是这一步的decoder state+xt是这一步的decoder的输入就是grandtruth)再sigmoid</p>
<ol>
<li>coverage</li>
</ol>
<p>这个是为了解决重复输出的问题，在decoder的每一步，更新一个coverage vector，是之前所有time step的decoder时attention权重的直接累加，可以来记录model已经关注过source中哪些词（也就是目前的生成已经覆盖到哪些词），并让这个向量影响attention权重的计算</p>
<p>同时在算loss的时候，摘要任务里要加一个coverage loss惩罚关注同一位置(相同的coverage)才行</p>
<p>疑问：为什么在Get To The Point看结果里面extractive方法还比pointer-generator-copy的rouge得分更高？<br>answer：新闻类重要的信息就是集中在前几句</p>
<h2 id="Neural-summarization-by-extracting-sentences-and-words"><a href="#Neural-summarization-by-extracting-sentences-and-words" class="headerlink" title="Neural summarization by extracting sentences and words"></a>Neural summarization by extracting sentences and words</h2><h2 id="SummaRuNNer-A-Recurrent-Neural-Network-Based-Sequence-Model-for-Extractive-Summarization-of-Documents"><a href="#SummaRuNNer-A-Recurrent-Neural-Network-Based-Sequence-Model-for-Extractive-Summarization-of-Documents" class="headerlink" title="SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents"></a>SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents</h2><h2 id="A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization"><a href="#A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization" class="headerlink" title="A Neural Attention Model for Abstractive Sentence Summarization"></a>A Neural Attention Model for Abstractive Sentence Summarization</h2><h2 id="Selective-Encoding-for-Abstractive-Sentence-Summarization"><a href="#Selective-Encoding-for-Abstractive-Sentence-Summarization" class="headerlink" title="Selective Encoding for Abstractive Sentence Summarization"></a>Selective Encoding for Abstractive Sentence Summarization</h2><h2 id="SummaRuNNer-A-Recurrent-Neural-Network-Based-Sequence-Model-for-Extractive-Summarization-of-Documents-1"><a href="#SummaRuNNer-A-Recurrent-Neural-Network-Based-Sequence-Model-for-Extractive-Summarization-of-Documents-1" class="headerlink" title="SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents"></a>SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents</h2><h2 id="Ranking-Sentences-for-Extractive-Summarization-with-Reinforcement-Learning"><a href="#Ranking-Sentences-for-Extractive-Summarization-with-Reinforcement-Learning" class="headerlink" title="Ranking Sentences for Extractive Summarization with Reinforcement Learning"></a>Ranking Sentences for Extractive Summarization with Reinforcement Learning</h2><h2 id="Get-To-The-Point-Summarization-with-Pointer-Generator-Networks-1"><a href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks-1" class="headerlink" title="Get To The Point: Summarization with Pointer-Generator Networks"></a>Get To The Point: Summarization with Pointer-Generator Networks</h2><h2 id="A-Reinforced-Topic-Aware-Convolutional-Sequence-to-Sequence-Model-for-Abstractive-Text-Summarization"><a href="#A-Reinforced-Topic-Aware-Convolutional-Sequence-to-Sequence-Model-for-Abstractive-Text-Summarization" class="headerlink" title="A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization"></a>A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization</h2><h2 id="A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss"><a href="#A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss" class="headerlink" title="A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss"></a>A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</h2><h2 id="Fast-Abstractive-Summarization-with-Reinforce-Selected-Sentence-Rewriting"><a href="#Fast-Abstractive-Summarization-with-Reinforce-Selected-Sentence-Rewriting" class="headerlink" title="Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"></a>Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</h2><h2 id="Neural-Document-Summarization-by-Jointly-Lea"><a href="#Neural-Document-Summarization-by-Jointly-Lea" class="headerlink" title="Neural Document Summarization by Jointly Lea"></a>Neural Document Summarization by Jointly Lea</h2>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/02/download-scp/" rel="next" title="download-scp">
                <i class="fa fa-chevron-left"></i> download-scp
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://ooo.0o0.ooo/2017/07/01/59575381605d5.png"
               alt="唐相儒" />
          <p class="site-author-name" itemprop="name">唐相儒</p>
           
              <p class="site-description motion-element" itemprop="description">Never Let Your Fear Decide Your Future</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/目录">
                <span class="site-state-item-count">68</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#start"><span class="nav-number">1.</span> <span class="nav-text">start</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequence-to-Sequence-Learning-with-Neural-Networks"><span class="nav-number">1.1.</span> <span class="nav-text">Sequence to Sequence Learning with Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-based-Encoder-decoder-Approach-with-Word-Frequency-Estimation"><span class="nav-number">1.2.</span> <span class="nav-text">RNN-based Encoder-decoder Approach with Word Frequency Estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"><span class="nav-number">1.3.</span> <span class="nav-text">Neural Machine Translation by Jointly Learning to Align and Translate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling"><span class="nav-number">1.4.</span> <span class="nav-text">Empirical evaluation of gated recurrent neural networks on sequence modeling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequence-to-sequence-rnns-for-text-summarization"><span class="nav-number">1.5.</span> <span class="nav-text">Sequence-to-sequence rnns for text summarization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-neural-attention-model-for-abstractive-sentence-summarization"><span class="nav-number">1.6.</span> <span class="nav-text">A neural attention model for abstractive sentence summarization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-headline-generation-on-abstract-meaning-representation"><span class="nav-number">1.7.</span> <span class="nav-text">Neural headline generation on abstract meaning representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pointer-networks"><span class="nav-number">1.8.</span> <span class="nav-text">Pointer networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#copynet"><span class="nav-number">1.9.</span> <span class="nav-text">copynet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><span class="nav-number">1.10.</span> <span class="nav-text">Get To The Point: Summarization with Pointer-Generator Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-summarization-by-extracting-sentences-and-words"><span class="nav-number">1.11.</span> <span class="nav-text">Neural summarization by extracting sentences and words</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SummaRuNNer-A-Recurrent-Neural-Network-Based-Sequence-Model-for-Extractive-Summarization-of-Documents"><span class="nav-number">1.12.</span> <span class="nav-text">SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization"><span class="nav-number">1.13.</span> <span class="nav-text">A Neural Attention Model for Abstractive Sentence Summarization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Selective-Encoding-for-Abstractive-Sentence-Summarization"><span class="nav-number">1.14.</span> <span class="nav-text">Selective Encoding for Abstractive Sentence Summarization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SummaRuNNer-A-Recurrent-Neural-Network-Based-Sequence-Model-for-Extractive-Summarization-of-Documents-1"><span class="nav-number">1.15.</span> <span class="nav-text">SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ranking-Sentences-for-Extractive-Summarization-with-Reinforcement-Learning"><span class="nav-number">1.16.</span> <span class="nav-text">Ranking Sentences for Extractive Summarization with Reinforcement Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks-1"><span class="nav-number">1.17.</span> <span class="nav-text">Get To The Point: Summarization with Pointer-Generator Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Reinforced-Topic-Aware-Convolutional-Sequence-to-Sequence-Model-for-Abstractive-Text-Summarization"><span class="nav-number">1.18.</span> <span class="nav-text">A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss"><span class="nav-number">1.19.</span> <span class="nav-text">A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-Abstractive-Summarization-with-Reinforce-Selected-Sentence-Rewriting"><span class="nav-number">1.20.</span> <span class="nav-text">Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Document-Summarization-by-Jointly-Lea"><span class="nav-number">1.21.</span> <span class="nav-text">Neural Document Summarization by Jointly Lea</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">唐相儒</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/10/03/Summarization/';
          this.page.identifier = '2018/10/03/Summarization/';
          this.page.title = 'Summarization';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  





  






  





  

  

  

  





</body>
</html>
