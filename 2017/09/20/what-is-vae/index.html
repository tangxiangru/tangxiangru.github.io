<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="googleb849f8ce9353f945.html" />













  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="欢迎戳进" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="vae的实现">
<meta property="og:type" content="article">
<meta property="og:title" content="what is vae">
<meta property="og:url" content="http://yoursite.com/2017/09/20/what-is-vae/index.html">
<meta property="og:site_name" content="唐相儒的博客">
<meta property="og:description" content="vae的实现">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=p_%5Ctheta+%28h%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=p_%5Ctheta%28x%7Ch%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5C%7Bx_i%5C%7D_%7Bi%3D1%7D%5En">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Ctheta">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=l%28%5Ctheta%29%3D%5Csum_%7Bk%3D1%7D%5En%5Cmathbb%7BE%7D_%7Bp_%7B%5Ctheta_i%7D%28h%7Cx_k%29%7D%5B%5Clog+p_%5Ctheta%28x_k%2Ch%29%5D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%2B1%7D%3D%5Carg+%5Cmax+l%28%5Ctheta_i%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=l%28%5Ctheta%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Clog+p%28x%29%3D%5Clog+%5Cint+p%28h%2Cx%29dh%3D%5Clog+%5Cint+q%28h%7Cx%29%5Cfrac%7Bp%28h%2Cx%29%7D%7Bq%28h%7Cx%29%7Ddh+%5Cgeq+%5Cint+q%28h%7Cx%29%5B%5Clog+p%28h%2Cx%29-%5Clog+q%28h%7Cx%29%5Ddh%3DB%28q%2Cx%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5C%7Bq_%5Cphi%28h%7Cx%29%5C%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=B%28q_%5Cphi%2Cx%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cnabla_%5Cphi+B%28q_%5Cphi%2Cx%29%3D%5Cint+%5B%5Clog+p_%5Ctheta%28h%2Cx%29-%5Clog+q_%5Cphi%28h%7Cx%29%5D+%5Cnabla_%5Cphi+%5Clog+q_%5Cphi%28h%7Cx%29+q_%5Cphi%28h%7Cx%29dh">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=B%28q%2Cx%29%3D%5Cint+q%28h%7Cx%29%5Clog+p%28x%7Ch%29dh-D%28q%28h%7Cx%29%7C%7Cp%28h%29%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=f_%5Cphi%28x%2C%5Cepsilon%29%5Csim+q_%5Cphi%28h%7Cx%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cepsilon">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=f_%5Cphi">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cint+g%28h%29+q_%5Cphi%28h%2Cx%29+dh%3D%5Cmathbb%7BE%7D_%5Cepsilon%5Bg%28f_%5Cphi%28x%2C%5Cepsilon%29%29%5D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cnabla_%5Cphi%5Cint+g%28h%29+q_%5Cphi%28h%7Cx%29+dh%3D%5Cmathbb%7BE%7D_%5Cepsilon%5B%5Cnabla+g%28f_%5Cphi%28x%2C%5Cepsilon%29%29%5D%3D%5Cmathbb%7BE%7D_%5Cepsilon%5Bg%27%28f%28x%2C%5Cepsilon%29%29%5Ccdot%5Cnabla_%5Cphi+f%28x%2C%5Cepsilon%29%5D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cepsilon+">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=f%28x%2C%5Cepsilon%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cphi">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=f%28x%2C%5Cepsilon%29%3Dm_%5Cphi%28x%29%2B%5Cepsilon%5Ccdot+V_%5Cphi%28x%29%2C+%5Cepsilon%5Csim+N%280%2C1%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cnabla_%5Cphi+f%28x%2C%5Cepsilon%29">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Ctheta">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cphi">
<meta property="og:updated_time" content="2017-09-26T16:18:12.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="what is vae">
<meta name="twitter:description" content="vae的实现">
<meta name="twitter:image" content="http://www.zhihu.com/equation?tex=p_%5Ctheta+%28h%29">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/20/what-is-vae/"/>





  <title> what is vae | 唐相儒的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">唐相儒的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/首页" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-ban"></i> <br />
            
            博客首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-hand-peace-o"></i> <br />
            
            文章分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/目录" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            文章归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/标签" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sign-language"></i> <br />
            
            文章标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-map-o"></i> <br />
            
            导航
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-asl-interpreting"></i> <br />
            
            team
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/20/what-is-vae/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="唐相儒">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ooo.0o0.ooo/2017/07/01/59575381605d5.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="唐相儒的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                what is vae
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-20T23:17:29+08:00">
                2017-09-20
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/09/20/what-is-vae/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/09/20/what-is-vae/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW/blob/master/vae.py" target="_blank" rel="external">vae的实现</a></p>
<a id="more"></a>
<p>papers：<br>《Generating Sentences From a Continuous Spaces》. ICLR 2016</p>
<p>《Neural Variational Inference for Text Processing》. ICML 2016</p>
<p>《Language as a Latent Variable: Discrete Generative Models for Sentence Compression》. EMNLP 2016</p>
<p>《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》. AAAI 2017</p>
<p>1) VAE 可以看做是 Standard autoencoder 的 regularized version（在 autoencoder 的架构上引入随机 latent variable）<br>2) VAE 从 data 学到的是在 latent space 的 region，而不是单个点。换句话说是 encode 学到了一个概率分布 q(z|x)<br>3) 引入 KL divergence 让后验 q(z|x)接近先验 p(z)。这里的 motivation 在于如果仅用 reconstruction loss，q(z|x)的 variances 还是会很小（又和原有的单个点差不多了）<br>要掌握变分推断和理解 reparametrization trick 。</p>
<p>VAE是Max Welling 和他的学生D. Kingma 提出的一种Generative Model.</p>
<p>VAE本质上不是一个deep generative model. 因为它实质上只是学习一个两层的graphical model. h 是一组 latent variables, 我们有一个概率模型<img src="http://www.zhihu.com/equation?tex=p_%5Ctheta+%28h%29" alt="">和 <img src="http://www.zhihu.com/equation?tex=p_%5Ctheta%28x%7Ch%29" alt="">, 我们希望能够从一组unlabelled data <img src="http://www.zhihu.com/equation?tex=%5C%7Bx_i%5C%7D_%7Bi%3D1%7D%5En" alt="">里面学习出<img src="http://www.zhihu.com/equation?tex=%5Ctheta" alt="">.</p>
<p>这个问题看起来很简单，其实是graphical model特别是latent variable model里面的一个老大难问题，也非常基本。第一次接触这类问题的话，可能会惊讶于machine learning学界连这么简单的一个二层的latent variable model都训练不出来，也反映出概率计算到底是多么地困难。</p>
<p>如果受过一些训练，就会想到EM算法, 不断地做两件事:</p>
<p>(1). 计算<img src="http://www.zhihu.com/equation?tex=l%28%5Ctheta%29%3D%5Csum_%7Bk%3D1%7D%5En%5Cmathbb%7BE%7D_%7Bp_%7B%5Ctheta_i%7D%28h%7Cx_k%29%7D%5B%5Clog+p_%5Ctheta%28x_k%2Ch%29%5D" alt=""></p>
<p>(2) <img src="http://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%2B1%7D%3D%5Carg+%5Cmax+l%28%5Ctheta_i%29" alt=""></p>
<p>但这个算法当然要求很高，必须l 是可计算的，换句话说 p(h|x) 形式必须简单。一般来说h只取有限个离散值才可以这么干(clustering)。过去20年unsupervised learning的主要发展就是clustering，但clustering是不足以解决AI问题的。</p>
<p>当然也可以做Monte Carlo EM, 也就是用MCMC sampling去逼近<img src="http://www.zhihu.com/equation?tex=l%28%5Ctheta%29" alt="">. 效果据说很差，因为每个data point 在每个update上都得sample一次。但现在计算资源如此丰富，以后也许不是问题</p>
<p>现在我们假设h 是连续随机变量(这是vae的核心假设)。目标是maximize log-likelihood. 一个很自然的想法来自 Variational inference. 首先我们有variational bound:<img src="http://www.zhihu.com/equation?tex=%5Clog+p%28x%29%3D%5Clog+%5Cint+p%28h%2Cx%29dh%3D%5Clog+%5Cint+q%28h%7Cx%29%5Cfrac%7Bp%28h%2Cx%29%7D%7Bq%28h%7Cx%29%7Ddh+%5Cgeq+%5Cint+q%28h%7Cx%29%5B%5Clog+p%28h%2Cx%29-%5Clog+q%28h%7Cx%29%5Ddh%3DB%28q%2Cx%29" alt=""></p>
<p>Variational Inference 的想法是， 对于q(h|x),我们可以取一些简单的distribution family <img src="http://www.zhihu.com/equation?tex=%5C%7Bq_%5Cphi%28h%7Cx%29%5C%7D" alt=""> 进行逼近，进而最大化<img src="http://www.zhihu.com/equation?tex=B%28q_%5Cphi%2Cx%29" alt="">, 对于每一个数据点x. 比如最常见的mean field VI，就假设q 是factorized 的分布。如果p 再比较简单，就可以解析地解出B,然后做最优化.但有时候p复杂，积分还是求不出来，那就只有再靠sampling了。</p>
<p><img src="http://www.zhihu.com/equation?tex=%5Cnabla_%5Cphi+B%28q_%5Cphi%2Cx%29%3D%5Cint+%5B%5Clog+p_%5Ctheta%28h%2Cx%29-%5Clog+q_%5Cphi%28h%7Cx%29%5D+%5Cnabla_%5Cphi+%5Clog+q_%5Cphi%28h%7Cx%29+q_%5Cphi%28h%7Cx%29dh" alt=""><br>上面这个gradient就可以用sampling 逼近了。 这个trick基本上起源于REINFORCE algorithm.但悲剧的是，这个estimator的variance相当大。所以并不实用。2012年Jordan的学生在这个式子的基础上加了一点variance reduction，然后就发了一篇ICML。</p>
<p>这里为啥不用variational inference呢？因为VI的distribution太局限了。一般都是mean field的，这样误差一般都会很大。假如h有很复杂的结构，给定x, 我们希望我们的逼近能capture h之间复杂的correlation，不希望简简单单假设他们是独立的。</p>
<p>Variational bound也可以写成</p>
<p><img src="http://www.zhihu.com/equation?tex=B%28q%2Cx%29%3D%5Cint+q%28h%7Cx%29%5Clog+p%28x%7Ch%29dh-D%28q%28h%7Cx%29%7C%7Cp%28h%29%29" alt=""></p>
<p>第一项是可以看做是reconstruction error. 这就是为啥被称为VAE.</p>
<p>先验概率p(h)往往非常简单，简单到第二项有时候可以算出来。 如果可以算出来，它的gradient estimator的variance就会进一步减小。</p>
<p>VAE做的事情是不对q<br>做非常简单的假设。这样我们能够capture的distribution就复杂得多。</p>
<p>而假设我们有随机变量满足分布<img src="http://www.zhihu.com/equation?tex=f_%5Cphi%28x%2C%5Cepsilon%29%5Csim+q_%5Cphi%28h%7Cx%29" alt="">, <img src="http://www.zhihu.com/equation?tex=%5Cepsilon" alt="">是一个random seed，<img src="http://www.zhihu.com/equation?tex=f_%5Cphi" alt="">是一个被参数化的光滑函数族，比如一个DNN. 为什么做这个变量代换呢？是因为上面的那个estimator的variance大的原因，是因为它需要sample from q，然后利用q的sample处的gradient修正q，这样两次产生的误差会叠加。但做了这个reparametrization，我们只需要sample一个分布非常稳定的random seed的distribution，比如N(0,1)所以noise小得多。</p>
<p>应用这个的trick,我们有对于任意的函数g, <img src="http://www.zhihu.com/equation?tex=%5Cint+g%28h%29+q_%5Cphi%28h%2Cx%29+dh%3D%5Cmathbb%7BE%7D_%5Cepsilon%5Bg%28f_%5Cphi%28x%2C%5Cepsilon%29%29%5D" alt="">. 所以我们求这个东西的gradient，就有<img src="http://www.zhihu.com/equation?tex=%5Cnabla_%5Cphi%5Cint+g%28h%29+q_%5Cphi%28h%7Cx%29+dh%3D%5Cmathbb%7BE%7D_%5Cepsilon%5B%5Cnabla+g%28f_%5Cphi%28x%2C%5Cepsilon%29%29%5D%3D%5Cmathbb%7BE%7D_%5Cepsilon%5Bg%27%28f%28x%2C%5Cepsilon%29%29%5Ccdot%5Cnabla_%5Cphi+f%28x%2C%5Cepsilon%29%5D" alt="">. 如果它是DNN决定的， 就可以backpropagation. 对<img src="http://www.zhihu.com/equation?tex=%5Cepsilon+" alt="">做sample, 这个estimator的variance就小了很多。</p>
<p>比如我们假设<img src="http://www.zhihu.com/equation?tex=f%28x%2C%5Cepsilon%29" alt="">满足的分布是一个正态分布，它的mean 和variance由被<img src="http://www.zhihu.com/equation?tex=%5Cphi" alt="">决定的两个DNN 算出来<img src="http://www.zhihu.com/equation?tex=f%28x%2C%5Cepsilon%29%3Dm_%5Cphi%28x%29%2B%5Cepsilon%5Ccdot+V_%5Cphi%28x%29%2C+%5Cepsilon%5Csim+N%280%2C1%29" alt="">，那么我们就可以很简单地计算<img src="http://www.zhihu.com/equation?tex=%5Cnabla_%5Cphi+f%28x%2C%5Cepsilon%29" alt="">. 然后交替地优化<img src="http://www.zhihu.com/equation?tex=%5Ctheta" alt="">和<img src="http://www.zhihu.com/equation?tex=%5Cphi" alt="">就可以得到结果。</p>
<p>VAE几乎是现存的最优秀的generative model之一，另外一个是GAN。</p>
<p>它的问题也非常明显。Real data, 比如natural images，它是非常高维的数据，但他一般处在一个low dimensional manifold上面。VAE使用Gaussian approximation去做inference，通常也假设p是gaussian的，这样缺点就是没办法capture这个low dimensional structure。因为gaussian不可能做到100%低维。</p>
<h1 id="《Generating-Sentences-From-a-Continuous-Spaces》-ICLR-2016"><a href="#《Generating-Sentences-From-a-Continuous-Spaces》-ICLR-2016" class="headerlink" title="《Generating Sentences From a Continuous Spaces》. ICLR 2016"></a>《Generating Sentences From a Continuous Spaces》. ICLR 2016</h1><p>motivation 在于作者为了弥补传统的 RNNLM 结构缺少的一些 global feature（其实可以理解为想要 sentence representation）。其实抛开 generative model，之前也有一些比较成功的 non-generative 的方法，比如 sequence autoencoders[1]，skip-thought[2]和 paragraph vector[3]。但随着 VAE 的加入，generative model 也开始在文本上有更多的可能性。<br>Loss 的组成还是和 VAE 一样。具体模型上，encoder 和 decoder 都采用单层的 LSTM，decoder 可以看做是特殊的 RNNLM，其 initial state 是这个 hidden code z（latent variable），z 采样自 Gaussian 分布 G，G 的参数由 encoder 后面加的一层 linear layer 得到。这里的 z 就是作者想要的 global latent sentence representation，被赋予了先验 diagonal Gaussians，同时 G 就是学到的后验。</p>
<p>模型很简单，但实际训练时有一个很严重的问题：KL 会迅速降到 0，后验失效了。原因在于，由于 RNN-based 的 decoder 有着非常强的 modeling power，直接导致即使依赖很少的 history 信息也可以让 reconstruction errors 降得很低，换句话说，decoder 不依赖 encoder 提供的这个 z 了，模型等同于退化成 RNNLM（摊手）。顺便一提，本文最后有一篇 paper 也是为了解决这个问题。</p>
<p>先看这篇 paper 提出的解决方法：KL cost annealing 和 Word dropout。</p>
<p>1) KL cost annealing<br>作者引入一个权重 w 来控制这个 KL 项，并让 w 从 0 开始随着训练逐渐慢慢增大。作者的意思是一开始让模型学会 encode 更多信息到 z 里，然后随着 w 增大再 smooth encodings。其实从工程/代码的角度看，因为 KL 这项更容易降低，模型会优先去优化 KL，于是 KL 很快就降成 0。但如果我们乘以一开始很小的 w，模型就会选择忽视 KL（这项整体很小不用降低了），选择优先去降低 reconstruction errors。当 w 慢慢增大，模型也慢慢开始关注降低 KL 这项了。这个技巧在调参中其实也非常实用。</p>
<p>2) Word dropout</p>
<p>既然问题是 RNN-based 的 decoder 能力太强，那我们就来弱化它好了。具体方法是把 input 的词替换成 UNK（我可能是个假的 decoder），模型被迫只能去多多依赖z。当然保留多少 input 也需要尝试，我们把全都不保留的叫做 inputless decoder，实验表明，inputless VAE 比起 inputless RNN language model 不知道好到哪里去了。<br>受到 GAN 的启发，作者还提出了一个 Adversarial evaluation，用一半真一半假的数据作为样本训练出一个分类器，再对比不同模型生成的句子有多少能骗过这个分类器，这个 evaluation 被用在 Imputing missing words 这个任务上，VAE 的表现同样比 RNNLM 出色。<br>最后，作者展示模型的确学到了平滑的 sentence representation。选取两个 sentence 的 code z1 和 z2，z1 和 z2 可以看做向量空间的两个点，这两个点连线之间的点对应的句子也都符合语法且 high-level 的信息也保持局部一致。</p>
<h1 id="《Neural-Variational-Inference-for-Text-Processing》-ICML-2016"><a href="#《Neural-Variational-Inference-for-Text-Processing》-ICML-2016" class="headerlink" title="《Neural Variational Inference for Text Processing》. ICML 2016"></a>《Neural Variational Inference for Text Processing》. ICML 2016</h1><p>其实这篇 paper 和第一篇是一起投的 ICLR，后来转投了 ICML 2016，所以时间上其实和第一篇是一样的（两篇文章也有互相引用）。不同于第一篇，作者的出发点是构建一个 generative neural variational framework。为了证明 framework 的优越性，分别在 unsupervised 和 supervised 的任务上提出了两个模型，结果也很令人满意。</p>
<p>第一个任务是 unsupervised document modeling，模型叫 Neural Variational Document Model（NVDM）。h 和第一篇的 z 一样，在这里代表 latent document semantics，但 document 是以 bag-of-words 的形式（个人以为这里作者主要还是受到 LDA 的影响）。encoder 采用MLP，decoder 是一层 softmax。<br>第二个任务是 supervised answer selection，模型叫 Neural Answer Selection Model（NASM）。文本的建模方式采用 LSTM（在第二个任务用 LSTM，第一个任务用词袋，可能为了证明普适性）。h 代表 latent question semantics。如上图所示，Zq 和 Za 用来表示 question 和 answer，y 代表 answer 是不是正确答案，用 Zq 和 Za 预测 y。那么 Zq 和 Za 是怎么得到的呢？Zq 延用 LSTM 的 last state，而 Za 则较为复杂，所谓脱离问题谈答案都是耍流氓，所以对 Za 建模时要显式的放入 question 的信息。可这里该怎么表示 question 呢？如果还用 Zq，模型很容易 overfitting。这里我们的 latent h 终于可以出场了，引入 h 不仅起到了 muti-modal 的效果，还让模型更 robust，再把基于 attention 的 c(a,h)和 answer 的 LSTM last state 组合得到 Za。这种做法对我们在寻找 representation 时有很好的借鉴作用。最后通过推导 variational lower bound 确定 h 的先验是 p(h|q)（第一个任务中先验是 p(h)）, 这里就不赘述了。</p>
<h1 id="《Language-as-a-Latent-Variable-Discrete-Generative-Models-for-Sentence-Compression》-EMNLP-2016"><a href="#《Language-as-a-Latent-Variable-Discrete-Generative-Models-for-Sentence-Compression》-EMNLP-2016" class="headerlink" title="《Language as a Latent Variable: Discrete Generative Models for Sentence Compression》. EMNLP 2016"></a>《Language as a Latent Variable: Discrete Generative Models for Sentence Compression》. EMNLP 2016</h1><p>这篇 paper 发表在 EMNLP 2016，同样出自第二篇 paper 的作者。传统的 VAE 是把数据 encode 成 continuous latent variable，这篇 paper 的贡献在于提出了一个 generative model 用来学到 language 的 discrete representation—一个带有 sequential discrete latent variable 的 VAE。所谓的 discrete latent variable 就是指一个单词，加上 sequential 其实就是一个句子，由于 VAE 本身是压缩数据的，换句话说是用短一点的句子来表示原来的句子，也就是句子压缩。我觉得作者的 intuition 在于每个句子可以有多个缩写，且都可以表示原句，有一点点 distribution 的意思，所以用 latent variable 很合适。<br>原句和压缩句分别是 s 和 c ，模型整体是 encoder -&gt; compressor -&gt; decoder。我们分解开看，encoder -&gt; compressor 采用 pointer network[4]只从 s 里选取合适的词而不是整个词典，从而大大减少了 search space。compressor -&gt; decoder 是一个带 soft attention 的 seq2seq。这个模型的好处是不需要 label 数据，但是如果我们有足够的 label 数据（真实数据里 c 里的词可不仅仅来自 s），需要额外加个 softmax 从整个词典里选词，同时再定义一个 latent factor 判断是从 s（pointer network）还是从词典里选，更加符合任务需求。<br>值得一提的是 Variational lower bound 里的 p(c)是 pre-train 好的 language model。因为 Language model 的一个特点是比较喜欢短句子，很适合句子压缩的场景。由于 reparameterisation trick 并不适用 discrete latent variable，作者还采用了 REINFORCE[5]的方法（凡是 discrete 的问题，GAN/VAE 都可以采用 REINFORCE）。</p>
<h1 id="《A-Hierarchical-Latent-Variable-Encoder-Decoder-Model-for-Generating-Dialogues》-AAAI-2017"><a href="#《A-Hierarchical-Latent-Variable-Encoder-Decoder-Model-for-Generating-Dialogues》-AAAI-2017" class="headerlink" title="《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》. AAAI 2017"></a>《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》. AAAI 2017</h1><p>这是第一篇把 VAE 的思想引入到 dialogue 的 paper。和普通的 VAE 区别在于 dialogue 的 reconstruction 是生成的下一句 utterance，而不是 input 自身。这篇 paper 的前身是 HRED[6]，HRED 的核心思想是，把 dialogue 看做是 two-level：dialogue 是 utterance 的组合，utterance 是 words 的组合。HRED 由 3 个 RNN 组成：encode RNN 把每个 utterance 变成 real-valued 的向量 u，context RNN 把每个 turn 里的 u 作为输入变成向量 c，最后把 c 交给 deocde RNN 生成下一个 utterance。<br>VHRED 在 HRED 的基础上每个 turn 里引入一个 latent variable z，z 由 context RNN 的 c 生成。z 的意义比较笼统，sentiment/topic 怎么解释都行。模型的训练技巧如 KL annealing 等大量借鉴了第一篇 paper 的思想，特别要注意训练时的 z 从后验采样（保证 decode 的正确性），测试时再从先验采样（ KL 已经把分布拉近）。实验表明，latent variable 有助于生成更加 diverse 的回复。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/20/understand-rnn-from-motivation/" rel="next" title="understand rnn from motivation">
                <i class="fa fa-chevron-left"></i> understand rnn from motivation
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/27/SingleInstructionMultipleData/" rel="prev" title="SingleInstructionMultipleData">
                SingleInstructionMultipleData <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://ooo.0o0.ooo/2017/07/01/59575381605d5.png"
               alt="唐相儒" />
          <p class="site-author-name" itemprop="name">唐相儒</p>
           
              <p class="site-description motion-element" itemprop="description">Never Let Your Fear Decide Your Future</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/目录">
                <span class="site-state-item-count">82</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#《Generating-Sentences-From-a-Continuous-Spaces》-ICLR-2016"><span class="nav-number">1.</span> <span class="nav-text">《Generating Sentences From a Continuous Spaces》. ICLR 2016</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#《Neural-Variational-Inference-for-Text-Processing》-ICML-2016"><span class="nav-number">2.</span> <span class="nav-text">《Neural Variational Inference for Text Processing》. ICML 2016</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#《Language-as-a-Latent-Variable-Discrete-Generative-Models-for-Sentence-Compression》-EMNLP-2016"><span class="nav-number">3.</span> <span class="nav-text">《Language as a Latent Variable: Discrete Generative Models for Sentence Compression》. EMNLP 2016</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#《A-Hierarchical-Latent-Variable-Encoder-Decoder-Model-for-Generating-Dialogues》-AAAI-2017"><span class="nav-number">4.</span> <span class="nav-text">《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》. AAAI 2017</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">唐相儒</span>
</div>






        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="true"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2017/09/20/what-is-vae/';
          this.page.identifier = '2017/09/20/what-is-vae/';
          this.page.title = 'what is vae';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  





  






  





  

  

  

  



<script type="text/javascript" color="176,23,31" opacity='1' zIndex="-1" count="66" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<!-- 背景动画 -->
<script type="text/javascript" src="/js/src/particle.js"></script>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
</body>
</html>
