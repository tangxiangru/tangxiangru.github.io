<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tianjin U RL Seminar]]></title>
    <url>%2F2019%2F08%2F07%2FTianjin-U-RL-Seminar%2F</url>
    <content type="text"><![CDATA[天津大学的RL讲习班，分为九个章节 GCN背景处理图类型的数据上遇到了问题 特征矩阵：N*F的矩阵X，和结构矩阵A来作为输入 传播规则A*X 更新方式分为基于谱 spectral-based GCN基于谱图 laplacian matrix节点的度矩阵：对角阵 laplacian matrix 可以被定义为L=D-A 进一步正则化：（带入D-A） 特点：正交矩阵，所以可以进一步分解U^U 为什么要用：可以和spectral domain进行对应 spectral-based GCN定义图上的傅里叶变化，定义的是x*g传播算法 但是计算开销大，使用切比雪夫多项式近似表示 所以最后x*g的卷积定义为 参数乘T乘L乘x的K个参数的式子累加 例子：空手道俱乐部半监督学习，link prediction spatial-based GCN邻居节点进行聚合 为什么谱的GCN的缺点：领域相关、扰动敏感、大图难处理 inductive leanring &amp; transductive leanring不同阶的采样得到中心节点的表征向量，为每个节点都生成表征向量。 inductive leanring：无监督学习 transductive leanring Embedding test：graphSAGEhash会对WL算法产生影响，所以用网络来学习代替hash函数 对比的baseline是deep walk LGCN邻居节点数量不定且无顺序，这篇把图结构的数据变成grid-like的数据 数量不定且无顺序：k largest node select 大图：subgraph training 发在KDD 2018。图中节点的邻节点数目是不确定的，邻节点之间也没有任何顺序可言。这2点对GCN提出了很大的挑战。该文提出的LGCNs模型能够基于数值排序的方法自动选择固定数目的邻居节点作为GCN的输入特征，从而解决了上述2个难点并使用常规的卷积操作来进行图编码，并使用子图训练方法来使LGCNs能够适用于大图(large-scale graphs)训练。 marlbackgroundmdp的一些定义 部分可观察的马尔科夫过程，完全合作的 coordinationmaddpg 集中式学习分布式执行 第一篇，用attention做策略估计，条件估值 第二篇，不同agent的联合信息 足球：只用关注某些球员，而非全部 maddpg不具有良好的泛化性 credit assignment in marl优化长期累积收益，agent只能收到局部信息。 分为差异奖励和对q value（长期累积收益）分解 Value-Decomposition Networks For Cooperative Multi-Agent Learning发现low level的架构在所有任务上都比high level好 Counterfactual Multi-Agent Policy Gradients差异奖励。构造baseline的方式，就是构造default action COMA学习一个集中式的critic，就是Q。当前策略下的动作分布依次去计算Q。相对于当前策略的优势。 星际：分布式的微观管理。 反事实思维（Counterfactual） QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning也是星际任务，Background：把 fully cooperative multi-agent task 多智体问题考虑成Dec-POMDP问题（部分可观察马尔可夫决策过程）。 只是基于自身的历史信息，缺乏正取评估不是很准。所以用qmix放缩，只假设和qy成正相关。 用Value Decomposition Networks（VDNs）值分解网络。 QMIX：三个网络。但是 QMIX（因为要满足单调性）对式子充分但不必要。介于 IQL 与 centralised Q-learning （应该是动作空间相互耦合）之间，让这个函数随着每个智能体值函数有单调性。（核心）。不是线性加和得到q total。是mixing Network 中的参数必须限制为非负的，这样就可以逼近任意非负函数 单调性使得分布式策略和总体策略是一致的啊。所以必须保证单调性。 左边网络接收输入就是q，参数是通过hypernetwork产生的，从而保证参数非负。（激活函数单调） exploration in rlExploration是一件很重要同时也很困难的事情。与其他机器学习范式相比，RL通常只知道某个动作的能得多少分，却不知道该动作是不是最好的——这就是基于evaluate的强化学习与基于instruct的监督学习的根本区别。 正因如此，RL的本质决定了它极其需要Exploration，我们需要通过不断地探索来发现更好的决策，或者至少证明当前的决策是最好的——所以Exploration-Exploitation成为了强化学习领域诸个Tradeoff中最出名的一个。 Exploration是尝试新的东西，本质上是为了获取更多信息。各种Exploration方法的核心就是用尽可能少的代价获得尽可能有价值的信息。 count based explorationICM好奇心机制curiosity这种intrinsic reward是基于智能体对下一步预测与实际下一步状态差距来定义的。不过之前的文章提到这种定义会出现的一个问题，那就是当环境出现与智能体无关的随机性的时候，智能体会因为始终不能预测下一步的状态，而卡在相应的位置。这篇文章就解决了这个问题。 反向模型来解决这个问题。关注随机性的部分就不能产生动作。inverse model更关注动作，其表示自然会不去包含与其无关的内容。通过求min（ahat和a）来迫使不去包含与其无关的内容。不希望考虑既不能被控制也不能影响智能体的部分。 RND：Random Network Distillation好几种intrinsic reward的设计模式了，比如基于动力学模型预测误差的（Curiosity、ICM）；基于各种信息增益的（Empowerment、VIME），这里的方法又是一种新的设计模式。 其设计的初衷主要还是基于对状态访问计数（count-based），但由于是高维连续空间，这个计数更多地可以看做是密度估计。如果类似的状态之前访问地少，那么说明这个状态比较新奇，那么就给予比较高的intrinsic reward。文章用了一个比较机智的方法来快速地估计某个状态是不是之前出现的比较少。 同时使用intrinsic reward和extrinsic reward Diversity-based exploration第二类方法是基于多样性的方法。这类方法的思想是尽量让agent学到的策略和之前的策略尽量不同。今天介绍的这个方法就是基于这个思想。它同过给通常RL优化的loss额外的加上正则项，来迫使agent去寻找新的策略。这里的D是用来衡量策略之间的差距的一个距离，根据不同的算法，D的计算方式也各不相同。 Diversity-driven exploration strategy for DRLHindsight Experience Replay处理的是特殊的任务–多目标的任务。任务是将绿色的物体推到红色的小球所在的位置，小球的位置是随机生成的，每一个小球的位置就对应一个目标（goal），我们的任务是训练一个策略能够应对所有的这些目标。环境的reward是稀疏的，只有在完成目标时才能获得reward。 我们先来看下multi-goal下的RL的一般做法。在普通的RL设置下，我们选择一个动作，只需要根据当前的状态，但是在multi-goal的setting下，我们选择动作不光依赖当前的状态，还要依赖我们的目标。所以我们的Q函数和策略都加入了额外的一项来表示当前的目标 gan与其他生成式模型相比，GAN这种竞争的方式不再要求一个假设的数据分布，即不需要formulate p(x)，而是使用一种分布直接进行采样sampling，从而真正达到理论上可以完全逼近真实数据 cGAN对generator和discriminator都加了一个condition，使得数据集是有标签的 ACGAN = cGAN+SGAN输出有分支，是否真假、标签。 对于生成器来说有两个输入，一个是标签的分类数据c，另一个是随机数据z。 对于判别器分别要判断数据源是否为真实数据的概率分布，以及数据源对于分类标签的概率分布。 第一部分 L_S 是面向数据真实与否的代价函数，第二部分 L_C 则是数据分类准确性的代价函数。在优化过程中希望判别器D能否使得 L_C+L_S 尽可能最大，而生成器G使得 L_C-L_S 尽可能最大；简而言之是希望判别器能够尽可能区分真实数据和生成数据并且能有效对数据进行分类，对生成器来说希望生成数据被尽可能认为是真实数据且数据都能够被有效分类。 InfoGAN让网络学习到了可解释的特征表示。 作者把原来的噪声输入分解成两部分：一是原来的z；二是由若干个latent variables拼接而成的latent code c，这些latent variables会有一个先验的概率分布，且可以是离散的或连续的，用于代表生成数据的不同特征维度，比如MNIST实验的latent variables就可以由一个取值范围为0-9的离散随机变量（用于表示数字）和两个连续的随机变量（分别用于表示倾斜度和粗细度）构成。 作者从信息论中得到启发，提出了基于互信息（mutual information）的正则化项。c的作用是对生成数据的分布施加影响，于是需要对这两者的关系建模， gail逆向强化学习：RL是通过agent不断与environment交互获取reward来进行策略的调整，最终得到一个optimal policy。但IRL计算量较大，在每一个内循环中都跑了一遍RL算法。 IRL不同之处在于，无法获取真实的reward函数，但是具有根据专家策略得到的一系列轨迹。假设专家策略是真实reward函数下的最优策略，IRL学习专家轨迹，反推出reward函数。 挺难的，这里没听懂。 gan in nlpchallenge：gan的问题，梯度不可回传，所以使用vae的mle。但这样training和inference的loss不一样。这样在优化目标的时候又有人提出一些rl的方法，但这样不是真实的评分，因为评分不一定足够好。同时，word level的reward是需要的。 seqgan首先如何解决离散数据梯度无法回传给Generator的问题：将Generator的优化转化为最大化rewards，然后利用policy gredient优化Generator就可以了。 其次seqGAN作者还指出Discriminator只能对完整的句子进行判断，而无法判断部分句子的好坏，而实际上一个句子并不是全部都很差，而仅仅其中部分不好而已：当解码到t时，即对后面T-t个timestep采用蒙特卡洛搜索搜索出N条路径，将这N条路径分别和已经decode的结果组成N条完整输出，然后将D网络对应奖励的平均值作为reward.因为当t=T时无法再向后探索路径，所以直接以完整decode结果的奖励作为reward。蒙特卡洛搜索是指在选择下一个节点的时候用蒙特卡洛采样的方式，而蒙特卡洛采样是指根据当前输出词表的置信度随机采样。 dpgansentiganrankganleakgan长文本生成和稀疏信息的问题 稀疏信息是每次只给一个reward。图片是梯度连续的，图片的每个pixel都是有梯度的，但是句子只有十几个，远远小于连续的数据。即序列生成过程中缺乏一些关于序列结构的中间信息的反馈。所以用层级强化学习，包括 Manager 模块和 Worker 模块。 判别器会在中间时间步泄露一些提取的特征给生成器，生成器则利用这个额外信息指导序列的生成。 improved adversarial image captionstack gan]]></content>
  </entry>
  <entry>
    <title><![CDATA[jupyter]]></title>
    <url>%2F2019%2F04%2F25%2Fjupyter%2F</url>
    <content type="text"><![CDATA[conda install ipykernelpython -m ipykernel install –user –name python_inn –display-name “python_inn” ssh -N -D 7073 txr@cas]]></content>
  </entry>
  <entry>
    <title><![CDATA[新服务器配置]]></title>
    <url>%2F2019%2F04%2F25%2F%E6%96%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[wget https://github.com/zsh-users/zsh/archive/zsh-5.6.2.tar.gz -O /tmp/zsh-5.6.2.tar.gz \ &amp;&amp; tar zxvf /tmp/zsh-5.6.2.tar.gz \ &amp;&amp; cd /tmp/zsh-zsh-5.6.2/ \ &amp;&amp; ./configure –prefix=$HOME/.local \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd - wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O /tmp/install-oh-my-zsh.sh \ &amp;&amp; sed -i ‘/env zsh/d’ /tmp/install-oh-my-zsh.sh \ &amp;&amp; sed -i ‘s/chsh -s/sudo chsh -s/g’ /tmp/install-oh-my-zsh.sh \ &amp;&amp; sh /tmp/install-oh-my-zsh.sh \ &amp;&amp; rm -f /tmp/install-oh-my-zsh.sh \ &amp;&amp; wget https://raw.githubusercontent.com/oskarkrawczyk/honukai-iterm/master/honukai.zsh-theme -O ~/.oh-my-zsh/themes/honukai.zsh-theme \ &amp;&amp; sed -i.bak ‘/ZSH_THEME/s/\”.*\”/\”honukai\”/‘ ~/.zshrc wget https://raw.githubusercontent.com/hughplay/lightvim/master/install.sh -O - | sh git clone https://github.com/hughplay/tmux-config.git&amp;&amp; wget https://raw.githubusercontent.com/hughplay/env/master/scripts/centos/install-tmux.sh&amp;&amp; bash install-tmux.sh&amp;&amp; cd tmux-config/&amp;&amp; bash install.sh&amp;&amp; export PATH=$HOME/.local/bin/:$PATH ssh -N -D 7073 txr@cas]]></content>
  </entry>
  <entry>
    <title><![CDATA[可解释性inNLP]]></title>
    <url>%2F2019%2F04%2F18%2F%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7inNLP%2F</url>
    <content type="text"><![CDATA[&lt;!–more–&gt; 可解释性in NLP 1.什么方法没做 我们可以用到nlp 2.做到什么程度 哪些好哪些坏 3.就4.18讨论 concept net比较重要 4.就理论而言，不同方法之间的区别和联系 同一个方向内大部分方法都是有关系的，重在其中的联系 ig能用contribution的角度去量化它]]></content>
  </entry>
  <entry>
    <title><![CDATA[aboutPretrained]]></title>
    <url>%2F2019%2F04%2F14%2FaboutPretrained%2F</url>
    <content type="text"><![CDATA[feature based：emlo得到上下文相关的embedd传递给下游任务 finetuning based：gptgpt用于下游任务对模型参数精调 bert会随着下游任务进行微调bert base和bert large 既可以feature也可以finetuning #对比bert和gpt bert时长和voc更大 bert优化目标是mlm和nsp两个任务 bert 的finetuning学习率根据task变化 #对比bert和elmo 1.bert使用了Transformer 2.bert实现了真正的双向，elmo是将两个模型拼接在一起的，而bert是用masked Transformer，克服了自己看自己的问题 bert优点：高效捕捉长距离依赖、捕捉真正双向信息 bert缺点：每次挑选百分之15，很慢 未来：MT-DNNfinetuning是多任务的（无需大量标注数据） bert不能用于生成式任务：序列生成的问题是给定前n个词，生成后面词。但是bert是挡出前面和后面，然后来挖空。他就特点就是完全双向。 bert启发是masked language model同时，也预测next Sentence （object function） language model+Transformer=bert（encoder）、gpt（Decoder） 经典的Transformer是从左到右自回归顺序解码，在解码中加mask 主要贡献是self Attention的提出，每个位置可以同步。在编码中没有mask，在解码中加。 nli任务是是不是有关系，如果这两句子确实跟language model有关系。而分类就没啥意义。 不管是bert 还是gpt肯定会对 task specific的任务有提升 gpt启示是把语言模型当做无监督多任务学习器。 低语料的文本生成上用language model会有用。]]></content>
  </entry>
  <entry>
    <title><![CDATA[NewFashionQA]]></title>
    <url>%2F2019%2F03%2F06%2FNewFashionQA%2F</url>
    <content type="text"><![CDATA[多轮问答（document based dialogue）DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension https://dataset.org/ dream/. Challenging reading comprehension on daily conversation: Passage completion on mul- tiparty dialog. QuAC: Question an- swering in context A Dataset for Document Grounded Conversations OF WIKIPEDIA:KNOWLEDGE-POWERED CONVERSATIONAL AGENTS Towards Exploiting Background Knowledge for Building Conversation Systems 综述A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC Emergent Logical Structure in Vector Representations of Neural Readers[https://arxiv.org/pdf/1611.07954v1.pdf] 机器阅读理解任务综述 林鸿宇 韩先培[https://mp.weixin.qq.com/s?__biz=MzIxNzE2MTM4OA==&amp;mid=2665643130&amp;idx=1&amp;sn=5f75f0d4978289caea6c4cb37b0b74c4\] 自动问答综述 郑实福， 刘挺， 秦兵， 李生 - 中文信息学报 面向问答社区的答案摘要方法研究综述 刘秉权， 徐振， 刘峰， 刘铭， 孙承杰， 王晓龙 - 中文信息学报 问答系统综述 李沛晏， 朱露， 吴多胜 - 数字技术与应用 特定领域问答系统中基于语义检索的非事实型问题研究仇瑜， 程力 - 北京大学学报 (自然科学版), 2019 基于汉语篇章框架语义分析的阅读理解问答研究王智强， 李茹， 梁吉业， 张旭华， 武娟， 苏娜 - 计算机学报 基于 Web 的问答系统综述 李舟军， 李水华 - 计算机科学, 2017 智能问答系统在医学领域的应用研究 贺佳， 杜建强， 聂斌， 熊旺平， 罗计根 - 医学信息, 2018 基于粗糙集知识发现的开放领域中文问答检索韩朝， 苗夺谦， 任福继， 张红云 - 计算机研究与发展, 201 DatasetsMCTesthttp://research.microsoft.com/en-us/um/redmond/projects/mctest/data.html bAbIhttps://research.fb.com/projects/babi/ WikiQAhttps://www.microsoft.com/en-us/download/details.aspx?id=52419 SNLIhttp://nlp.stanford.edu/projects/snli/ Children’s Book Testhttps://research.fb.com/projects/babi/ BookTesthttps://ibm.ent.box.com/v/booktest-v1 CNN / Daily Mailhttp://cs.nyu.edu/~kcho/DMQA/ Who Did Whathttps://tticnlp.github.io/who_did_what/download.html NewsQAhttp://datasets.maluuba.com/NewsQA SQuADhttps://rajpurkar.github.io/SQuAD-explorer/ LAMBADAhttp://clic.cimec.unitn.it/lambada/ MS MARCOhttp://www.msmarco.org/dataset.aspx WikiMovieshttps://research.fb.com/projects/babi/ WikiReadinghttps://github.com/dmorr-google/wiki-reading CoQA: A conversational question answering challenge. SemEval-2018 Task 11: Machine comprehen- sion using commonsense knowledge. ODSQA: Open-domain spoken question an- swering dataset. Looking beyond the surface: A challenge set for reading comprehension over multiple sen- tences SearchQA: A new Q&amp;A dataset augmented with context from a search engine 论文Teaching Machines to Read and Comprehend[https://arxiv.org/abs/1506.03340] Learning to Ask: Neural Question Generation for Reading Comprehensionhttps://arxiv.org/pdf/1705.00106.pdf Attention-over-Attention Neural Networks for Reading Comprehensionhttps://arxiv.org/pdf/1607.04423.pdf R-NET: MACHINE READING COMPREHENSION WITH SELF-MATCHING NETWORKShttps://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf Mnemonic Reader for Machine Comprehensionhttps://arxiv.org/pdf/1705.02798.pdf TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehensionhttps://arxiv.org/pdf/1705.03551.pdf S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehensionhttps://arxiv.org/pdf/1706.04815.pdf RACE: Large-scale ReAding Comprehension Dataset From Examinationshttps://arxiv.org/pdf/1704.04683.pdf Adversarial Examples for Evaluating Reading Comprehension Systemshttps://arxiv.org/pdf/1707.07328.pdf Machine comprehension using match-lstm and answer pointer[https://arxiv.org/pdf/1608.07905] Multi-perspective context matching for machine comprehension[https://arxiv.org/abs/1612.04211] Reasonet: Learning to stop reading in machine comprehension[http://dl.acm.org/citation.cfm?id=3098177] Learning recurrent span representations for extractive question answering[https://arxiv.org/abs/1611.01436] End-to-end answer chunk extraction and ranking for reading comprehension[https://arxiv.org/abs/1610.09996] Words or characters? fine-grained gating for reading comprehension[https://arxiv.org/abs/1611.01724] Reading Wikipedia to Answer Open-Domain Questions[https://arxiv.org/abs/1704.00051] An analysis of prerequisite skills for reading comprehension[http://www.aclweb.org/anthology/W/W16/W16-60.pdf#page=13] A Comparative Study of Word Embeddings for Reading Comprehensionhttps://arxiv.org/pdf/1703.00993.pdf Chen et al., 2017;Reading wikipedia to answer open- domain questions. Huang et al., 2018;Flowqa: Grasping flow in history for conversational machine comprehension Yu et al., 2018; Qanet: Combining local convolution with global self-attention for reading comprehen sion. Zhipeng Chen, Yiming Cui, Wentao Ma, Shijin Wang, and Guoping Hu. 2019. Convolutional spatial attention model for reading comprehen- sion with multiple-choice questions. Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. 2017. Attention- over-attention neural networks for reading com- prehension. Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen, and Ruslan Salakhutdinov. 2017. Gated-attention readers for text com- prehension. 一些模型：1 Memory Networks1.1 Memory Network1.2 End-To-End Memory Networks1.3 Ask Me Anything: Dynamic Memory Networks for Natural Language Processing1.4 Key-Value Memory Networks for Directly Reading Documents1.5 The Goldilocks Principle: Reading Children’s Books with Explicit Memory Representations1.6 Can Active Memory Replace Attention?2 DeepMind Attentive Reader2.1 Teaching Machines to Read and Comprehend3 Danqi’s Stanford Reader3.1 [A thorough examination of the cnn/daily4 Attention Sum Reader5 Gated Attention Sum Reader6 Attention Over Attention Reader6.1 Attention-over-Attention Neural Networks for Reading Comprehension]]></content>
  </entry>
  <entry>
    <title><![CDATA[install_pytorch_with_conda]]></title>
    <url>%2F2019%2F02%2F14%2Finstall-pytorch-with-conda%2F</url>
    <content type="text"><![CDATA[安装步骤 http://anaconda.com/download/ 下载anaconda3 放到 目录下，命令：bash Anaconda3-5.1.0-Linux-x86_64.sh echo ‘export PATH=”~/anaconda3/bin:$PATH”‘ &gt;&gt; ~/.bashrc source ~/.bashrc conda create –name python3 python=3.4source activate python3 改为清华源conda config –add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config –add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/conda config –add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config –set show_channel_urls yes cp ~/.condarc{,.bak} cat ~/.condarc.bak vim ~/.condarc修改为如下channels: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/show_channel_urls: true如上 conda install *]]></content>
  </entry>
  <entry>
    <title><![CDATA[Interpretable-Machine-Learning]]></title>
    <url>%2F2019%2F01%2F30%2FInterpretable-Machine-Learning%2F</url>
    <content type="text"><![CDATA[areaMachine Learning:Auto-encoderOscar Li, Hao Liu, Chaofan Chen, Cynthia Rudin: Deep Learning for Case- Based Reasoning Through Prototypes: A Neural Network That Explains Its Predictions. AAAI 2018: 3530-3537 NLP：Hui Liu, Qingyu Yin, William Yang Wang: Towards Explainable NLP: A Generative Explanation Framework for Text Classification. CoRR abs/1811.00196 (2018) Computer VisionUncertainty Map:Alex Kendall, Yarin Gal: What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? NIPS 2017: 5580-5590 Saliency Map：Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, BeenKim: Sanity Checks for Saliency Maps. NeurIPS 2018: 9525-9536 Game TheoryShapley Additive ExplanationScott M. Lundberg, Su-In Lee: A Unified Approach to Interpreting Model Predictions. NIPS 2017: 4768- 4777 Planning and SchedulingXAI Plan:Rita Borgo, Michael Cashmore, Daniele Magazzeni: Towards Providing Explanations for AI Planner Decisions. CoRR abs/1810.06338 (2018) Human-in-the-loop Planning:Maria Fox, Derek Long, Daniele Magazzeni: Explainable Planning. CoRR abs/1709.10256 (2017) Robo$csNarration of Autonomous Robot Experience:Stephanie Rosenthal, Sai P Selvaraj, and Manuela Veloso. Verbalization: Narration of autonomous robot experience. In IJCAI, pages 862–868. AAAI Press, 2016. From Decision Tree to human-friendly informationL:Raymond Ka-Man Sheh: “Why Did You Do That?” Explainable Intelligent Robots. AAAI Workshops 2017 The Need to Explain• User Acceptance &amp; Trust • Legal• Conformance to ethical standards, fairness • Right to be informed• Contestable decisions• Explanatory Debugging • Flawed performance metrics• Inadequate features • Distributional drift• Increase Insightfulness • Informativeness• Uncovering causality[Lipton 2016, Ribeiro 2016, Weld and Bansal 2018][Goodman and Flaxman 2016, Wachter 2017] [Kulesza et al. 2014, Weld and Bansal 2018] ACL 2018Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation. Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi. Word Embedding and WordNet Based Metaphor Identification and Interpretation. Rui Mao, Chenghua Lin, Frank Guerin. Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder. Ryo Takahashi, Ran Tian, Kentaro Inui. Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph. AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency. Automatic Estimation of Simultaneous Interpreter Performance. Craig Stewart, Nikolai Vogler, Junjie Hu, Jordan Boyd-Graber, Graham Neubig. ACL 2017An Interpretable Knowledge Transfer Model for Knowledge Base Completion Qizhe Xie, Xuezhe Ma, Zihang Dai and Eduard Hovy Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation Lotem Peled and Roi Reichart Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function Oren Melamud and Jacob Goldberger ICLR 2018Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement LearningInterpretable Counting for Visual Question Answering ICML 2018Varia onal Bayes and Beyond:Bayesian Inference for Big DataTamara Broderick (MIT)Loca on: Victoria Learning to Explain: An Information Theoretic Perspective on Model InterpretationJianbo Chen, Le Song, Marn Wainwright, Michael Jordan Discovering Interpretable Representa ons for BothDeep Genera ve and Discrimina ve ModelsTameem Adel, Zoubin Ghahramani, Adrian Weller Programma cally Interpretable Reinforcement LearningAbhinav Verma, Vijayaraghavan Murali, Rishabh Singh,Pushmeet Kohli, Swarat Chaudhuri Differentiable Abstract Interpreta on for Provably RobustNeural NetworksMa hew Mirman, Timon Gehr, Mar n Veche Interpretability Beyond Feature A ribu on:Quan ta ve Tes ng with Concept Ac va on Vectors(TCAV)Been Kim, Mar n Wa enberg, Jus n Gilmer, Carrie Cai,James Wexler, Fernanda B Viégas, Rory sayres oi-VAE: Output Interpretable VAEs for Nonlinear GroupFactor AnalysisSamuel Ainsworth, Nick J Fo , Adrian KC Lee, Emily Fox Fairness, Interpretability, and Explainability Federa on of Workshops NeurIPS 2018Towards Robust Interpretability with Self-Explaining Neural Networks Representer Point Selection for Explaining Deep Neural NetworksExplaining Deep Learning Models – A Bayesian Non-parametric Approach Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples Learning Conditioned Graph Structures for Interpretable Visual Question Answering Diminishing Returns Shape Constraints for Interpretability and Regularization Uncertainty-Aware Attention for Reliable Interpretation and Prediction Integrated GradientsAxiomatic Attribution for Deep Networks]]></content>
  </entry>
  <entry>
    <title><![CDATA[How_to_GOOGLE]]></title>
    <url>%2F2019%2F01%2F24%2FHow-to-GOOGLE%2F</url>
    <content type="text"><![CDATA[step 1你需要一个国外服务器（请自行购买及） step 2进入服务器输入以下命令 1wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 1wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/bbr.sh &amp;&amp; chmod +x bbr.sh &amp;&amp; bash bbr.sh step 3你需要在github上搜索shadowsocksr，然后下载release，安装。点击服务器配置，填入你在step2完成后输出的信息]]></content>
  </entry>
  <entry>
    <title><![CDATA[AI-en-Interview]]></title>
    <url>%2F2019%2F01%2F10%2FAI-en-Interview%2F</url>
    <content type="text"><![CDATA[英文面试准备Artificial intelligence is a technology that sparks the human imagination. What will our future look like as we come to share the earth with intelligent machines? Our minds gravitate to extremes, to the sharply contrasting visions that have captured public attention and divided much of the technological community. As a longtime AI researcher and venture capitalist in China and the U.S., I’ve observed these two camps across continents and over many decades. 人工智能（Artificial Intelligence）总能激发人类无限的想象力，当AI大规模普及世界各地、和我们共享地球时，人类的前景会是什么模样？我们对这种未来场景的想象，很容易想得比较极端，这些极端情绪模糊了现实与未来，把知识界分化成两派看法：乌托邦派、反乌托邦派。我在中美两国长年都是AI研究人员和创投从业者，在长达35年的职业生涯中，第一手观察不同国家这两派阵营的看法。 Utopians believe that once AI far surpasses human intelligence, it will provide us with near-magical tools for alleviating suffering and realizing human potential. In this vision, super-intelligent AI systems will so deeply understand the universe that they will act as omnipotent oracles, answering humanity’s most vexing questions and conjuring brilliant solutions to problems such as disease and climate change. 乌托邦派认为，人工智能一旦发展到超越人类智慧，就能为我们提供近乎神人般的工具，可以减轻人类负担、帮助人类发挥潜能。这一派相信，超人工智能将能理解宇宙运作，简直就像上帝，全知全能，能够帮助人类文明解决一些不可能的问题，为全球变暖和绝症提供难以想象的聪明解方。 But not everyone is so optimistic. The best-known member of the dystopian camp is the technology entrepreneur Elon Musk, who has called super-intelligent AI systems “the biggest risk we face as a civilization,” comparing their creation to “summoning the demon.” This group warns that when humans create self-improving AI programs whose intellect dwarfs our own, we will lose the ability to understand or control them. 当然，不是所有人都那么乐观。反乌托邦派最知名人物之一伊隆‧马斯克（Elon Musk）就曾说过，超人工智能是「人类文明的最大风险」，并且把它比喻为「召唤恶魔」。这一派阵营提出警告，当人类创造出能够不断自我精进、超越人类智慧的AI，我们就会失去理解或控制权。 Which vision to accept? I’d say neither. They simply aren’t possible based on the technology we have today or any breakthroughs that might be around the corner. Both scenarios would require “artificial general intelligence”—that is, AI systems that can handle the incredible diversity of tasks done by the human brain. Making this jump would require several fundamental scientific breakthroughs, each of which may take many decades, if not centuries. 应该接受哪一派的看法？我认为两者皆不。按照现在的技术，或是任何近期可能发生的重大技术创新来推测，很多「想象中」的情境，包括永恒不朽的数字大脑，或是无所不能的超人工智能，全都不可能发生。因为这些都需要「通用人工智能」（Artiﬁcial General Intelligence, AGI）才能实现，也就是会思考、解决问题、做决策的机器，有能力执行人类能做的任何智识工作，以及更多超越人类能力的事。然而，要发展到通用人工智能，还需要一系列根本性的AI科学突破，但这种科学突破每一项都需要几十年、甚至上百年的时间才能出现。 The real battles that lie ahead will lack the apocalyptic drama of Hollywood blockbusters, but they will disrupt the structure of our economic and political systems all the same. Looming before us in the coming decades is an AI-driven crisis of jobs, inequality and meaning. The new technology will wipe out a huge portion of work as we’ve known it, dramatically widening the wealth gap and posing a challenge to the human dignity of us all. 我们眼前必须面对的挑战，并不是像好莱坞科幻大片那样毁天灭地的剧情，尽管AI对各国现存政经结构的破坏力量，确实足以招致毁灭性的后果。在未来数十年间，由AI引发的人类就业危机、不均问题和人类对人生意义的探索，是各国政府和每个人都必须严肃面对的课题。新科技普及将会消灭我们如今熟知的许多工作，也会严重加剧如今已经非常严重的不均问题，同时对每个人的人性尊严带来最深刻、最内省的挑战。 This unprecedented disruption requires no new scientific breakthroughs in AI, just the application of existing technology to new problems. It will hit many white-collar professionals just as hard as it hits blue-collar factory workers. 而且，这股前所未有的破坏力量，并不需要任何新的AI科学突破，只要将现有的深度学习等AI技术应用到新的问题上便蔚然成形。各位可别以为就业受到波及的，只有蓝领的工厂员工，传统认为「高学历=金饭碗」的看法也将遭到颠覆，白领被取代的风险一样大。 Despite these immense challenges, I remain hopeful. If handled with care and foresight, this AI crisis could present an opportunity for us to redirect our energy as a society to more human pursuits: to taking care of each other and our communities. To have any chance of forging that future, we must first understand the economic gauntlet that we are about to pass through. 前方的挑战十分巨大，但我仍然保持乐观。如果我们能够深谋远虑，对相关课题予以足够的关切、做好准备，AI造成的就业危机将是一次难得的机会，让我们的社会能够展现充沛的活力，追求人类怀抱的各种兴趣、期望与梦想，并对他人展现更多的关爱。为了落实这种更美好的未来，我们必须先了解接下来我们可能会面对的经济难题。 Many techno-optimists and historians would argue that productivity gains from new technology almost always produce benefits throughout the economy, creating more jobs and prosperity than before. But not all inventions are created equal. Some changes replace one kind of labor (the calculator), and some disrupt a whole industry (the cotton gin). Then there are technological changes on a grander scale. These don’t merely affect one task or one industry but drive changes across hundreds of them. In the past three centuries, we’ve only really seen three such inventions: the steam engine, electrification and information technology. 许多技术乐观主义者和史学家都认为，新科技发展带来的生产力提升，几乎总是对经济有利，能够创造出更多的就业机会，让人类社会比以前更加繁荣。但不是每一项发明都一样，有些发明改变了我们执行单一工作的方式（例如打字机），有些发明消除了对某种人力的需求（例如计算器），有些发明则是彻底颠覆了一整个产业（例如轧棉机）。此外，还有规模完全不同的技术变化，这些重大突破不只影响了单一工作或产业，可以延伸至从数十种产业，根本改变整个经济流程，甚至社会组织。在过去三个世纪以来，对人类社会影响如此巨大的「通用技术」（General PurposeTechnologies, GPTs）只有三种：蒸汽引擎、电力和信息通讯科技（information and communications technology, ICT）。 Looking at this smaller data set, we have a mixed bag of economic impacts. The steam engine and electrification created more jobs than they destroyed, in part by breaking down the work of one craftsman into simpler tasks done by dozens of factory workers. But information technology (and the associated automation of factories) is often cited by economists as a prime culprit in the loss of U.S. factory jobs and widening income inequality. 蒸汽引擎和电力创造出来的工作比消灭的多，主要是因为这两项通用技术「去技能化」（deskilling），把原先需要一个高技能工作者的工作（例如手工纺织），拆解成数十个低技能工作者能做的更简单工作（例如操作动力织布机）。但信息革命（ICT）与工厂自动化不同，通常被经济学家认为是造成美国工厂就业流失与贫富不均加剧的主因。 The AI revolution will be of the magnitude of the Industrial Revolution—but probably larger and definitely faster. Where the steam engine only took over physical labor, AI can perform both intellectual and physical labor. And where the Industrial Revolution took centuries to spread beyond Europe and the U.S., AI applications are already being adopted simultaneously all across the world. AI革命的破坏和冲击，将比前两次工业革命更广大，发生的速度肯定也快许多。蒸汽引擎基本上改变了体力劳动的本质，ICT基本上改变了认知劳动的本质，而AI会同时改变两者，因为它可以执行多种不同的体能工作和智识工作，而且速度和效能远远胜过人类，大幅提升交通、制造到医疗等许多产业的效率。工业革命花了百年以上的时间横跨欧洲、美国到世界各地，AI应用基本上可以实时同步在世界各地展开。 AI’s main advantage over humans lies in its ability to detect incredibly subtle patterns within large quantities of data and to learn from them. While a human mortgage officer will look at only a few relatively crude measures when deciding whether to grant you a loan (your credit score, income and age), an AI algorithm will learn from thousands of lesser variables (what web browser you use, how often you buy groceries, etc.). Taken alone, the predictive power of each of these is minuscule, but added together, they yield a far more accurate prediction than the most discerning people are capable of. 与人类相比，AI最大的优势在于能从大量的数据中辨识非常细微的型态，并可从中学习。举例来说，银行的核贷专员可能只会看几项比较相关的「强特征」指针，也许是你的信用评分、所得级距、年龄等，来决定是否要放款给你，但AI算法会根据成千上万条看似不相关的「弱特征」变量，也许包括你使用什么浏览器、多久采买一次生活杂货等，来决定是否放款给你。如果把这些变量拆开来单独看，可能会令人完全想不到有这些特质跟还款能力什么关系，但是加总起来，AI算法据以评估、预测出来的贷款违约率，比起行内最谨慎、高竿的专业核贷人员都精准许多。 For cognitive tasks, this ability to learn means that computers are no longer limited to simply carrying out a rote set of instructions written by humans. Instead, they can continuously learn from new data and perform better than their human programmers. For physical tasks, robots are no longer limited to repeating one set of actions (automation) but instead can chart new paths based on the visual and sensor data they take in (autonomy). 在认知任务的表现上，AI自我学习的能力表示计算机不再局限于只能听从、执行人类编写的程序，而是能从不断累积的新数据中持续学习、精进，表现甚至比人类码农更出彩、惊人许多。在体力劳动的任务上，机器人不再局限于只能反复执行单一动作（自动化），而是能够根据机器视觉和感测数据规划出新的路径，并且安稳地在各种不同环境中运作（自主化）。 Together, this allows AI to take over countless tasks across society: driving a car, diagnosing a disease or providing customer support. AI’s superhuman performance of these tasks will lead to massive increases in productivity. According to a June 2017 study by the consulting firm PwC, AI’s advance will generate $15.7 trillion in additional wealth for the world by 2030. This is great news for those with access to large amounts of capital and data. It’s very bad news for anyone who earns their living doing soon-to-be-replaced jobs. 结合这些新的能力，AI如今可以完成人类社会中的许多任务，包括开车、诊断疾病、提供客服等。AI执行这些任务的超能力，将导致生产力大幅提升。 普华永道（PricewaterhouseCoopers）估计，到了2030年，AI的应用部署将为全球GDP增加15.7万亿美元。这对拥有大量数据和资本的人来说，无疑是个好消息；但对每日辛勤工作只为温饱，而且工作极可能有被取代的人来说，则是非常令人忧心的坏消息。 There are, however, limits to the abilities of today’s AI, and those limits hint at a hopeful path forward. While AI is great at optimizing for a highly narrow objective, it is unable to choose its own goals or to think creatively. And while AI is superhuman in the coldblooded world of numbers and data, it lacks social skills or empathy—the ability to make another person feel understood and cared for. Analogously, in the world of robotics, AI is able to handle many crude tasks like stocking goods or driving cars, but it lacks the delicate dexterity needed to care for an elderly person or infant. 不过，AI现在的能力仍然有限，而这些局限正好是人类希望的所在，可以为人类的未来指出一条明路。虽然AI能在相对狭窄的领域将结果优化，但仍然无法自行选定目标，或是发挥创意思考。 虽然AI在由0和1组成的冰冷世界里拥有超能力，但仍然缺乏社交能力和同理心，无法使人觉得受到关怀、被照顾。在机器人的世界，AI虽然能够轻松搬运重货、开车，但仍然完全无法胜任照顾年长者或婴童这类需要灵巧性与敏捷度的工作。 What does that mean for workers who fear being replaced? Jobs that are asocial and repetitive, such as fast-food preparers or insurance adjusters, are likely to be taken over in their entirety. For jobs that are repetitive but social, such as bartenders and doctors, many of the core tasks will be done by AI, but there remains an interactive component that people will continue to perform. The jobs that will be safe, at least for now, are those well beyond the reach of AI’s capabilities in terms of creativity, strategy and sociability, from social workers to CEOs. 这对担心工作遭到取代的人来说，有何涵义？社交程度低、重复性高的工作，例如连锁快餐店的备餐员或保险理算人，有可能完全被AI取代。重复性高、社交程度高的工作，例如调酒师、医师，虽然有许多核心任务将被AI取代，但仍旧扮演和人互动的重要角色。 那么，究竟什么工作比较安全、不会被AI取代（至少就目前而言）？我认为是那些需要发挥创造力、策略、社交程度高的工作，因为这些都超越AI现有的能力，包括从社工到CEO等等职业。 Even where AI doesn’t destroy jobs outright, however, it will exacerbate inequality. AI is inherently monopolistic: A company with more data and better algorithms will gain ever more users and data. This self-reinforcing cycle will lead to winner-take-all markets, with one company making massive profits while its rivals languish. 即使在AI难以取代人类工作的领域，也会严重加剧不均问题。AI本质上自然倾向形成垄断，一间公司如果拥有更多数据、更好的算法，由于大量数据有助于做出改善，更好的产品会吸引到更多用户，更多用户会产生更多数据，更多数据又会产生更好的产品，然后又会产生更多用户和数据，这会形成一个自我永续的良性循环，发展出赢家通吃的市场。结果就是，单一公司囊括了巨额利益，竞争对手被远远抛在身后，惨淡经营，苦撑下去。 A similar consolidation will occur across professions. The jobs that will remain relatively insulated from AI fall on opposite ends of the income spectrum. CEOs, home care nurses, attorneys and hairstylists are all in “safe” professions, but the people in some of these professions will be swimming in the riches of the AI revolution while others compete against a vast pool of desperate fellow workers. 这种两极化的不均情形，也发生在各个职业的收入上。比较不受到AI影响的工作，收入会非常两极分化，像CEO、居家看护、律师、发型师都属于相对「安全」的工作，有些人明显会因为AI革命而迅速累积更多财富，其他人则要辛苦地跟一大票同业竞争低薪工作。 We can’t know the precise shape and speed of AI’s impact on jobs, but the broader picture is clear. This will not be the normal churn of capitalism’s creative destruction, a process that inevitably arrives at a new equilibrium of more jobs, higher wages and better quality of life for all. Many of the free market’s self-correcting mechanisms will break down in an AI economy. The 21st century may bring a new caste system, split into a plutocratic AI elite and the powerless struggling masses. AI革命对人类就业的规模和实际影响目前犹未可知，但大致的前景是确定的。这不会是一波新的创造性破坏，像以前一样，引领出一波新的均衡，创造出更多就业、更高薪资，让所有人的生活质量变得更好。 由于AI发展，人类在21世纪可能创造出新的种姓制度，分化出两种截然不同的阶级：高高在上、因为AI革命而获利丰厚的超级富豪，以及对自身处境毫无能力改变的亿万黎民。 Recent history has shown us just how fragile our political institutions and social fabric can be in the face of disruptive change. If we allow AI economics to run their natural course, the geopolitical tumult of recent years will look like child’s play. 近代史告诉我们，在严重不均的情况之下，我们的政治体制和社会结构有多么脆弱。如果我们容许AI经济自由发展，我担心，历史上的动乱相较于AI时代的破坏力量，也不过是场小演习而已。 On a personal and psychological level, the wounds could be even deeper. Society has trained most of us to tie our personal worth to the pursuit of work and success. In the coming years, people will watch algorithms and robots easily outmaneuver them at tasks they’ve spent a lifetime mastering. I fear that this will lead to a crushing feeling of futility and obsolescence. At worst, it will lead people to question their own worth and what it means to be human. 甚至，在个人和心理层面，AI革命造成的创伤也会更深刻。社会训练我们将个人价值和工作与成就紧密结合在一起，在接下来的数年间，当人们陆续看到算法和机器人那么简单就能把工作做得又快又好，但自己可是花了一辈子的时间才学会这些赚钱技能，那会是什么样的感觉？我想，可能会觉得自己非常没用，感觉被时代淘汰吧！最糟的是，这可能会导致人们怀疑自我价值，纳闷活着到底有什么意思？ So what can be done? 针对这种隐忧，我们能做什么？ This grim vision is shared by many technologists in Silicon Valley, and it has sent them casting about for solutions. As the architects and profiteers of the AI age, they feel a mix of genuine social responsibility and fear of being targeted when the pitchforks come out. In their rush for a quick fix, many of the techno-elite have seized on the idea of a universal basic income: an unconditional, government-provided cash stipend to allow every citizen to meet their basic needs. 硅谷很多科技专家早就预见这种未来情景，并且设想可能的解方。这些一手打造出AI时代，并且从中收割巨大利益的科技精英们，感觉自己有必要「做点什么」来改善可能的劣况。这一部分是出于社会责任，一部分是害怕当社会真的变得动荡不安时，自己可能会成为众矢之的。 I can see the appeal. UBI is exactly what Silicon Valley entrepreneurs love: an elegant technical solution to tangled social problems. UBI can be the magic wand that lets them wish away the messy complexities of human psychology and get back to building the technologies that “make the world a better place,” while making them rich. It’s an approach that maps well onto how they tend to view society: as a collection of “users” rather than as citizens, customers and human beings. 这些硅谷精英提出的许多技术性解方，大多偏向迅速修正，其中有一项最受热议的提案是「全民基本收入」（Universal Basic Income, UBI），核心概念很简单：每个国民（或成年人）定期领取政府发放的津贴，以支应日常的基本需求，不用任何附带条件。 我了解硅谷精英为何如此醉心于这个点子，因为这是一种简单、优雅的技术解决方案，或许有助于应付他们创造出来的庞大、复杂社会问题。或许，UBI会是一支神奇的「魔杖」，简单挥个几下，就能消除他们在AI时代创造出来的种种经济、社会与心理冲击，他们也能够安心地继续发展各种最新科技，「让世界成为一个更美好的地方」，同时大量累积财富。这种技术性解方符合他们看待社会的方式：把整个社会看成一大群「用户」、不是「公民」，是一大群「顾客」、不是「个人」。 We can do better. Some form of guaranteed income may indeed be necessary, but if we allow such support to be the endgame, we will miss the opportunity presented by this transformative technology. Instead of simply falling back on an economic painkiller like a universal basic income, we should use the economic bounty generated by AI to double down on what separates us from machines: human empathy and love. 但我们能做到的，当然更好。某种形式的基本保障收入有必要，但如果我们错把手段当成目的，也会错失这项科技带来的转型良机。 我们不该让UBI沦为一剂止痛药，用来麻痹被AI相关技术伤害的人们，而是应该善用AI创造出来的经济红利，加倍下注于人类和机器不同的地方，那就是人类的同理心与爱人的能力。 Such a revolution in how we relate to work will require a rethink from all corners of society. In the private sector, instead of simply viewing AI as a means for cost-cutting through automation, businesses can create new jobs by seeking out symbiosis between AI optimizations and the human touch. This will be especially powerful in areas such as health care and education, where AI can produce crucial insights but only humans can deliver them with care and compassion. 要如何重新定义自我价值与工作的关系，这件事着实不易，需要社会整体努力。科技公司引领AI革命，所以我认为，科技公司也应该主导创造更人性化的新就业机会。其中，有些机会会自然运作而生，有些则必须靠人为创造，要花费一番努力。 Beyond the private sector, governments across the world need to start thinking now about how to use the riches generated by AI to rewrite the social contract and reorient our economies to promoting human flourishing. 企业不应只是把AI当成节省成本的利器，应该设法推动结合人类与机器能力的共生机制，创造一些可让人类与AI安然共存的新工作机会，尤其是保健照护和教育等领域的工作。在这些领域，AI可以通过海量数据产生最优化的结果，人类则扮演表达关怀与同理心的重要角色。 面对眼前前所未有的挑战，各国政府当然也需要开始思考，如何运用AI创造出来的财富改写社会契约，引领经济做好必要转型，以促进人类在未来的繁荣发展。 At the center of this vision, I would suggest, there needs to be what I call the Social Investment Stipend, a respectable government salary for those who devote their time to three categories of activities: care work, community service and education. These activities would form the pillars of a new social contract, rewarding socially beneficial activities just as we now reward economically productive activities. The idea is simple: to inject more ambition, pride and dignity into work focused on enhancing our communities. 为了达到这样的愿景，我提议的是「社会贡献薪资」（social investment stipend）这样的方案。这些薪资由政府支付，凡是投资时间和精力在让社会变得更仁慈、和善、更有创意的活动的人，政府就会支付一笔还算不错的报酬。 这些活动主要可以分成三大类：照护工作、小区服务和教育，它们将形成一种新社会契约的支柱，就像我们现在奖酬高经济生产力的活动，这些活动重视、奖酬的是高度对社会有益的活动。背后的核心概念很简单：为专注于改善社群的工作，注入更多抱负、自尊和尊严。 Care work could include parenting or home schooling of young children, assisting aging parents or helping a friend with mental or physical disabilities live life to the full. Service work would focus on much of the current work of nonprofit and volunteer groups: leading after-school programs, guiding tours at parks or collecting oral histories from elders in our communities. Supported education activities could range from professional training for the jobs of the AI age to taking classes that turn a hobby into a career. 照护工作可能包括：在家里养育年幼的孩子、照料年迈父母、帮助照顾生病的亲友，或是帮助提升身心障碍者的生活质量等。服务工作的定义很广泛，涵盖了现在很多非营利组织的工作，还有我在台湾看到的那些志工所从事的活动，包括环境修复、课后辅导、公园导游、整理小区老人的口述史等。在教育这个工作类别，涵盖了从AI时代的专业工作训练，到把爱好变成职业的训练课程。 The participation requirements of the stipend wouldn’t be designed to dictate the lives of citizens. There would be a wide enough range of choices for all workers who have been displaced by AI. The more people-oriented could opt for care work, the ambitious could enroll in high-tech training, and others could take up community-service work. 我要特别强调一点，规定领取社会贡献薪资的人从事这些活动，并不是要强制他们的日常活动。人类的美在于多元性，每个人都有不同的背景、技能、兴趣和个人特质，我不是主张要用一些狭窄的社会活动，靠着指挥控制制度来扼杀这种多元性。而且我相信，会有足够广泛的选择，可以让所有被AI取代的人找到合适的工作。喜欢和人亲近的人，可以选择照护工作；比较有抱负一点的人，就可以去上高科技职训课程；受到社会理想激励的人，就可以选择小区服务或倡议类的工作。 By requiring some social contribution to receive the stipend, we would foster a public philosophy far different from the laissez-faire individualism of universal basic income. Providing a stipend in exchange for participation in community-building activities carries a clear message: Collective effort from people across society allowed us to reach this point of economic abundance, and now we must use that abundance to recommit ourselves to one another and to our humanity. 之所以必须对社会做出贡献才能领取薪资，是为了培养与UBI自由放任个人主义明显不同的意识形态。参与社会活动可以领取薪资，强化了一个明确的信息：我们之所以能够达到目前这样的经济富裕，靠的是社会上无数人的努力；现在，我们共同利用这些富裕，对彼此做出贡献，加强人际之间的关爱和连结，这些都是我们身为人类的特别之处。 Many difficult questions remain to be answered, of course, before we could consider implementing such a sweeping and idealistic policy. The urgency to create, and the ability to pay for, a far-reaching Social Investment Stipend will depend on the pace and nature of AI’s economic impact. But the humanistic values it embodies can serve as a guide while we navigate the treacherous waters that lie ahead. We may yet be able to harness the full potential of both machines that think and humans who love. 要实施这种社会方案，当然会引发一连串的问题与摩擦，很多问题有待解答，而且要等到AI大量普及后，才可能会知道答案。我们究竟会多快实践「社会贡献薪资」、有多少能力给付、能够推广得多深入，都要看AI革命对各国经济造成冲击的速度和程度。 不过，它背后隐藏的人性价值，是我们在惶惶穿越前方深不可测的漆黑水域时的一盏明灯。如果提前做好准备、善用各种应对方案，或许我们可以结合机器思考和人类爱人的双重能力，真正发挥出最大潜能。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Stanford-seminar]]></title>
    <url>%2F2018%2F11%2F29%2FStanford-seminar%2F</url>
    <content type="text"><![CDATA[attention elmo bert qanet]]></content>
  </entry>
  <entry>
    <title><![CDATA[ict-seminar]]></title>
    <url>%2F2018%2F11%2F29%2Fict-seminar%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[CCF-student]]></title>
    <url>%2F2018%2F11%2F27%2FCCF-student%2F</url>
    <content type="text"><![CDATA[ccf1]]></content>
  </entry>
  <entry>
    <title><![CDATA[ict-intern]]></title>
    <url>%2F2018%2F11%2F25%2Fict-intern%2F</url>
    <content type="text"><![CDATA[text Generation surveyGeneration问题的本质是什么Non-linguistic input (logical forms, database entries, etc.) or text ⇒ Text Seq2Seq [Sutskever, Vinyals, Le 2014]VAE [Kingma &amp; Welling 2014]GAN[Goodfellow+ 2014]ACL [Mooney 2015] seq2seq最大的问题：information bottleneck vae cvae gan An LSTM is not enough三个方面：1.有助于unsupervised training比如LM, AE, GAN 2.输入是additional conditioning inputs，就带有semantic信息 3.可以根据任务设计loss function（超越mle，如rl），以及评价方式 MLE的问题：dull, generic ，repetitive, and short-sighted Exposure bias、loss mismatch cdCD 给每个词、短语之间的交互 一个分数 使得lstm可解释性 具有identifying phrases of varying sentiment, and extracting meaningful word (or variable) interactions的能力 意思是说我们这个操作在word-level importance（）之上，能更好的理解LSTM 你说LSTM也是种发现特征之间的非线性交互，但是我们无法描述这种交互 lstm判断词的情感 相关工作说是有四条线去理解这个黑匣子1.计算词级别的重要性分数（3个baseline），但忽略了interactions Cell decomposition (Murdoch &amp; Szlam, 2017) Leave one out (Li et al., 2016) Integrated gradients (Sundararajan et al., 2017) 2.基于移动，识别出位置，缺点是不具有可解释性 3.基于分解，像cnn去算像素之间的interactions 4.Attention，虽然效果好，但是是间接的，也不能表现interactions cd分解c和h，是由当前词贡献的 和 其他词贡献的 可以分别对这两部分做softmax，对当前词，就有了它对于情感分类的贡献 Explaining Character-Aware Neural Networks for Word-Level Prediction:摘要想去知道到底lstm学到的是一种什么样的pattern，就是把cd也放到cnn上面了。词性标注任务，还是去证明lstm学习到了什么样的语意信息 一个贡献是拓展到textcnn，另一个是发现学到的词性标注的pattern和语言学家的规则是一致的 cd分解为来自一个词里面每个特定的字母 和 这个词中的的所有其他的字母 cnn分别分解卷积层、 激活函数、max-pooling 操作 本质上都是句子表示之后再softmax多分类]]></content>
  </entry>
  <entry>
    <title><![CDATA[暑期实习失败原因]]></title>
    <url>%2F2018%2F11%2F08%2F%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E5%A4%B1%E8%B4%A5%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[杂想 1.进组前一定要详细了解每个正式学生的工作方向，确实一个最有潜力和match的 2.别人的idea永远是别人的，自己的才是自己的 3.把事情想的太简单了，浮躁 4.做research不是为了paper而paper 5.读博士更重要是个人能力的提高，而不是越读越窄。同一个问题不同人来解决，效果是不一样的 6.对于未来，肯定是有新的需求，要拥抱变化，但是做research解决问题的那一套是不太会变的 7.灌水大家都心知肚明，安安心心踏踏实实的做工作，把work做漂亮一些，思考问题更深刻一些，透彻一些 8.做实验正确思路是从底到上，从理论角度验证然后实验得到好的效果，而不是本末倒置。research的根本目的是认识世界，优化技术]]></content>
  </entry>
  <entry>
    <title><![CDATA[CCL2018]]></title>
    <url>%2F2018%2F10%2F20%2FCCL2018%2F</url>
    <content type="text"><![CDATA[ccl how to do good researchgoals:metricswhat you care about (每个人不同) 观众个人经历如何成功：1.解决重要问题 2.把问题卖给需要的人 表达形式（组织）重于内容 演讲目的、组织结构逻辑1234等、如何去表达 重要的话说三遍 paper面向数学和算法，不同的取舍，不同面向对象 只要不要超过200个词 10-30个ref 易于理解、表达，别人能下载、复现的工作更会流传 2主要去攻克一个大的问题，然后去想其子问题 从其他领域获取灵感 秘密武器 如何去获取idea：问题易于理解、方法易于描述 beginning researcher–》junior–》senior reading：catch-up writing：什么问题、尝试了什么有没有用、graph去描述、下个方向的可能性 ask：带着问题去参加会议或者讲座（包括你最近遇到的问题） 实验过程：记录bug paperamr图有cross的问题非投影弧 amr：基于依存结构的句法 就是构建了一个数据集 amr 单根有向无环图方法：基于图和基于转移 词向量的evaluationembedd中的类比、类推关系 翻译集成2finetune 九歌先生成再检索（句向量+最大公共子序列） 法律多关键词抽取就是摘要，曾道建 摘要很普通的hire+pointer 黄学东英中翻译： transformer based dual leanring增大数据量（中文到英文，再英文到中文） 推敲网络 方向一致性 数据选择、filtering 模型融合 前沿研讨孙微微naacl bestelmo：太好用了 之前的word2vec工作，如果用多层lstm，把每一层的vector都拼到一块，而不是最后的一层 传统说句法分析好，但是现在更注重数据，传统的句法结构，是否还有用？ –1.如何结合，2.nn是如何去获取这些语意信息的 句法分析+self attention。emnlp best传统的任务， 把传统解析树+self attention结合起来 实验在跨领域的评测上鲁棒性很强 acl best qa-srl标注角色，采用之前的方式，不需要专业背景（标注者），所以构建大规模数据简单 transfer leanring、multitask leanring(acl2017 keynote)数据很重要！！张家俊数据驱动的机器翻译 四个假设：文本翻译、句子为level、自左往右、需要双语对 最核心是学习一个映射函数（source–target） transformer之后，没有发展！！ 映射函数今年没有什么发展！！ 对比acl文章，其实没啥区别，从题目上看，但是独有的是： 2017把句法信息放进去domain adaptation，2018UNsupervised、document 那么新的： 语音翻译(acl2018 liuyang)、用到前（后）n个句子的信息（如何对document level去评价）、并行解码(iclr2018)、领域使用(coling2018 a survey)、无监督翻译 学习词与词的映射关系(emnlp2018 best) dl在nlp遇到瓶颈了！！ jiajia多模态情感交互之后是什么 高质量数据最重要 2018关键词：多模态、弱监督（海量数据标注很少）、多任务、attention 1.多模态直接concat文本信息和cnn表示的图形信息 2.个性化 15年融入真实信息、16年融入社交网络、17年做group、18年做融合 抑郁（depression detection ijcai2018） 总结：目标：social good，带给人更好的感受 用心理学的维度来标定情感、dimensional的方式来标定（三维的点表示很多的词）、而不是简单地分类 冯 文本生成文本到文本、数据到文本、图像到文本 最重要的：seq2seq、copy、attention 1、生成和抽取相结合 fast 2.与任务结合更紧密 commonsence映射到知识库、动态和静态的attention 结构语意知识，结构化数据生成、 分割的方式代替卷积平扫 discourse-aware（naacl2018） 3.风格 加入可控因子（语意和风格拆开） 学习不同风格的decoder 4.借鉴人类模式 多次编辑（推敲网络） xiayance、前后向网络、自动确定解码层数 粗粒度、细粒度 泛化 关系抽取kb四件需要做的事情：知识获取+知识融合+知识补充+知识推理 知识补充：现有的知识图谱不完备：链接预测方法：表示学习。 1.翻译模型transe 2.语意模型 数据：fb15k-237 问题：挖掘必要属性：obligatory attributes 挖掘计数量词（counting quantifiers） 知识融合任务：实体对奇、本体匹配 知识推理目标：基于表示学习的规则推理 对话技术及任务 检索–》翻译–》生成 从匹配到生成的时代 2018sigir conversation Recommend 数据和评测： yelp数据，推荐式对话 mrc要素：document、question、candicate、answer 从13年开始算，这个任务，2016squad开始火起来 词云：；wordsift.org]]></content>
  </entry>
  <entry>
    <title><![CDATA[how to be a good phd]]></title>
    <url>%2F2018%2F10%2F20%2Fhow-to-be-a-good-phd%2F</url>
    <content type="text"><![CDATA[学生研讨会 刘知远选题和文献综述 学术研究是一个系统工程文献阅读+沟通能力（导师、师兄、同辈）+坚持不懈耐心+代码能力+数据分析+英语能力 主要是讲最开始如何去开始这个问题，学术研究需要天时地利人和， 重要的问题+新颖的方法+努力、积累、坚持（主观能动性） 博士不同时期不同的追求：解决开放问题（填坑）—-》相关领域的专长—–》引领工作（挖坑） 研究方向的选择：兴趣爱好、社会需求、个人特长的交集 研究建议正视自己的特点：人和人之间有差异性 第一项工作的时候：负责模型的具体设计和实现，师兄负责选题、技术路线、论文写作 首次完成一个工作之后，可以在选题上承担更多责任，从而得到全面锻炼 迅速进入研究实战状态！！！ 科研是高优先级 坚持积极主动的态度，积极交流，充分利用实验室资源 如何查阅文献如果想到一个idea，第一步，粗粒度的文献调研，再就是google scholar，顶会 题目越短的文章越需要读 arxiv订阅！ 看论文顺序：题目、摘要、导论、结果、模型、参考文献 如何选题如何找问题：颠覆性思考 哪里人少去哪里？ 读paper读paper读paper读paper读paper 熟知学术界动态 富有远见 做好不认可的准备（读博之后） 注意：：语言学相关领域文章？？？？ nips和icml的新的方法！！！！！！ 博士多份工作穿起来，长期考虑 问题：新瓶新酒、如何看待和师兄沟通中的对自己安排的过快或者过慢（和其本身性格有关），以及老师不亲自带 邱希鹏写作为什么要写一流高手提问题，二流高手解问题，三流抄问题 写作以读者为中心，逻辑清晰，易于复现 大部分审稿人5分钟：标题、摘要、introduction、related work、conclusion 所以要在abstract就要说服审稿人 写作时间轴问题–》挑战–》方法–》贡献 具体写法摘要：概括工作，不要说细节，只说贡献和解决的问题，外行也看懂，不要有公式 介绍：一页半，再说所做工作的必要性和重要性，一定要逻辑性非常强，不要有废话，充分体现逻辑性，不要跟摘要重复，妙用转折、烘托，一定要用起承转合 一定要看adverarial multi- learing for Chinese word segmentation 方法：可以先介绍背景知识（baseline），从而有利于对比baseline和的introduction里面论证进行解释，必要的问题形式定义，图 where不缩进 实验：贡献的证据、扎实：大量、可靠：实验设计层层递进、有逻辑性 不辞辛劳、做到极致！！ 数据集介绍、超参设定。辅助实验说明为什么要用这个超参 在caption里面说描述，而不是正文，更易于去找 别人会关心的问题：比如速度、收敛速度，比如错误分析，为什么好，好在哪里，什么情况下好。 相关工作：为读者梳理历史脉络，而不是贬低 ，参看文献的罗列一定要描述完善。 对比是为了表现你的创新性，向审稿人说明你对本领域的深刻把握 标题：问题+方法+贡献 tikz！！、gnuplot、metapost 语法：wwww.grammarly.com 好好用google 万小军 如何看待困难和挫折答案：重在坚持、努力克服 因素环境、能力、投入、运气 环境老板水平、实验室氛围、硬件 能力调研能力：选题 学术思维能力：博士生要培养自己的学术思维 代码能力：这是必备技能 沟通能力：不要一个人闷着，寻找帮助 写作表达：论文 投入时间、效率、思想 运气课题的选择 实验结果 论文投稿 ！！！！ 夏令营的时候一定要吹王选 不能做一个idea就废掉、做一个idea就废掉！！！！！！！！！！ 林衍凯必须要面临选择 热门和冷门 一定要自己感兴趣。即将成为热门 压力是肯定的，要坚持， motivation太强了 论文提前一个月写完，你不是超人 学术报告简介、模型、模型、模型、实验、结论 为什么做报告1.抓住眼光，重要的前移2.模型描述有所取舍3.用流程图等可视化4.考虑对齐等设计模式 simon：how to give a great talk]]></content>
  </entry>
  <entry>
    <title><![CDATA[qintao-rl]]></title>
    <url>%2F2018%2F10%2F19%2Fqintao-rl%2F</url>
    <content type="text"><![CDATA[强化学习-秦涛 总体环境交互 不断试错达到目标 basic是一个闭环的系统，那什么时候可以用？ 1、action是二阶效应：输出会影响数据的采样 2、长期的reward要设置得当 时间上对齐的马氏性，仅与状态相关，马尔科夫决策过程（mdp） 通过把reward 的discounted累加之后去学习使之最大的policy q是一种E:取期望，因为有随机性 那就是在使得max的q下的action 总结：model、policy、value有图 经典rl1.mdp过程环境的已知的，不需要采样，问题很简单，state-value(action-value) function，去找最优的policy：bellman optimality equation ①：policy evaluation：value或者policy ②：optimal contral 2.mdp过程环境的未知的，只有不断采样去学习（不知先验信息），分为model-free和model-based，即不管系统和先学reward怎么给化作第一种已知环境。 ①：蒙特卡洛采样：就像下围棋，累加reward然后平均。是model-free，只关注output，期望等于均值 temporal-difference learning：每走一步就观测一次(update)，就更快。以猜来使得猜更新，但是方差会变大 总而言之，mc找一条链走到头，td每次都去尝试（variance小，但是一定要一门四龙-greedy），dp是全部去试 ②去estimate模型：但是状态负责度很高，就不好用，因为太难去估计 drl对state抽取特征，不是枚举s，function approximation去做近似，很多种近似的方法 policy gradient ，推导比较麻烦 ①model-free1，value based学习function 2，policy based 3，actor-critic为了降低variance ②model basederror 挑战算法内部波动性很大 不同种子点效果不同 不同任务表现不同 到底要用多少样本才能学好 multi task]]></content>
  </entry>
  <entry>
    <title><![CDATA[GroupStudy-Classification and NN]]></title>
    <url>%2F2018%2F10%2F18%2FGroupStudy-Classification-and-NN%2F</url>
    <content type="text"><![CDATA[我用心准备了，即便没意义 This lecture introduced这节课介绍了根据上下文预测单词分类的问题，与常见神经网络课程套路不同，以间隔最大化为目标函数，推导了对权值矩阵和词向量的梯度；初步展示了与传统机器学习方法不一样的风格。 （word Window classification就是对语义的vector做分类） 1、Classification分类这个任务以及词向量在分类上的应用 Updating both the weight parameters and the word vectors Window classification（窗口（上下文）分类） 2、Cross-entropy loss（交叉熵误差推导）、Max-margin loss Cross-entropy: H(p,q) = H(p)+KL(p||q) 3、Back propagation for a single layer neural networkBP: applying the chain rule and reusing derivative calculations（单层神经网络、最大间隔损失(一种新的损失)和反向传播） 上课2 Overview Today课程概述什么是分类 一般情况下我们会有一个训练模型用的样本数据集 x是输入数据，比如：单词（所以或者向量）、上下文窗口、句子、文档 通过“更新”词向量内容去做分类任务，更新的是一些真实的信号，这里的类别：比如类别：情感、命名实体、买/卖 词向量有个下游任务：窗口分类 交叉啥和softmax的连接 然后是nn，从这里开始开始这节课为什么叫deep learning and nlp 对习题一、二都有帮助 我们今天会涉及到很多数学知识，也是第一次涉及到nn 3 Classifica6on setup and notation 分类任务的定义看这个基本的定义和数学符号 有 输入 和 输出label（one-hot向量） 传统上认为是去找一个边界将数据集分开，比如说一个逻辑回归分类，要训练的就是W这个参数：使用比如逻辑回归分类2维词向量，得到线性决策边界。 一般的ML方法：假设x是确定的，训练逻辑回归只修改参数W，值改变决策边界 当然目的是最后预测x 4 Classifica6on intui6on输出就是一个标签，但是比如机器翻译就是一个词 分类是一个另类的回归 i表示整个数据集，没有i表示一个数据 5 sofrmax算法细节因为后面要求导 所以要清楚概念 softmax定义 总而言之，整理分为两步，y是类别，W的每一行为每一类的参数，该行每个去乘以每个x得到一个向量，再归一化，使得每个类相加=1 第一步：1、算第y类，就取权值矩阵W的第y行的每个元素乘以x的每个元素 之后累加. 第二步：2.归一化得到softmax函数的概率 6 softmax和交叉熵误差由于是分类任务，所以希望输出的是概率最大的类别（最大化正确类别的概率），即argmax，最大化正确类别y的概率。 所以我们希望模型能够最大化概率==最大化对数概率==最小化对数概率的负数，就变为最小化为负log概率： （作用：映射到（0,1）区间内，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标） 称之为交叉熵误差 7 为什么是交叉熵误差(交叉熵定义）其实这个损失函数等效于交叉熵： （公式推导） 假设一个真实的概率分布为：正确为1，错误为0。所以类别是one-hot向量。 由于p是一个one-hot向量，只有当左边是一个真实标签的负log概率。 p是真实的事实，q是softmax之后计算的 所以只有真实那一项非0，并不是求和复杂 (SVM只选自己喜欢的男神，Softmax把所有备胎全部拉出来评分，最后还归一化一下) 当我们对分类的Loss进行改进的时候，我们要通过梯度下降，每次优化一个step大小的梯度，这个时候我们就要求Loss对每个权重矩阵的偏导，然后应用链式法则。那么这个过程的第一步，就是对softmax求导传回去，不用着急，我后面会举例子非常详细的说明。在这个过程中，你会发现用了softmax函数之后，梯度求导过程非常非常方便！ 举个例子说明一下上面两个slide： 定义选到y_i的概率是P=（e^(f_y_i)） / 求和e^j 然后求loss对每个权重矩阵的偏导，用链式法则 直接真正结果那一维减1，把偏导回传就行 所以交叉熵目的是最小化这两个分布的kl散度 8 kl散度：最小化两个分布之间的kl散度交叉熵可以重新写成熵和KL散度两个分布：H(p,q)=H(p)+D_KL 因为H（P）是0，如果在求梯度时没有贡献，最小化上面等式，就是最小化KL散度的p和q。 KL散度不是一个分布，具有非对称性，但是是一种测量两个概率分布p和q差异的方法。 在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。） 9 全数据集上的分类J=对所有正确类别的概率负对数求和 所以最终我们是想最小化这个J 接下来有很多公式推导 10 正则化刚刚只是讨论了目标函数的前半部分，往往后面还有一个正则化项 斯塔是参数，就是鼓励权重尽可能小，接近于0 防止参数爆炸 一般的ML问题中，参数由权值矩阵的列组成维度不会太大。而在词向量或其他深度学习中，需要同时学习权值矩阵和词向量。参数一多，就容易过拟合 为了鼓励权重尽可能小，防止过拟合 图很重要x是很多变量，y是误差，模型越强某时候会出现过拟合 其实就是变得平滑 添加正则项来防止过拟合是机器学习中很常见的方式。在此正则也无特殊之处： （视频中，称这个图是ML学习过程中最重要的图之一。。） 红线是test error，蓝线是training error，横轴是模型复杂度或迭代次数。直线是方差偏差均衡点。 11 细节:传统ml优化方法一般的ML问题中，参数由权值矩阵的列组成维度不会太大。而在词向量或其他深度学习中，需要同时学习权值矩阵和词向量。参数一多，就容易过拟合： 因为不止权重 还有词向量的维度 比如300维*1w个词 12 分类不同的词向量参数量=类别*词向量维度 由于词向量也要学习，可以反向传播到词向量 将词向量作为总体目标函数的一部分来去训练 13 新练词向量使失去泛化因为同时需要同时学习权值矩阵和词向量，所以基于词向量的分类问题参数拟合，还会造成re-training词向量失去泛化效果。 比如：对电影评论数据情感分析训练逻辑回归单词在训练数据中有“TV” and “telly”，在测试数据中有“television”，在于训练词向量中他们是相似的单词。（来自于已经训练的词向量模型） 14 当我们重新训练了词向量会发生什么？1）在训练集中的单词会被重新安排到合适的位置2）在已经训练的词向量模型中但是不在训练集中的单词将保留在原来的位置对于上例, “TV”和”telly”会被重新安排，而”television”则保留在原位，尴尬的事情就发生了： 训练数据中的数据运动了，预训练的词没有出现在训练数据中。这个例子说明，如果任务的语料非常小，则不必在任务语料上重新训练词向量，否则会导致词向量过拟合。 于是在测试集上导致television被误分类。 15 Take home message:这个例子说明： 启示：当我们的训练数据集很小，我们不能训练词向量，会出现过拟合，失去泛化能力。如果数据量很大，训练应该就会得到很好的词向量结果。 所以可以直接用google训练好的word2vec，如果测试集词少 但是如果词多，就要重新训练 16 词向量相关术语1、词向量矩阵L也叫lookup table（d * V维）。2、词向量=词嵌入=词表示3、主要方法有word2vec、Glove。(张乾 包慧语) Word vectors = word embeddings = word representations (mostly) 4、这样的就表示为词的特征。L=d*V d维 V个词5、新方向（课程后）：character models 17 Window classification这是一种根据上下文给单个单词分类的任务，可以用于消歧或命名实体分类。上下文Window的向量可以通过拼接所有窗口中的词向量得到： 这是一个列向量。 1、分类一个单词很少去做。 2、关注的问题就像：上下文出现的歧义。（消歧） 18 Window classification比如，以为一个四分类为例子：人名地面组织 非 19 Window classifica6on这是当时第一个文本分类 针对2.3.1中提到的问题，windows classification的思路为：将对一个单词进行分类的问题扩展到对其临近词和上下文窗口进行分类，就是对窗口中的单词打上标签同时把它前后的单词向量进行拼接然后训练一个分类器 3、想法：分类一个在上下文窗口中的词。（命名实体识别） 4、在上下文中分类一个词很可能存在，比如：在窗口中平均每一个单词但是可能失去了位置信息。 5、通过给中心词设置一个标签来训练softmax分类器，并把他周围的词向量连接起来。 本节向量是列向量 x是5d维向量 20 Simplest window classifier: Softmax5个词向量拼接放进softmax 公式跟之前一样，yhat表示正确的 step1.目的是预测P(y|x)，用softmax分类器 step2.跟之前一样，使用交叉熵loss 注意：softmax中的W*x就是交叉熵里的f_y_i 怎么更新词向量呢？ 21 更新词向量我们可以多次求导 定义变量， yhat是归一化之后的得分输出 t是onehot向量，目标分布 f是一个矩阵乘法 22 更新词向量J对x求导，注意这里的x指的是窗口所有单词的词向量拼接向量 变量相乘， 提示1：仔细定义变量和跟踪它们的维度 链式法则， 提示2：懂得链式法则(chain rule)并且记住在哪些变量中含有其他变量 23 更新词向量变量相乘， 提示1：仔细定义变量和跟踪它们的维度 链式法则， 提示2：懂得链式法则(chain rule)并且记住在哪些变量中含有其他变量 提示3：对于softmax中求导的部分：首先对当c=y(正确的类别）求导，然后对当（其他所有非正确类别）求导 24 更新词向量 提示4：当你尝试对f中的一个元素求导时，试试能不能在最后获得一个梯度包含的所有的偏导数 提示5：为了你之后的处理不会发疯，想象你所有的结果处理都是向量之间的操作，所以你应该定义一些新的，单索引结构的向量 谁是谁的参数，在链式法则里面，f_y即是y的也是x的函数 分两种情况，c是正确的和错误的 25 更新词向量 提示6：当你开始使用链式法则时，首先进行显示的求和（符号），然后再考虑偏导数，例如 or 的偏导数 提示7：为了避免之后更复杂的函数（形式），确保知道变量的维度，同时将其简化为矩阵符号运算形式 提示8：如果你觉得公式不清晰的话，把它写成完整的加和形式 正确的-1 错误的不动 step3。J对x求导，注意这里的x指的是窗口所有单词的词向量拼接向量。 step4.于是就可以更新词向量了： 26 更新词向量行向量装置，求和项变成内积 求和项是一定可以重写成向量的 27 更新词向量梯度的维度？5d 注意：如果维度不相等，那就是有bug 28 Updating concatenated word vectors接下来只需要求导就好了（对求导，注意这里的指的是窗口所有单词的词向量拼接向量。）。 更新的计算词向量每个元素的梯度 拆分window，变成每个词向量 29 更新词向量step5.另一方面，对W求偏导数 将W和词向量的偏导数写到一起 现在缺少的是关于权重的偏导 再加上刚刚词向量的偏导 30 实现的细节有两个比较复杂度高的运算， 在softmax中有两个代价昂贵的运算: 矩阵运算 f = Wx 和 exp指数运算 for不好，矩阵乘法更好 在做同样的数学运算时for循环永远没有矩阵运算有效 基础知识小课堂:在softmax中有两个代价昂贵的运算: 矩阵运算 f = Wx 和 exp指数运算在做同样的数学运算时for循环永远没有矩阵运算有效,遍历词向量 VS 将它们拼接为一个大的矩阵 然后分别和softmax的权重矩阵相乘 31 代码遍历词向量 VS 将它们拼接为一个大的矩阵 然后分别和softmax的权重矩阵相乘 遍历词向量，而不是拼接成一个大的 是不好的 结果证明矩阵相乘更有效矩阵运算更优雅更棒应该更多的去测试你的代码速度 32 代码33 Softmax (= logistic regression) alone not very powerful（效果有限）softmax方法仅限于较小的数据集，能够提供一个勉强的线性分类决策边界。 34 Softmax 效果有限1、softmax只是在原始空间上得到一个线性分类边界。2、小数据集上有一个好的效果。3、大数据集效果有限。 35 Neural Nets for the Win!window classification在少量的数据上（正则化）效果会不错，在大量的数据上效果会有限，softmax仅仅给出线性决策边界举例： 但是神经网络能够提供非线性的决策边界： 于是我们使用神经网络 非线性好很多 36 转折从逻辑disi回归到nn 37 Demystifying neural networks一些简单的介绍 有输入 偏置 激活函数 输出 如果你了解softmax的运行机制，那你就已经了解了一个基本的神经元的运行机制例子：一个神经元就是一个基础的运算单位，拥有n(3)个输入和一个输出，参数是W, b 每个神经元是一个二分类逻辑斯谛回归单元 38 每个神经元是一个二分类逻辑斯谛回归单元：一个神经网络等价于同时运行了很多逻辑回归单元，神经网络同时运行多个逻辑斯谛回归，但不需要提前指定它们具体预测什么 基本上你可以把它看做成一个二元逻辑回归单元 看内部结构 有权重跟输入相乘，加上偏置 激活函数，就是使之接近于1的非常高的概率 这里是sigmoid，映射到0-1之间 39 神经网络同时运行多个逻辑斯谛回归，但不需要提前指定它们具体预测什么：把输入向量喂给这些小的逻辑回归函数和神经元，就有了输出 然后我们就有了多层神经网络 40 神经网络同时运行多个逻辑斯谛回归，但不需要提前指定它们具体预测什么： 如果我们给一批逻辑回归函数一堆输入向量，我们就得到了一批输出向量… ,这些输出又可以作为其他逻辑回归函数的输入 我们把预测结果喂给下一级逻辑斯谛回归单元，由损失函数自动决定它们预测什么： 41 神经网络同时运行多个逻辑斯谛回归，但不需要提前指定它们具体预测什么：所以多层神经网络就是这样一些节点的连接 42 每一层的矩阵这些连接都可以用矩阵相乘的简单形式去描述 神经网络中单层的矩阵符号表示我们有激活函数： a1= a2= 表示成矩阵符号形式: 中间变量（链式法则的时候需要去表示） z=Wx+b a=f(z) w的行数是神经元个数，列数是输入x的维度 f（z）也是个向量 其中f应用的是element-wise规则(也就是点乘）:f([z1,z2,z3])=[f(z1),f(z2),f(z3)] 43 为什么非线性为什么需要非线性的f?没有非线性函数，深度神经网络相对于线性变换价值不大,其他的层次会被编译压缩为单个的线性变换:,有了更多的层次，它们可以逼近更复杂的函数 因为线性系统所有层等效于一层： 而非线性模型可以捕捉很复杂的数据： (隐层越多，越拟合) 44 neural net window classifier回到窗口分类器, 一个更牛的窗口分类器 单个（神经网络）层是一个线性层（函数）和非线性函数的组合 输入和输出之间就多了隐藏层 定义一个单层神经网络 输入x是多个词向量的拼接 a是最终分类层的输入 默认softmax分类器 45 A Single Layer Neural Network46 Feed-forward Computa6on得到每个窗口，然后计算得分，使得中心词是ner位置的窗口能得到高分 以简单三层神经网络为例： 这种红点图经常在论文里看到，大致代表单元数；中间的空格分隔开一组神经元，比如隐藏层单元数为2 \times 4。U是隐藏层到class的权值矩阵： 一个简单的网络： 这种红点图经常在论文里看到，大致代表单元数；中间的空格分隔开一组神经元，比如隐藏层单元数为2×4 U是隐藏层到class的权值矩阵，其中a是激活函数： x是所有词的拼接 维度：每个词4维，x是20维，隐藏层有8个单元，w维度是8行20列，U是列向量 47 extra layer隐藏层作用是学习不同输入词之间的非线性相互作用 48 The max-margin loss怎么设计目标函数呢，记s_c代表误分类样本的得分，s表示正确分类样本的得分。则朴素的思路是最大化(s−s_c) 或最小化 (s_c−s)。但有种方法只计算s_c&gt;s⇒(s_c−s)&gt;0时的错误，也就是说我们只要求正确分类的得分高于错误分类的得分即可，并不要求错误分类的得分多么多么小。这得到间隔最大化目标函数, 可以调整其他参数使得该间隔为1： 这实际上是将函数间隔转换为几何间隔，类似于svm 很强大损失函数，比sofatx更rubest， 本质是让正确窗口的得分更大，错误窗口得分更小，直到足够好（参数为1） sgd softmax只是分开了，这里是使得间隔更大 另外，这个目标函数的好处是，随着训练的进行，可以忽略越来越多的实例，而只专注于那些难分类的实例。 49 Max-margin Objec6ve func6on错误是s_c（负样本），通常通过负采样算法得到负例。 50 Training with Backpropaga6onJ&gt;0 随机初始化 51 Backpropagation这些参数U b W x s关于U的导数是a 一些定义 52 Training with Backpropaga6on把他们放在一起，就可以得到一个更复杂的矩阵表达 w_ij只出现在隐层的第i个激活层 看图，三维输入，两个隐层单元，一个最后的得分U 如果去求W_23的导数，只有a2需要 53 Training with Backpropaga6on意思就是，如果要求W_23，只用考虑a的第i个元素，不用考虑整个内积 U是常量，把u拿出来 第二行公式：把a_i换成f（z），设导数 z_i定义在右边 上面都是链式法则 54 Training with Backpropaga6on现在对W_ij来做 最后就有了表达式 因为只用了下标i，所以简化记号，而x是输入 55 Training with Backpropaga6on现在想得到整个式子的导数 用derta来乘以x的转置，作为外积 W是2*3维，derta二维， 56 Training with Backpropaga6on求导最后一项是b_i 导数就是derta 57 Training with Backpropaga6on这就完成了，只需要求导和链式法则 用更高层的值去算底层的导数 58 Training with Backpropaga6on对得分求导，把词向量拼接 对多个激活单元进行求导（比如右边公式的两个） 就得到了derta乘以W_ij 是与内积相关的 反向传播是用之前计算的值的导数 因为有求和，求和是内积的第j项，点乘在第一项后面。取出一类，作为一个列向量，然后做点乘得到答案 并行计算所有参数的 只更新当前窗口的词 59 Pu[ng all gradients together:结合一个x的参数变成整个，就是w转置乘以derta 放在一起，max是目标函数，所有有灯饰，1或者0，正确和错误x x_c derta是第k层的误差 δ(k)=f′(z(k))∘(W(k)Tδ(k+1) δ(k)=f′(z(k))∘(W(k)Tδ(k+1) 算梯度：相乘就行 60 Summary 通过一个三层神经网络计算这个窗口向量的得分：s = score(museums in Paris are amazing) 间隔最大化目标函数]]></content>
  </entry>
  <entry>
    <title><![CDATA[lawchallenge]]></title>
    <url>%2F2018%2F10%2F12%2Flawchallenge%2F</url>
    <content type="text"><![CDATA[第一届法研杯 汉王任务一罪名预处理：分词、数值替换！！！！、tf-idf for SVM、wordembedding for CNN、数值上采样 传统做法：9个svm(子采样)做投票 cnn多分类：如果单类别就是onehot，双类别就是0000.50000.5000，三类别就是000 0.3 00000.3 0000.3。这使得累加都是1 第二种cnn,架构不一样 三种模型做集成 注意：attention很重要，dl优于传统，svm速度很快，重点：把差异大的模型继承是很好的做法 中电28所任务三刑期model1：dpcnn、model2：fmcnn 早起multi task还行 后来就不行了 安徽省高院多分类，也是传统+dl融合的方式 预处理：停用词、金额、重量、酒精浓度、地区、时间、当事人姓名都用不同的大写字母去代替，意思是这些命名实体对最后结果影响非常大 传统：tf-idf做特征选择（10w），模型：线性svc、labelpowerset、rakelD 三种方法来解决多标签：问题转化(lp)、改编算法、集成方法，来转化为单标签问题 问题转化：二元关联、分类器链、标签powerset 有个所用模型：。。。。。。。注意看ppt 改编算法：knn的多标签版本mlknn 集成：rakelo、rakeld：大标签集分成一定数目小标签集，使用label powerset训练相应的分类器，最后投票 西电样本少的类比设置高的权重 jieba分词 有没有易混淆的：抢劫、抢夺，加入要素维度，利用fine-tuning 训练易混淆模型：正则 多模型融合：textcnn、textrnn 小的技巧、数据分析、详细的实验记录（想好做哪些尝试） 达观数据ali模型hir句子、离散特征用fm、法条embed 法条预测：nilinear做一个相关性匹配、sigmoid loss 罪名预测：attr classifier 级联的方式：hard、soft 三个loss 刑期预测:先做分类再做回归，mae、huber loss(介于mae-mse之间) 一个五个loss做multi task 调优elmo真的有效果 找到一些性能比较相似的embed去融合，因为可能是不同角度去描述 fewshot：人为设定属性 bilinear做异构数据拉到同一个空间 国双任务1、2绝对是一起训练的 业务规则特征就是一些数字的不同有不同意义 用邴立东的ram，dpcnn，rcnn 注意模型细节！！！不同结构非常重要]]></content>
  </entry>
  <entry>
    <title><![CDATA[Summarization]]></title>
    <url>%2F2018%2F10%2F03%2FSummarization%2F</url>
    <content type="text"><![CDATA[一些摘要paper的笔记 startSequence to Sequence Learning with Neural Networks1.使用4层LSTM，每层1000个单元–&gt;长句表现好 2.第一个在encode将输入句子用lstm编码成一个固定的维度的向量(cnn无序列化顺序) 3.将input sentence逆序输入可以明显改善LSTM模型，说是减小“minimal time lag” Neural Machine Translation by Jointly Learning to Align and Translate用encoder的所有hidden state的加权平均来表示context，权重表示decoder中各state与encoder各state的相关性，原始seq2seq认为decoder中每一个state都与input的全部信息（用last state表示）有关，而这篇文章则认为只与相关的state有关系，即在decoder部分中，模型只将注意力放在了相关的部分，对其他部分注意很少。 context vector是一个加权平均，权重用了一个最简单的mlp来计算，然后归一化。这里的权重反应了decoder中的state s(i-1)和encoder中的state h(j)之间的相关性。这篇文章在encoder部分采用了BiRNN。 在机器翻译中，attention model可以理解为source和target words的soft alignment，一个很经典的图,越亮的地方表示source和target中的words相关性越强（或者说对齐地越准），图中的每一个点的亮度就是前面计算出的权重 虽然在文章中并没有用到Attention这个词，但其实就是这篇文章提出的attention model Empirical evaluation of gated recurrent neural networks on sequence modeling这篇是虽然不是第一个提出gru的，但是系统的对比了lstm和gru。传统RNN 有两个主要的问题: 梯度消失, 长期记忆急速衰减。LSTM 不会每次都重写 memory，而是可以通过 input/forget gate 在需要的时候尽量地保留原来的 memory，LSTM/GRU 中额外增加的 cell state，让它们能记住较早之前的某些特定输入，同时让误差反向传播时不会衰减地太快 实验后， LSTM/GRU 在收敛速度和最后的结果上，都要比经典 RNN 要好，但 LSTM 和 GRU 在不同的数据集和任务上虽然互有优劣但差异不大，具体使用 LSTM 还是 GRU 视情况而定。但是gru参数少 Sequence-to-sequence rnns for text summarization1、Encoder-Decoder with Attention：Encoder是一个双向GRU-RNN，Decoder是一个单向GRU-RNN，两个RNN的隐藏层大小相同，注意力模型应用在Encoder的hidden state上，一个softmax分类器应用在Decoder的生成器上。没有啥特别的 2、Large Vocabulary Trick：large vocabulary trick(LVT)技术到文本摘要问题上。每个mini batch中decoder的词汇表受制于encoder的词汇表，decoder词汇表中的词由一定数量的高频词构成。重点解决的是由于decoder词汇表过大而造成softmax层的计算瓶颈。 比较合适摘要，因为摘要中的很多词都是来自于原文之中。所以将decoder的词汇表做了约束，降低了decoder词汇表规模，加速了训练。 3、Vocabulary expansion：为了生成新颖的有意义的词，扩展LVT词表，将原文中所有单词的一度最邻近单词扩充到词汇表中，最邻近的单词在词向量空间中cosine相似度来计算 4、Feature-rich Encoder：设计了一些的features，比如：词性，命名实体标签，单词的TF和IDF。将features融入到了word embedding上，对于原文中的每个单词构建一个融合多features的word embedding，而decoder部分，仍采用原来的word embedding。 5、Switching Generator/Pointer：由于word embedding对低频词的处理并不友好，所以用decoder/pointer机制来解决这个问题。模型中decoder带有一个开关，如果开关状态是打开generator，则生成一个单词；如果是关闭，decoder则生成一个原文单词位置的指针，然后拷贝到摘要中，挺像copy 6、Hierarchical Encoder with Hieratchical Attention：关键词和关键句子都很重要，所以用两个双向RNN来捕捉这一个是word-level，一个是sentence-level，在两个层次上都使用注意力模型 A neural attention model for abstractive sentence summarization很经典的ABS，之后都作为Baseline 论文尝试了三种 encoder 的方式，分别是 Bag-of-Words，CNN 和 Attention-Based，结果 enc1 &lt; enc2 &lt; enc3。另外，还尝试了与 extractive 结合，就有了 ABS+。即在每次解码出一个词的时候，不仅考虑神经网络对当前词的预测概率 logp，还要开个窗口，去找一找当前窗口内的词是否在原文中出现过，如果有的话，概率会变大。相当于加了个feature吧，当然ABS+ 的效果好于 ABS 目标函数是negative log likelihood，使用mini-batch SGD优化 DUC-2004: Rouge-1:26.55/Rouge-2:7.06/Rouge-L:22.05 Gigaword: Rouge-1:30.88/Rouge-2:12.65/Rouge-L:28.34 Neural headline generation on abstract meaning representationbaseline是朴素的使用了 encoder-decoder去做生成式(ABS)。 由于Abstract meaning representation(AMR图) 具有高度的结构化句法和语义信息，如何融入nn，得到这样一个attention-based AMR encoder-decoder model ，结果也就提升了一个点 Pointer networks是attention (decode生成y时，需要计算X1到Xn对生成y的贡献) 的一个变体 之前的翻译模型无法解决某类问题：在decode时，若source sentence长度是变化的(增多)，则target classes也是变化的（增多)。也可以描述为seq2seq输出严重依赖输入 而Ptr-Nets在输入序列中就挑出一个member作为输出，避免了这个问题。不像attetion将source通过encoder变成context vector，而是将attention转化为一个pointer，来选择原来输入序列中的元素 经典的attention： Ptr-Net中的attention：Pointer Net没有上面传统attention的最后一个公式(将权重关系a和隐式状态整合为context vector)，而是直接进行通过softmax，指向输入序列选择中最有可能是输出的元素。 然后使用得到的softmax结果去拷贝encoder对应的输入元素作为的decoder输入的向量。 （本身是解决旅行商、凸包等，nips2015），显而易见，如果是抽取式摘要直接sentence labeling就行，如果是生成式，就是下面的pointer generator这篇，这种混合的模型能够从原文中直接复制词语，因而可以提高摘要的准确率并处理OOV词语，同时还保留下了生成新词语的能力 总而言之，Ptr-Net解决了seq2seq模型中output dictionary大小固定的问题，并且是一种用attention把copy机制带入生成模型中的方法 copynetcopy机制是对于seq2seq(attention)的decoder时候的，具体来说有两点改变：1.decoder的状态更新会加一个位置信息进去，因为copy的时候需要知道那个词在source里面的位置(hidden state)，当然如果不在source里面，位置信息=0 2.另一个是，输出是个generate-code和copy-mode(从input sentence 中选词)的混合模型,即最终词的概率p是两种算p的方法的和，分别对应两种处理表示source sentence的向量来算p的方法：attentive read 和 selective read。 attentive read 和 selective read都是根据词在不同的集合(vocab或者source sentence的交、补集)有不同的打分函数，其中有参数需要学习。比如target不在source里面p(c)就是0，若target仅在source里面p(g)就是0 Get To The Point: Summarization with Pointer-Generator Networks解决三个问题：：产生不准确的事实类细节、生成重复的词，以及不能很好的处理OOV 既可以通过pointing直接从原文中copy单词又保留了通过generator生成新词的能力；另外，用coverage来记录已经总结出的内容，防止重复 即是模型=pointergenerator+coverage=seq2seq+attention+copy+coverage Sequence-to-sequence attentional model biLSTM作为encoder，decoder hidden state被用来计算注意力分布(source中的词的概率分布),用来计算hidden state的加权和作为新的t时间的context vector，和decoder state 过两个线性变化之后softmax来计算target 词的概率分布 pointer-generator=Sequence-to-sequence attentional model+copy 重要的是，每一步还计算了一个概率值P_gen，这代表从词表中生成单词的概率。这个P_gen用来给seq2seq 模型softmax后的结果和Ptr-Net产生的来自source词的概率分布进行加权求和 两点说明，一个是，这里copy-mode是直接从attention权重计算的词的分布中去采样。另一个是，感觉p_gen是一种soft的形式，另外它计算方法是(h_t*是source向量+s_t是这一步的decoder state+xt是这一步的decoder的输入就是grandtruth)再sigmoid coverage 这个是为了解决重复输出的问题，在decoder的每一步，更新一个coverage vector，是之前所有time step的decoder时attention权重的直接累加，可以来记录model已经关注过source中哪些词（也就是目前的生成已经覆盖到哪些词），并让这个向量影响attention权重的计算 同时在算loss的时候，摘要任务里要加一个coverage loss惩罚关注同一位置(相同的coverage)才行 疑问：为什么在Get To The Point看结果里面extractive方法还比pointer-generator-copy的rouge得分更高？answer：新闻类重要的信息就是集中在前几句 Neural summarization by extracting sentences and words把任务定义为分为sentence和word两个level的summarization。sentence level是一个序列标签问题，每个句子有0或1两个标签，为1表示需要提取该句作为总结。而word level则是一个限定词典规模下的生成问题，词典规模限定为原文档中所有出现的词。 首先在encoder端将document分为word和sentence来encode，word使用CNN encode得到句子表示，接着将句子表示输入RNN得到encoder端隐藏层状态。从word到sentence的encode体现了本文的hierarchical document encoder的概念。 在decoder端根据任务的不同使用不同网络结构，sentence任务就是一个简单的有监督下二分类问题，使用RNN网络结构更新decoder端隐藏层状态， decoder端隐藏层状态串联encoder端隐藏层状态后接入一个MLP层再接sigmoid激活函数得到句子是否被extract的概率。 word任务则是用传统的attention-based的方法来计算每个词的概率。但要注意本文的计算的attention不是word-level attention，而是encoder端sentence-level attention。 应该是之前都是seq2seq在abstractive summarization上，本文想解决seq2seq用在extractive summarization。（之前是对每个句子计算一个分数，然后分类这种） SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents这是一个两层RNN。 最下面是词的输入，第一层是词级别的双向GRU，用来建模句子表示。第二层的每个句子的隐层各自做average pooling作为各自句子的表示。包括2类RNN结果，一个forward，一个backward，是双向的RNN 第二层是句子级别的双向GRU，使用word level layer的输出结果hidden state representation串联(用[]表示)起来作为本层的input。得到隐层再做average pooling就能够得到文档的表示。 最后利用文档的表示来帮助我们依次对句子做分类。 (abstractive summary引入到extractive model中作为训练数据) 预测的时候，遍历每个句子，使用二分类的方法判断这个句子需不需要加入摘要里面去，对应于一个logistic layer，最后分类层的公式如下： content意思是与hidden输出有关，salience就是看文档和摘要关系，novely是看摘要和句子s_j的关系。其中s_j是到达第j个句子的已经生成的部分摘要的表示，d是文章的表示。 Wh表示第j个句子的信息， hWd计算的是当前的句子和文章表示的相似度，-hW*tanh(s)表示的是当前的句子能够带来多少“新”的信息，接下来的三项分别表示绝对位置，相对位置和偏置。所以文中说他自己可解释性很强。 最后Loss采用的负对数似然。在最终选取摘要的时候不是简单的分类，而是根据每个句子的概率高低排序，选择概率最高的前几句即可。 注意两个事情：一个是，abstractive仅用于训练，即在extractive的基础上，利用摘要的信息，来优化extractive的模型，后面test的时候也是extractive的方式 另一个是，对于SummaRuNNer的loss function中真实数据的label的产生是用去每次从所有句子中选出一个句子，让该句子加入摘要中，就直到能够使ROUGE值增加得最多，一直选择，直到ROUGE值不变或者减少的时候停止。 Selective Encoding for Abstractive Sentence Summarization一种选择性编码模型。模型包含了一个句子encoder、选择门网络和带注意力decoder。 Encoder是一个BiGRU，Selective编码选择层是将词的h_i与句子的s拼接到一起，放到一个前馈网络里来生成输出h’_i。文中说是s=[h←_1, h→_n]就代表整个句子。h←_1表示从右到左读取了整个句子, h→_n表示从左到右读取了整个句子。总体来说，编码选择层来对编码信息进行过滤，通过控制从编码器到解码器的信息流来构建额外的一层信息表示，该层表示为摘要构建了一种特殊的语义表示。 Decoder的不同在于maxout。GRU使用s_t-1, c_t-1, y_t-1更新s_t；s_t+h_i计算e_i然后归一化得到权重α_i，乘以h’_i得到context向量c_t，和s_t、y_t-1一起放到一个maxout层(k=2)中得到output，然后使用softmax。这个maxout层比较特殊，相当于不同层网络之间有2套互相独立的权重参数，输出z的时候选一个能让z大的参数。这里encoder使用了BiGRU，decoder得到的输出是2d，使用k=2的maxout合并相邻的两个数值，将输出降为d维（其实并不知道为什么这么做）。 不过效果还可以 Gigaword(Rush et al., 2015): Rouge-1:36.15/Rouge-2:17.54/Rouge-L:33.63 Gigaword(ours): Rouge-1:46.86/Rouge-2:24.58/Rouge-L:43.53(sounds something strange??? why so high?) DUC2004: Rouge-1:29.21/Rouge-2:9.56/Rouge-L:25.51 Ranking Sentences for Extractive Summarization with Reinforcement Learning在之前的cnn+seq2seq（ConvS2S），引入结合主题模型的注意力机制 如何引入：结合多步注意力机制和带偏置生成机制的方法，将主题信息整合进了自动摘要模型中 强化学习：在 ConvS2S 的训练优化中使用了 self-critical，以ROUGE 来直接做reward，说是有助于缓解曝光偏差问题（exposure bias issue）：由于只将一个模型暴露于训练数据的分布而不是其自身的分布中。在训练过程中，模型由groundtruth来预测下一个单词，而在推理过程中，它们会生成下一个单词，并将预测的单词作为输入。因此，在测试过程中，每个步骤的错误会累积并导致性能的恶化 self-critical：根据输入序列x生成两个输出序列，第一个序列是通过贪婪地选择最大化输出概率分布的词来获得的，而另一个输出序列是通过从分布中取样生成的。在获得了两种序列的ROUGE之后，计算差值，然后最小化了损失：-差值*logp，由此可以直接优化离散的评估指标 什么是topic信息：topic model是一种传统的用于发现抽象主题思想或隐藏语义的统计模型。其实这样做相当于主题模型来获取文档的隐含知识，相当于增大先验知识。 A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization总而言之就是Conv Seq2seq + LDA + Reinforcement Learning。这个图上源序列的单词和主题的嵌入是由相关的卷积块（左下方和右下方）编码的。然后，通过计算解码器表示（左上角）和文字/主题编码器表示的点积来共同关注单词和主题。最后，通过一个有偏差的概率生成机制来生成目标序列。 跟上一篇非常类似，将topic信息引入到ConvS2S模型中并使用 self-critical 强化学习训练方法（SCST）来进行优化。引入词语和主题信息，加入多步注意力机制 1、Position Embedding: 每个词的输入=词向量+位置向量。就是embedding的时候加一个位置信息e=（w1+p1,…,wm+pm） 2、Gated Linear Unit: encoder decoder都是通过叠加几个卷积层来构建的。每层的卷积操作做个线性变化到2d维，输出重写成[A; B]，然后两边残差连接喂入下一层。使用门控线性单元（GLU） 3、Multi-Step Attention: 先对隐层状态做个embedding再计算权重,此处跟上一篇一样 4、Topic Embedding: 对于每个主题，抽取出前N个词出来构成词表K，预训练得到topic embedding。对于输入中的每个词，如果在K中，则使用topic embedding，否则使用word embedding。 吉布斯抽样技术的经典LDA方法，对主题嵌入初始化的语料库进行预训练，并为有偏差的概率生成过程提供候选方案。嵌入值被规范化为一个平均值为0和0.1的分布。这篇论文选取了最高的N=200个单词，在每个主题中获得了主题词集的最高概率。 5、Joint Attention: 在Topic-aware Conv过程中，计算Attention权重时，除了要计算当前decoder隐层状态与每个encoder输出的点积，还要计算当前decoder隐层状态与input ebmedding中每个encoder输出的点积，再求和并归一化作为权重。 6、Biased Probability Generation:其中有个线性变化，有点复杂 7、Reinforcement Learning: 策略和上一篇一模一样，λ_RL=0.99 Gigawords: Rouge-1:36.92/Rouge-2:18.29/Rouge-L:34.58 DUC2004: Rouge-1:31.15/Rouge-2:10.85/Rouge-L:27.68 LCSTS(word): Rouge-1:39.93/Rouge-2:33.08/Rouge-L:42.68 A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss本文提出在分层编码解码模型的基础上利用分层attention机制将抽取式与生成式结合。并提出了不一致性损失函数将两者结合提高生成质量。 提出一个抽取式和提取式融合的模型来做 summarization，利用抽取式模型对句子的重要程度打分，获得 sentence-level 的 attention 进而影响生成式对每个词的 word-level 的 attention权重；提出 inconsistency loss； 因为单纯的抽取式rouge高但是很不流利，生成式模型可读性更高且较为简洁，但是在某些事实细节方面往往不够准确。 CNN/Daily Mail 数据集 ROUGE 分数超过抽取式模型 lead-3，本文的模型可看作是 pointer-generator 和抽取式模型的融合； 用Hierarchical 的结构（word-level encoding 和 sentence-level encoding），分别对 sentence 和word 做 attention；sentence 的 attention 权重使用 sigmoid；word 的 attention 权重计算时用 sentence-level 的 attention 权重进行 scale； Inconsistency Loss 对 decode 每个 step 的 topK attention word 的 word-level 和 sentence-level 的 attention 乘积做 negative log；鼓励 word-level attention sharp，sentence-level 的 attention high。 抽取式和生成式方法可以作为两个独立步骤训练，也可以采取端到端的方式进行训练，分开训练时，抽取式部分直接将信息量较大的句子输出，生成式部分只接受部分原始句子作为输入；端到端训练时，抽取式方法对句子进行打分，生成式部分会在句子打分的基础上，更新对每个词的attention权值。在端到端训练时，生成式部分权值较大的词可能不在抽取式权值较大的句子里面，因此文章提出了不一致损失来约束该情况。 只超越了lead3… 5.loss有4个部分，抽取、生成、coverage、inc（最重要） Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting 1、用cnn做句子level编码 2、把cnn的输出作为一个bilstm的输入，去学习一个Hierarchical的更强的句子表示 3、Sentence Selection：用一个pointer作为抽取式，去抽重要句子（曾被选过，则为-无穷大） 4、Abstractor Network：改写器是 encoder-aligner-decoder + copynet（就是get the point） 快: parallel decoding 5、强化学习仅调整Extractor参数，不调整Abstractor参数，避免生成的句子可读性差，同样使用Policy Gradient学习算法。直接用rouge作为reward。两次离散化 6.几个trick：伪标签算rouge最大，extract部分用eoe表示让抽取停止，beam search抽句子（防止ngram重复太大） 性能 CNN/Daily Mail: Rouge-1:40.88/Rouge-2:17.80/Rouge-L:38.54 Neural Document Summarization by Jointly Learning to Score and Select Sentences又是将句子排序和选择联合在一个端到端模型里，这篇文章joint就在于把当前权重最大的句子的隐层状态作为decode下一个time step的输入，这样相当于同时拿文章信息和之前已经抽取句子的信息再次对其他句子进行attention操作，这样让抽取句子的信息起到一个抑制作用]]></content>
  </entry>
  <entry>
    <title><![CDATA[download-scp]]></title>
    <url>%2F2018%2F10%2F02%2Fdownload-scp%2F</url>
    <content type="text"><![CDATA[1.1tar -zcvf 58.tar.gz xsum-raw-downloads1 --remove-files 2.1split -b 570M -d -a 1 54.tar.gz 54.tar.gz. 3.1scp -P 2333 54.tar.gz.1 fzx@s.eecser.com:~/ 4. crtl+z 5.1bg %1 6.1disown -h %1 7.1cat 56.tar.gz.* | tar -zxv 8.1find ./xsum-raw-downloads2/ -type f -name &apos;*.html&apos; -exec mv &#123;&#125; ~/XSum-Dataset/xsum-raw-downloads/. \;]]></content>
  </entry>
  <entry>
    <title><![CDATA[挑战杯-智能新闻]]></title>
    <url>%2F2018%2F07%2F13%2F%E6%8C%91%E6%88%98%E6%9D%AF-%E6%99%BA%E8%83%BD%E6%96%B0%E9%97%BB%2F</url>
    <content type="text"><![CDATA[2018第三学期 headline generation新闻标题生成“Automatic headline generation is an important research area within text summarization and sentence compression. “ A Neural Attention Model for Abstractive Sentence Summarization 2015This paper wants to max P(y_i+1|x,y_c;θ), tries three encoder methods: Bag-of-Words，CNN, Attention-Based.And the decoder is NNLM. code for ABS Controlling Output Length in Neural Encoder-Decoders EMNLP2016Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, more important, this model is able to control the length of the summarization text by feeding to the Seq2seq base model a label that indicates the intended output length in addition to the source input 是一种加label的idea,uses a mechanism to control the summary length by considering the length embedding vector as the input Neural Headline Generation on Abstract Meaning Representation EMNLP2016This paper utilize encoder-decoder neural networks for generating abstractive summaries. Abstract meaning representation is utilized to incorporate syntactic and semantic information of input sentence into the headline generation model. Neural Headline Generation with Minimum Risk Training 2016This paper proposes a minimum risk training method to directly optimize the evaluation metrics and the results show that optimizing for ROUGE improves the test performance. Neural Headline Generation with Sentence-wise Optimization 2016As traditional neural network utilizes maximum likelihood estimation for parameter optimization, it essentially constrains the expected training objective within word level rather than sentence level.To overcome these drawbacks, this paper employs minimum risk training strategy, which directly optimizes model parameters in sentence level with respect to evaluation metrics and leads to significant improvements for headline generation. Abstractive Sentence Summarization with Attentive Recurrent Neural Networks 2016Seq2seq model with CNN encoder and attention mechanism has achieved good results on abstractive summarization from a single sentence, what’s more, the model has been extended to use recurrent neural network as decoder.This paper reaching a ROUGE-1 score of 35.51 on the Gigaword data. Selective Encoding for Abstractive Sentence Summarization ACL2017This paper is achieved by selectively encoding words as a process of distilling salient information（proposed selective gate to improve the attention in abstractive summarization）, using BiGRU encoders and GRU decoders with selective encoding. In fact, this paper pointed out that there are salient problems in the attention mechanism. Which means, there is no obvious alignment relationship between the source text and the target summary, and the encoder outputs contain noise for the attention. From Neural Sentence Summarization to Headline Generation: A Coarse-to-Fine Approach IJCAI2017This paper proposes a coarse-to-fine approach, which first identifies the important sentences of a document using document summarization techniques, and then exploits a multi-sentence summarization model with hierarchical attention to leverage the important sentences for headline generation. Learning to Explain Ambiguous Headlines of Online News IJCAI2018table2docs 新闻生成Content Selection for Real-time Sports News Construction from Commentary Texts INLG 2017Rather than receiving every piece of text of a sports match before news construction, as in previous related work, they novelly verify the feasibility of a more challenging setting to generate news report on the fly by treating live text input as a stream. This paper designs scoring functions to address different requirements of the task and use stream substitution for sentence selection. Towards Automatic Construction of News Overview Articles by News Synthesis 2017This paper investigates a new task of automatically constructing an overview article from a given set of news articles about a news event. They propose a news synthesis approach to address this task based on passage segmentation, ranking, selection and merging. 还是多文档摘要 Table-to-text Generation by Structure-aware Seq2seq Learning AAAI2018To encode both the content and the structure of a table, this paper proposes a novel structure-aware seq2seq architecture which consists of field-gating encoder and description generator with dual attention. They introduce a modified LSTM that adds a field gate into the LSTM to incorporate the structured data. Further, they use a dual attention mechanism that combines attention of both the slot names and the actual slot content. code]]></content>
  </entry>
  <entry>
    <title><![CDATA[机试准备for 2019]]></title>
    <url>%2F2018%2F07%2F11%2F%E6%9C%BA%E8%AF%95%E5%87%86%E5%A4%87for-2019%2F</url>
    <content type="text"><![CDATA[北大2018机试 2018 aaisA 第n小的质数描述 输入一个正整数n，求第n小的质数。 输入 一个不超过10000的正整数n。 输出 第n小的质数。 样例输入 10 样例输出 29 B潜伏者描述 R国和S国正陷入战火之中，双方都互派间谍，潜入对方内部，伺机行动。 历经艰险后，潜伏于S国的R国间谍小C终于摸清了S国军用密码的编码规则： 1、 S国军方内部欲发送的原信息经过加密后在网络上发送，原信息的内容与加密后所的内容均由大写字母‘A’—‘Z’构成（无空格等其他字母）。 2、 S国对于每个字母规定了对应的“密字”。加密的过程就是将原信息中的所有字母替换为其对应的“密字”。 3、 每个字母只对应一个唯一的“密字”，不同的字母对应不同的“密字”。“密字”可以和原字母相同。 例如，若规定‘A’的密字为‘A’，‘B’的密字为‘C’（其他字母及密字略），则原信息“ABA”被加密为“ACA”。 现在，小C通过内线掌握了S国网络上发送的一条加密信息及其对应的原信息。小C希望能通过这条信息，破译S国的军用密码。小C的破译过程是这样的：扫描原信息，对于原信息中的字母x（代表任一大写字母），找到其在加密信息中的对应大写字母y，并认为在密码里y是x的密字。如此进行下去直到停止于如下的某个状态： 1、 所有信息扫描完毕，‘A’—‘Z’所有26个字母在原信息中均出现过并获得了相应的“密字”。 2、 所有信息扫描完毕，但发现存在某个（或某些）字母在原信息中没有出现。 3、 扫描中发现掌握的信息里有明显的自相矛盾或错误（违反S过密码的编码规则）。例如某条信息“XYZ”被翻译为“ABA”就违反了“不同字母对应不同密字”的规则。 在小C忙得头昏脑胀之际，R国司令部又发来电报，要求他翻译另外一条从S国刚刚截取到的加密信息。现在请你帮助小C：通过内线掌握的信息，尝试破译密码。然后利用破译的密码，翻译电报中的加密信息。 输入 共3行，每行为一个长度在1到100之间的字符串。 第1行为小C掌握的一条加密信息。 第2行为第1行的加密信息所对应的原信息。 第3行为R国司令部要求小C翻译的加密信息。 输入数据保证所有字符串仅由大写字母‘A’—‘Z’构成，且第1行长度与第2行相等。输出 共1行。 若破译密码停止时出现2，3两种情况，请你输出“Failed”（不含引号，注意首字母大写，其它小写）。 否则请输出利用密码翻译电报中加密信息后得到的原信息。样例输入 样例 #1： AA AB EOWIE 样例 #2： QWERTYUIOPLKJHGFDSAZXCVBN ABCDEFGHIJKLMNOPQRSTUVWXY DSLIEWO 样例 #3： MSRTZCJKPFLQYVAWBINXUEDGHOOILSMIJFRCOPPQCEUNYDUMPP YIZSDWAHLNOVFUCERKJXQMGTBPPKOIYKANZWPLLVWMQJFGQYLL FLSO 样例输出 样例 #1： Failed 样例#2： Failed 样例#3： NOIP 提示 输入输出样例1说明：原信息中的字母‘A’和‘B’对应相同的密字，输出“Failed”。 输入输出样例2说明：字母‘Z’在原信息中没有出现，输出“Failed”。 C:逃离迷宫描述 你在一个地下迷宫中找到了宝藏，但是也触发了迷宫机关，导致迷宫将在T分钟后坍塌，为此你需要在T分钟内逃离迷宫，你想知道你能不能逃离迷宫。迷宫是一个边长为m的正方形，其中&quot;S&quot;表示你所在的位置，&quot;E&quot;表示迷宫出口，&quot;.&quot;是可以随意走动的区域，&quot;#&quot;是不可穿行的墙壁，每次你可以耗费1分钟在区域间移动（上下左右四个方向）。 输入 输入包含多组数组，第一行是一个整数K（1 &lt;= K &lt;= 10)，表示有K组数据。接下来每组数组包含整数m(2&lt;=m&lt;=10)和整数T，m表示正方形迷宫的边长，T表示坍塌时间。其后是一个m*m的字符矩阵，包含字符”S”, “E”, “.”和”#”。输出 每组数据输出一行，输出“YES”或者”NO”，表示是否可以在坍塌之前逃离（也就是说移动次数是否可以不超过T）。样例输入 2 4 7 S... ###. .#E. ..#. 3 4 S.. ..# .#E 样例输出 YES NO D:跑步描述 奶牛们打算通过锻炼来培养自己的运动细胞，作为其中的一员，贝茜选择的运动方式是每天进行N(1 &lt;= N &lt;= 10,000)分钟的晨跑。在每分钟的开始，贝茜会选择下一分钟是用来跑步还是休息。 贝茜的体力限制了她跑步的距离。更具体地，如果贝茜选择在第i分钟内跑步，她可以在这一分钟内跑D_i(1 &lt;= D_i &lt;= 1,000)米，并且她的疲劳度会增加 1。不过，无论何时贝茜的疲劳度都不能超过M(1 &lt;= M &lt;= 500)。如果贝茜选择休息，那么她的疲劳度就会每分钟减少1，但她必须休息到疲劳度恢复到0为止。在疲劳度为0时休息的话，疲劳度不会再变动。晨跑开始时，贝茜的疲劳度为0。 还有，在N分钟的锻炼结束时，贝茜的疲劳度也必须恢复到0，否则她将没有足够的精力来对付这一整天中剩下的事情。 请你计算一下，贝茜最多能跑多少米。 输入 第1行: 2个用空格隔开的整数：N 和 M 第2..N+1行: 第i+1为1个整数：D_i输出 第1行: 输出1个整数，表示在满足所有限制条件的情况下，贝茜能跑的最大 距离样例输入 5 2 5 3 4 2 10 样例输出 9 提示 对于30%的数据，N &lt;= 50, M &lt;= 10 E:What time is it?描述 An accutron shows time with four digits, from 0000 to 2359. Every digit is represented by 3*3 characters, including ‘|’s, ‘_’s and blanks. When the LCD screen works well, the digits look like the following: _ _ _ _ _ _ _ _ | | | _| _||_||_ |_ ||_||_| |_| ||_ _| | _||_| ||_| _| There are two accutrons at hand. One shows the accurate time, and the other is 15 minutes late. For example, at 8:25am, the first accutron shows &apos;0825&apos;, while the second shows &apos;0810&apos;. Unfortunately, there is something wrong with the two LCD screens, namely some parts of the digits missed. Your task is to decide the accurate time, according to the fragmental digits showed on the two accutrons. 输入 The first line of the input is a single integer t (1 &lt;= t &lt;= 20), the number of test cases. Each case contains three lines, indicating the time on the accurate accutron and the time on the slow accutron, separated by a blank column. (Please refer to the Sample Input.)输出 For each input, print the accurate time with four digits if it can be ensured, or otherwise the string ‘Not Sure’.样例输入 2 _ _ _ _ _ | _ _|| _ || | _ |_ | | _ |_| _ _ _ _ _ _ ||_ _|| _| || | _ |_ | || |_| 样例输出 Not Sure 0825 F:Sorting It All Out描述 An ascending sorted sequence of distinct values is one in which some form of a less-than operator is used to order the elements from smallest to largest. For example, the sorted sequence A, B, C, D implies that A &lt; B, B &lt; C and C &lt; D. in this problem, we will give you a set of relations of the form A &lt; B and ask you to determine whether a sorted order has been specified or not. 输入 Input consists of multiple problem instances. Each instance starts with a line containing two positive integers n and m. the first value indicated the number of objects to sort, where 2 &lt;= n &lt;= 26. The objects to be sorted will be the first n characters of the uppercase alphabet. The second value m indicates the number of relations of the form A &lt; B which will be given in this problem instance. Next will be m lines, each containing one such relation consisting of three characters: an uppercase letter, the character “&lt;” and a second uppercase letter. No letter will be outside the range of the first n letters of the alphabet. Values of n = m = 0 indicate end of input.输出 For each problem instance, output consists of one line. This line should be one of the following three: Sorted sequence determined after xxx relations: yyy...y. Sorted sequence cannot be determined. Inconsistency found after xxx relations. where xxx is the number of relations processed at the time either a sorted sequence is determined or an inconsistency is found, whichever comes first, and yyy...y is the sorted, ascending sequence. 样例输入 4 6 A&lt;B A&lt;C B&lt;C C&lt;D B&lt;D A&lt;B 3 2 A&lt;B B&lt;A 26 1 A&lt;Z 0 0 样例输出 Sorted sequence determined after 4 relations: ABCD. Inconsistency found after 2 relations. Sorted sequence cannot be determined. G:Rails描述 There is a famous railway station in PopPush City. Country there is incredibly hilly. The station was built in last century. Unfortunately, funds were extremely limited that time. It was possible to establish only a surface track. Moreover, it turned out that the station could be only a dead-end one (see picture) and due to lack of available space it could have only one track. The local tradition is that every train arriving from the direction A continues in the direction B with coaches reorganized in some way. Assume that the train arriving from the direction A has N &lt;= 1000 coaches numbered in increasing order 1, 2, ..., N. The chief for train reorganizations must know whether it is possible to marshal coaches continuing in the direction B so that their order will be a1, a2, ..., aN. Help him and write a program that decides whether it is possible to get the required order of coaches. You can assume that single coaches can be disconnected from the train before they enter the station and that they can move themselves until they are on the track in the direction B. You can also suppose that at any time there can be located as many coaches as necessary in the station. But once a coach has entered the station it cannot return to the track in the direction A and also once it has left the station in the direction B it cannot return back to the station. 输入 The input consists of blocks of lines. Each block except the last describes one train and possibly more requirements for its reorganization. In the first line of the block there is the integer N described above. In each of the next lines of the block there is a permutation of 1, 2, …, N. The last line of the block contains just 0. The last block consists of just one line containing 0. 输出 The output contains the lines corresponding to the lines with permutations in the input. A line of the output contains Yes if it is possible to marshal the coaches in the order required on the corresponding line of the input. Otherwise it contains No. In addition, there is one empty line after the lines corresponding to one block of the input. There is no line in the output corresponding to the last ``null’’ block of the input.样例输入 5 1 2 3 4 5 5 4 1 2 3 0 6 6 5 4 3 2 1 0 0 样例输出 Yes No Yes H:The Rotation Game描述 The rotation game uses a # shaped board, which can hold 24 pieces of square blocks (see Fig.1). The blocks are marked with symbols 1, 2 and 3, with exactly 8 pieces of each kind. Initially, the blocks are placed on the board randomly. Your task is to move the blocks so that the eight blocks placed in the center square have the same symbol marked. There is only one type of valid move, which is to rotate one of the four lines, each consisting of seven blocks. That is, six blocks in the line are moved towards the head by one block and the head block is moved to the end of the line. The eight possible moves are marked with capital letters A to H. Figure 1 illustrates two consecutive moves, move A and move C from some initial configuration. 输入 The input consists of no more than 30 test cases. Each test case has only one line that contains 24 numbers, which are the symbols of the blocks in the initial configuration. The rows of blocks are listed from top to bottom. For each row the blocks are listed from left to right. The numbers are separated by spaces. For example, the first test case in the sample input corresponds to the initial configuration in Fig.1. There are no blank lines between cases. There is a line containing a single 0&#39; after the last test case that ends the input. 输出 For each test case, you must output two lines. The first line contains all the moves needed to reach the final configuration. Each move is a letter, ranging fromA’ to H&#39;, and there should not be any spaces between the letters in the line. If no moves are needed, outputNo moves needed’ instead. In the second line, you must output the symbol of the blocks in the center square after these moves. If there are several possible solutions, you must output the one that uses the least number of moves. If there is still more than one possible solution, you must output the solution that is smallest in dictionary order for the letters of the moves. There is no need to output blank lines between cases.样例输入 1 1 1 1 3 2 3 2 3 1 3 2 2 3 1 2 2 2 3 1 2 1 3 3 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 0 样例输出 AC 2 DDHH 2]]></content>
  </entry>
  <entry>
    <title><![CDATA[socks5-Git代理]]></title>
    <url>%2F2018%2F07%2F04%2Fsocks5-Git%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[全局代理，写入配置‘’’git config –global http.proxy ‘socks5://127.0.0.1:1086’git config –global https.proxy ‘socks5://127.0.0.1:1086’‘’’ 清除配置‘’’git config –global –unset http.proxygit config –global –unset https.proxy‘’’ 临时代理‘’’ALL_PROXY=socks5://127.0.0.1:8888 git clone https://github.com/some/one.git‘’’]]></content>
  </entry>
  <entry>
    <title><![CDATA[暑期工作icst]]></title>
    <url>%2F2018%2F06%2F30%2F%E6%9A%91%E6%9C%9F%E5%B7%A5%E4%BD%9Cicst%2F</url>
    <content type="text"><![CDATA[多轮对话:should have Better objective functions and evaluation metrics 1.Neural Responding Machine for Short-Text Conversation2015较老data: 微博数据, 每句限长140个字。每条微博有平均20条回复，共约22w个微博，也就是约440w条问答对。方法：带attention的seq2seq翻译模型 2.Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models2016 AAAI For me there are two main takeaways. First, the use of a hierarchy of RNNs using one to model the sequence of utterances in the dialogue, and one to model the sequences of tokens in an individual turn. And secondly the value of bootstrapping the model using external data, which makes a significant difference to model performance. utterance是某轮里面的某个人的话，tokens就是word。然后web query任务就是那种百度上搜一个问题他直接给你回复了，我想起来了，这种任务实质就是阅读理解任务。然后boostrap是分为两个，一个是用一个大规模的word embed，可以让他会更多的词，另一个方面就是在另一个非对话数据集上pretrain，让他会说话 2.A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues引入随机隐变量来刻画utterance之间的关系 3.Hierarchical recurrent attention network for response generation. extended the hierarchical structure with the attention mechanism [2] to at- tend to important parts within and among utterances with word level attention and utterance level attention, respectively. 4.Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation5.AN ABSTRACTIVE APPROACH TO QUESTION ANSWERING6.Query and Output: Generating Words by Querying Distributed Word Representations for Paraphrase Generation7.2017AAAITopic Aware Neural Response Generation8.How to Make Context More Useful?An Empirical Study on Context-Aware Neural Conversational Models统计行数方法wc -l 统计轮数方法4244675/492065= 8.6262485647 #AAAI2018对话相关 Dialogue Act Sequence Labeling Using Hierarchical Encoder with CRFIn some approaches, a hierarchical convolutional and recurrent neural encoder model are used to learn utterance representations by processing a whole conversation. The utterance representations are further used to classify DA classes using the conditional random field (CRF) as a linear classifier. However, these models might fail in a dialogue system where one can perceive the past utterances, but cannot see future ones. 任务叫Dialogue Act recognition，就是用rnn把句子表示之后放进crf做序列标注 Eliciting Positive Emotion through Affect-Sensitive Dialogue Response Generation: A Neural Network Approach生成式回复 Augmenting End-to-End Dialogue Systems with Commonsense KnowledgeImproving Variational Encoder-Decoders in Dialogue GenerationVaritional encoder-decoder (VED)已经被广泛应用于对话生成，但与用于编码和解码的强大RNN结构，隐向量分布通常由一个简单的多的模型来近似，导致了KL弥散和难以训练的问题。在本篇论文中，作者将训练过程拆分为两个阶段：第一个阶段负责学习通过自编码(AE)将离散的文本转换为连续的embedding；第二个阶段学习通过重构编码得到的embedding来泛化隐含表示。这样一来，通过单独训练一个VED模型来对高斯噪声进行变化，进而采样得到隐变量，能够得到一个更加灵活的分布。 BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue SystemsPersonalizing a Dialogue System with Transfer Reinforcement LearningIt is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users&apos; data as a source domain and an individual user&apos;s data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose &quot;PETAL&quot;(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting. Elastic Responding Machine for Dialog Generation with Dynamically Mechanism SelectingRUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog SystemsAddressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNsContext Aware Conversational Understanding for Intelligent Agents with a ScreenConversational Model Adaptation via KL Divergence RegularizationTowards a Neural Conversation Model with Diversity Net Using Determinantal Point ProcessesTowards Building Large Scale Multimodal Domain-Aware Conversation SystemsExploring Implicit Feedback for Open Domain Conversation GenerationEmotional Chatting Machine: Emotional Conversation Generation with Internal and External MemoryCustomized Nonlinear Bandits for Online Response Selection in Neural Conversation ModelsA Knowledge-Grounded Neural Conversation Modelpropose a knowledge grounded approach which infuses the output utterance with factual information relevant to the conversational context without slot filling. The neural architecture of the knowledge grounded model which uses a set of external world facts to augment the output utterance generated bt the model. Instead of just having a set of facts to augment the conversation, a richer way could be to use knowledge graphs or commonsense knowledge bases which consist of [entity-relation-entity] triples proposed a knowledge grounded neural conversation model[3], where the research is aiming at combining conversational dialogs with task-oriented knowledge using unstructured data such as Twitter data for conversation and Foursquare data for external knowledge. However, the task is still limited to a restaurant information service, and has not yet been tested with a wide variety of dialog tasks. CoChat: Enabling Bot and Human Collaboration for Task Completion propose a memory-enhanced hierarchical RNN (MemHRNN) to handle the one-shot learning challenges caused by instantly introducing new actions in CoChat. aaai2019related workWhy social bots?• Maximize user engagement by generating enjoyable and more human-like conversations• Help reduce user frustration• Influence dialogue research in general(social bot papers often cited in task-completion dialogue papers) • 2010: Response retrieval system (IR) [Jafarpour+ 10]• 2011: Response generation using Statistical Machine Translation(phrase-based MT) [Ritter+ 11]• 2015: First neural response generation systems (RNN, seq2seq)[Sordoni+ 15; Vinyals &amp; Le 15; Shang+ 15] Similar to sequence models in Neural Machine Translation (NMT), summarization, etc. Uses either RNN, LSTM, GRU, etc.Source:conversation history.Target:response. Blandness problem: cause and remedies Common MLE objective (maximum likelihood)[Li+ 16a]提出Mutual Information for Neural Network Generation(Mutual information objective) consistency problem: Personalized Response Generation [Li+ 2016b] Personal modeling as multi-task learning [Luan+ 17] Improving personalization with multiple losses [Al-Rfou+ 16] Long conversational context problem: It can be challenging for LSTM/GRU to encode very long context (i.e. more than 200 words: [Khandelwal+ 18]) Hierarchical Encoder-Decoder (HRED) Serban+ 16 + conversation (turn by turn)) Hierarchical Latent Variable Encoder-Decoder (VHRED) [Serban+ 17] （Adds a latent variable to the decoder，and Trained by maximizing variational lower-bound on the log-likelihood）which was Related to persona model [Li+ 2016b]:Deals with 1-N problem, but unsupervisedly. Grounded problem: A Knowledge-Grounded Neural Conversation Model [Ghazvininejad+ 17] or Conversations around images e.g.,Q-As [Das+ 16] or chat [Mostafazadeh+ 17] or Grounding: affect [Huber+ 18] DSTC7 Challenge: Knowledge-Grounded Conversation（“Sentence Generation” track (61 registrants as of June) Registration link: http://workshop.colips.org/dstc7/call.html） Emergence of reinforcement learning (RL) for E2E dialogue Tries to promote long-term dialogue success REINFORCE algorithm [Williams+ 92] Reward functions: Ease of answering:-Pr Information flow:-logSigmoidcos Meaningfulness:logP+logP Survey on dialogue datasets [Serban+ 15] Evaluate problem： Human evaluation (crowdsourcing) automatic: Machine-Translation-Based Metrics:BLEU [Papineni+ 02]: ngram overlap metric、NIST [Doddington+ 02]（Seldom used in dialogue, but copes with blandness issue• Considers info gain of each ngram: score(interesting calculation) &gt;&gt; score(of the)）、METEOR（Accounts for synonyms, paraphrases, etc.） Trainable Metric• Towards an automatic turning test [Lowe+ 17]: ADEM: Metric based on hierarchical RNN (VHRED) problem: Dialogue task:“How NOT to evaluate dialogue systems” [Liu+ 16] But same problem even for Translation task[Graham +15] motivation 1.Challenge: The blandness problem 2.Challenge: The consistency problem 3.Challenge: Long conversational context 4.Challenge: Grounded 5.Reward functions: Ease of answering:-Pr Information flow:-logSigmoidcos Meaningfulness:logP+logP 6.Datasets 7.Evaluate in a netshell： MLE causes blandness (mitigated by MMI) Evaluation metrics (BLEU, METEOR, etc.) reliable only on large datasets➡️expensive for optimization (e.g., sequence-level training [Ranzato+ 15]) RL reward functions currently too ad-hoc Open Benchmarks Alexa Challenge (2017-) –Academic competition, 15 sponsored teams in 2017, 8 in 2018 – $250,000 research grant (2018) – Proceedings [Ram+ 17] Dialogue System Technology Challenge (DSTC) (2013-) (formerly Dialogue State Tracking Challenge)Focused this year on grounded conversation: Visual-Scene [Hori +18], background article [Galley +18] Conversational Intelligence Challenge (ConvAI) (2017-)Focused this year on personalized chat (FB Persona-Chat dataset) code旧hred：python hred_main.py –path=./hred_pretrain/ –w2v=./word2vec/word2vec.128d.117k.bin –emb_dim=128 –max_sent=10 –batch=32 –vsize=30000 -seshid=512 –max_word=50 新hred：python hred_main.py –path=./hred_pretrain/ –emb_dim=128 –max_sent=10 –batch=32 –vsize=30000 -seshid=512 –max_word=50 –emtraining –model=GRU –lr_p=5 –w2v=./word2vec/word2vec.128d.117k.bin rl:python train_full_rl.py –path=./saverl_model/ –abs_dir=../fast_rl_init/hred_pretrain –ext_dir=./hred_extractor –lr_p=5 –ckpt_freq=5000 –patience=10]]></content>
  </entry>
  <entry>
    <title><![CDATA[Question Generation]]></title>
    <url>%2F2018%2F03%2F07%2FQuestion-Generation%2F</url>
    <content type="text"><![CDATA[nlpcc2018 paperkbnlpcc2017：Large-Scale Simple Question Generation by Template-Based Seq2seq Learning 方法：Template-based Seq2seq 任务： 语料：nlpcc2017 Chinese KBQA 比较方法： 结论：用the DIVERSE metric衡量多样性 不足： db Question Generation via Overgenerating Transformations and Ranking (Technical report) 方法： 任务： 语料： 比较方法： 结论： 不足： Automation of question generation from sentences Good question!statistical ranking for question generation Question generation from paragraphs at upenn: Qgstec system description Automatically generating questions from queries for community-based question answering How to Generate Cloze Questions from Definitions: A Syntactic Approach Generating natural language questions to support learning on-line Deep questions without deep understanding Leveraging multiple views of text for automatic question generation Revup: Automatic gap-fill question generation from educational texts Towards topic-to-question generation Ranking automatically generated questions using common human queries Generating quiz questions from knowledge graphs Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus Knowledge Questions from Knowledge Graphs Machine Comprehension by Text-to-Text Neural Question Generation Question Generation from a Knowledge Base with Web Exploration On Generating Characteristic-rich Question Sets for QA Evaluation Neural Question Generation from Text: A Preliminary Study Semi-supervised qa with generative domain-adaptive nets 语意解析/教育领域：Leveraging Multiple Views of Text for Automatic Question Generation]]></content>
  </entry>
  <entry>
    <title><![CDATA[nlpcc2018好友推荐评测]]></title>
    <url>%2F2018%2F02%2F26%2Fnlpcc2018%E5%A5%BD%E5%8F%8B%E6%8E%A8%E8%8D%90%E8%AF%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[（会持续更新,作为nlpcc2018的笔记 2.26传统的好友推荐算法通常是基于链接信息(如友谊)：没考虑tag 或用户中存在的内容信息(如标签、帖子或个人资料)：没考虑很多共同好友情况 非负矩阵分解nonegative matrix factorization：给定mn阶矩阵X可以分解为X=W H两个矩阵 （m r 和r n)，可以通过最小化（非凸函数，只能迭代更新求解）目标函数（目标函数是X=W * H的这个等于号，计算方法是 Frobenius 范数或者Kullback-leibler 离散度）来获得 W 和 H eg：例如,基于正则化 NMF 的协同过滤方法 RSNMF、基于联合概率矩阵 分解的推荐算法 UPMF、基于受限约束 NMF 的协 同过滤方法 BNMF以及基于结构投影 NMF 的协同 过滤算法 CF-SPNMF 传统方法基于链接共同好友、基于内容tag、混合方法 基于链接共同好友：用社交网络的拓扑结构去做概率预测 基于内容tag：把一条微博视为一个item,利用协同过滤技术预评分,然后根据其评分高低进行推荐。 混合方法：将结合链接信息和内容信息建立好友推荐 初步baseline初步可以做混合方法，链接关系做一个矩阵，内容特征做一个矩阵，用NMF分类产生n个主题社区,计算每个社区的成对用户的相似性,以生成候选推荐好友列表,然后合并候选人以获取每个目标用户的 Top-K 好友推荐 对于链接矩阵P，有则为1，无关系为0，为对称矩阵，可以做三分解。对于内容，因为目前数据未知，暂时假定内容包含tag、微博和个人资料，用bow提取特征，矩阵每一项是用户所关联词项的 TF/IDF 值，为矩阵Q。目标函数为r(分解P)+（1-r）(分解Q)+正则化。是约束问题，拉格朗日乘数方法最小化使收敛，也可以引入 Karush-Kuhn-Tucker( KKT) 条件的平滑条件变形，得到局部最优解，就是每个主题社区的成员 再对主题社区做好友推荐，相似度也是由用户内容相似度和链接信息特征相似度组成。用贝叶斯算候选好友和目标用户的后验概率，还是用权重r来控制这两个相似度。也可以算聚类指示矩阵做点积 评价指标Precision 是用来测量推荐列表中是否都是用户偏好的好友, Recall 是用来表示推荐列表中是否包含了用户偏好的全部好友 结果横轴top-k（k=0，2，4，6，8，10）或者参数r 纵轴可以做评价指标 问题如何权衡内容 信息、直接信任关系和间接信任关系这三者在推荐系统中所占的比重？另，转发评论点赞等未考虑？计算用户相似度的改进？ 3.6 DUTIR有感2016级硕士 王安然四个用户画像相关比赛的代码：https://github.com/liyumeng/SmpCup2016 2016 ccf 国家电网客户画像 2016 ccf“大数据精准营销中搜狗用户画像挖掘” 2016smp weibo 深圳的中国青年人工智能创新创业大赛中以微博用户画像为主体的“思微（SearchWeibo）-微博可视化分析平台”获得三等奖]]></content>
  </entry>
  <entry>
    <title><![CDATA[manning cs224n]]></title>
    <url>%2F2018%2F02%2F03%2Fmanning-cs224n%2F</url>
    <content type="text"><![CDATA[向manning大神学习如何去把知识讲授给他人，使人理解 1、introduce“The goal of NLP isto be able to design algorithms to allow computers to “understand”natural language in order to perform some task.”一些实际任务，简单的有找关键词、找同义词，难一点的有机器翻译、对话、qa nlp的第一步就是如何去represent word？ —-word vector 为什么要用深度学习？—-“数据量大、gpu、好的算法(…)模型” 基础课程：线代math51、概统cs109、机器学习CS229 or CS221 2、Word Vectors1Word2vec 3、Word Vectors24、NN5、bp6、tf7、Dependency Parsing8、rnn9、lstms10、Seq2Seq with Attention11、Attention and Transformer Networks12、CNN13、Tree RNN and Constituency Parsing14、Coreference Resolution15、Advanced Topics16、rl17、Semi-supervised and Multi-task Learning18、future]]></content>
  </entry>
  <entry>
    <title><![CDATA[五大正则化方法和七大优化策略]]></title>
    <url>%2F2018%2F01%2F20%2F%E4%BA%94%E5%A4%A7%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E5%92%8C%E4%B8%83%E5%A4%A7%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[梯度下降是一种优化技术，它通过最小化代价函数的误差而决定参数的最优值，进而提升网络的性能。但它在处理高度非凸函数和搜索全局最小值时也存在很多局限性。 正则化技术令参数数量多于输入数据量的网络避免产生过拟合现象。正则化通过避免训练完美拟合数据样本的系数而有助于算法的泛化。 正则化正则化技术是保证算法泛化能力的有效工具，因此算法正则化的研究成为机器学习中主要的研究主题。此外，正则化还是训练参数数量大于训练数据集的深度学习模型的关键步骤。正则化可以避免算法过拟合，过拟合通常发生在算法学习的输入数据无法反应真实的分布且存在一些噪声的情况。 数据增强数据增强是提升算法性能、满足深度学习模型对大量数据的需求的重要工具。数据增强通过向训练数据添加转换或扰动来人工增加训练数据集。数据增强技术如水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转通常应用在视觉表象和图像分类中。 L2 正则化（权重衰减）L1 和 L2 正则化是最常用的正则化方法。L1 正则化向目标函数添加正则化项，以减少参数的绝对值总和；而 L2 正则化中，添加正则化项的目的在于减少参数平方的总和。根据之前的研究，L1 正则化中的很多参数向量是稀疏向量，因为很多模型导致参数趋近于 0，因此它常用于特征选择设置中。机器学习中最常用的正则化方法是对权重施加 L2 范数约束。 L1 正则化 L1 正则化在零点不可微，因此权重以趋近于零的常数因子增长。很多神经网络在权重衰减公式中使用一阶步骤来解决非凸 L1 正则化问题 [19]。L1 范数的近似变体是： 在《深度学习》一书中，参数范数惩罚 L2 正则化能让深度学习算法「感知」到具有较高方差的输入 x，因此与输出目标的协方差较小（相对增加方差）的特征权重将会收缩。而 L1 正则化会因为在方向 i 上 J(w; X, y) 对 J(w; X, y) hat 的贡献被抵消而使 w_i 的值变为 0（J(w; X, y) hat 为 J(w; X, y) 加上 L1 正则项）。此外，参数的范数正则化也可以作为约束条件。对于 L2 范数来说，权重会被约束在一个 L2 范数的球体中，而对于 L1 范数，权重将被限制在 L1 所确定的范围内。 DropoutBagging 是通过结合多个模型降低泛化误差的技术，主要的做法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。而 Dropout 可以被认为是集成了大量深层神经网络的 Bagging 方法，因此它提供了一种廉价的 Bagging 集成近似方法，能够训练和评估值数据数量的神经网络。 Dropout 指暂时丢弃一部分神经元及其连接。随机丢弃神经元可以防止过拟合，同时指数级、高效地连接不同网络架构。神经元被丢弃的概率为 1 − p，减少神经元之间的共适应。隐藏层通常以 0.5 的概率丢弃神经元。使用完整网络（每个节点的输出权重为 p）对所有 2^n 个 dropout 神经元的样本平均值进行近似计算。Dropout 显著降低了过拟合，同时通过避免在训练数据上的训练节点提高了算法的学习速度。 Drop ConnectDrop Connect 是另一种减少算法过拟合的正则化策略，是 Dropout 的一般化。在 Drop Connect 的过程中需要将网络架构权重的一个随机选择子集设置为零，取代了在 Dropout 中对每个层随机选择激活函数的子集设置为零的做法。由于每个单元接收来自过去层单元的随机子集的输入，Drop Connect 和 Dropout 都可以获得有限的泛化性能 [22]。Drop Connect 和 Dropout 相似的地方在于它涉及在模型中引入稀疏性，不同之处在于它引入的是权重的稀疏性而不是层的输出向量的稀疏性。 早停早停法可以限制模型最小化代价函数所需的训练迭代次数。早停法通常用于防止训练中过度表达的模型泛化性能差。如果迭代次数太少，算法容易欠拟合（方差较小，偏差较大），而迭代次数太多，算法容易过拟合（方差较大，偏差较小）。早停法通过确定迭代次数解决这个问题，不需要对特定值进行手动设置。 优化技术动量（Momentum）随机梯度下降和小批量梯度下降是机器学习中最常见的优化技术，然而在大规模应用和复杂模型中，算法学习的效率是非常低的。而动量策略旨在加速学习过程，特别是在具有较高曲率的情况下。动量算法利用先前梯度的指数衰减滑动平均值在该方向上进行回退 [26]。该算法引入了变量 v 作为参数在参数空间中持续移动的速度向量，速度一般可以设置为负梯度的指数衰减滑动平均值。对于一个给定需要最小化的代价函数，动量可以表达为： https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650734959&amp;idx=1&amp;sn=717b747cec3fdc7a4b9aa08c5f1195af&amp;chksm=871ac511b06d4c0734c5d66953c66210206693be9a6f7d5346a9c86cc801b3a0559a3700fc8f&amp;mpshare=1&amp;scene=23&amp;srcid=1220XBPoXX3xnDouAigwXqSz#rd]]></content>
  </entry>
  <entry>
    <title><![CDATA[graph representing leanring]]></title>
    <url>%2F2018%2F01%2F10%2Fgraph-representing-leanring%2F</url>
    <content type="text"><![CDATA[grl综述、graphGAN、more grl综述问题定义权重越大，空间中距离越近 用输入来分类：同构:节点和边都只有一种 异构：多模态的图、知识图谱 label是onehot、attribute是离散或者连续 第四种流行学习–降维方法 按输出来分类：节点、边、子图、全图（蛋白质或者分子来比较多个图的相似性 方法来分类：矩阵分解（奇异值、特征值）、random walk（deep walk）、cnn或者autoencoder、自定义损失函数（最大化边重建概率、最小化距离损失函数，常见于kg） 历史：传统降维，le拉普拉斯，最近五年 locally linear embedding：流行学习，高维中有低纬度结构，通过lle降维，做法是算邻居集合，计算低纬度的embedding word2vecskip-gram model：每个词特征可以预测周围的词，概率softmax计算，负采样来防止softmax开销大 deep walk：从图中的每个节点出发随机进行random walk来采样，重复，得到path当成sentence，就可以做word2vec了 node2vec：deepwalk选择下个节点是均匀的，但是node2vec是不均匀的，可以bfs或者dfs，打转或者往深处走 line：自定义两个损失函数：1一阶临界关系 2二阶临接关系 transx：一系列方法，就只是head embed+relation embed=tail embed sdne:autoencoder，输入是点的vector，三项损失，点自身的重建损失+临接的点之间+正则化 graphGANmotivation之前按输入、输出、方法来分类 g生成式模型：如deepwalk d判别式模型：直接去学习两个点之间有边的概率，如sdne、ppne g和d怎么联合？gan中用minimax game来联合g是要接近ture，d是试图判断有无边，minmax操作：对每个点都做（g对真实的点的给分+d对给定g的参数的给分）g：hierarchical sigmod函数d：求导会有麻烦（policy gradient） framework 问题：1、没考虑图的结构特征（把图看成二叉树） 2、负采样不是很好，生成的不符合概率分布 提出 graph softmax1、bfs搜索得到树 2、计算邻居概率（relevence 概率） 3、边计算边sample 实验：1、预测边概率2、节点分类3、推荐 more workdkn：推荐系统shine：预测用户微博情感]]></content>
  </entry>
  <entry>
    <title><![CDATA[numerical methods]]></title>
    <url>%2F2017%2F12%2F26%2Fnumerical-methods%2F</url>
    <content type="text"><![CDATA[看完这本书才挺后悔，明显作者是数学家，写作口吻不是很好，并且年代有些久远 monte carlo用概统来理解蒙特卡洛模拟的结果，为什么是n-1（贝塞尔修正），作用：估计|模拟 一元非线性方程解带余项的taylor定理、牛顿法、拟牛顿法（避免求导数） 最小二乘法选主元、cholesky分解。条件化：范数 分段多项式插值k阶均差 数值微分richardson外推:在导数的近似中得到高阶精度的方法 数值微分newton-cotes公式、复合梯形法则、clenshaw-curtis求积公式 常微分方程组]]></content>
  </entry>
  <entry>
    <title><![CDATA[强化学习做关系抽取和文本分类]]></title>
    <url>%2F2017%2F11%2F22%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%81%9A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E5%92%8C%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[清华冯珺（黄明烈） 强化学习基本对每一个位置都有最佳的policy 关系抽取（一句话包含多个实体关系无法解决） 问题描述其实是关系分类：已经标注出了entity，对两个实体之间的关系分类，监督学习。 但是标记成本很高，现有不带噪音数据集提出distant supervision。问题：标记有噪音可能有问题 传统方法传统：把关系抽取转化成袋子 不足：不知道具体关系，上一个和下一个句子 ##我们方法有两个挑战:数据集中不知道哪些label的正误、training中不是独立过程，互相影响 第一个：通过分类效果好坏来判断选择的子数据集是否好第二个：train and error search，不停的选不同的子数据集 弱监督的结果就相当于reword（平均的likelihood）来更新policy likelihood：通过分类器产生的概率 模型强化学习来选择最好的子数据集 instance seletor来帮助做分类，做分类结果反过来帮助更好选择 （有一个jointy training的过程） instance seletor问题是数据集的traning和test都是自动生成 优化就是用 relation classifier 训练25轮 实验NYT（riedel 2010）三个baseline：一个纯粹cnn不考虑noise，两个bag level（CNN+ATT，CNN+MAX） 总结sentence level去做 只需要弱监督信息 还可以适用于任何有noise的数据集做分类 文本分类 背景句子分类：用sentence representation再做分类 传统方法sentence representation词带、cnn、rnn、att的共同问题都没有考虑句子结构 树lstm：树形结构是给定的 我们希望做到和任务相关的树形的表示，并且问题是不知道做什么任务的时候是什么结构—–&gt;得到结构之后丢进去分类看好坏 model有点复杂 分成三部分 policy把句子结构变成序列，输出policy 基于当前state生成action encode 树形表示根据动作生成句子树形表示 训练:最大化 分类（Cnet）最小化 information distilled LSTM的过程输入每个词，看哪个词比较重要，就保留：输出的动作就是删除或者保留（二分类） retain就是跟传统lstm一样reword加人为限制使它删除词更多 Hierarchically Structured LSTM生成的lstm分为两层 word level 和 phrase level问题：怎么样去分句子，action就是怎么去切分句子 动作：inside或者end(切割或者不切） reword函数跟information distilled LSTM比较像 数据集：MR、SST、Subi、AG 实验结果： 例子无关紧要的词和短语都删了 分析 总结]]></content>
  </entry>
  <entry>
    <title><![CDATA[面经更新]]></title>
    <url>%2F2017%2F11%2F16%2F%E9%9D%A2%E7%BB%8F%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[面试题目 session1 传统机器学习1.SVM全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。 2. LR（建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好） 3.LR和SVM相同：1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 区别：1、LR是参数模型，SVM是非参数模型。2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。 4.核函数session2 神经网络1.tf计算图Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。 session3 数学1欧氏距离or曼哈顿距离欧氏距离曼哈顿距离 session4 trick1.overfittingdropout、regularization、batch normalizatin]]></content>
  </entry>
  <entry>
    <title><![CDATA[知识图谱一些小结]]></title>
    <url>%2F2017%2F11%2F09%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E4%B8%80%E4%BA%9B%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[东南大学漆桂林 人工智能领域里较为传统的一个分支就是知识表示与推理（把知识用符号逻辑表示） 知识图谱的构建（事实性知识和模式知识） 除此之外，问答、推荐和情感分析都是一些上层的领域。（就是从知识表达的角度来融合知识图谱和深度学习，提出新的深度学习模型） 问题是，在知识图谱的构建上面，花了大量的时间给数据做标注，结果深度学习的方法只比传统翻译模型提升了一到两个点、 知识图谱构建上还有很多技术：实体消岐、实体链接、关系推理 可以使用这些别人对我们知识图谱的使用后的反馈对我的知识图谱做各种修改和更新 知识图谱不像nlp只是构建出语法树，也就是说不仅要把这个句子解析出来，而且还需要知道句子中的每个词的含义是什么、词跟词之间的关系是什么、每个词有什么属性等等。我们要了解其中方方面面的信息 知识图谱不像深度学习：第一是缺工具，第二是缺数据]]></content>
  </entry>
  <entry>
    <title><![CDATA[自然语言推理]]></title>
    <url>%2F2017%2F11%2F05%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%8E%A8%E7%90%86%2F</url>
    <content type="text"><![CDATA[两句话之间的逻辑关系，只有三类：推理p-&gt;q、无关、矛盾p！-&gt;q 自然语言推理最近相关工作（传统）2005年专家系统，2015年用深度学习找两句话的表示，然后做分类：lstm去encode句子表示，也可以加attention等等。问题是不能并行计算，使得时间周期长。 传统第一类做法：传统第二类做法 交互空间中的自然语言推理《natural language inference over interaction space》数据集是SNLI和multiNLI IIN：embedd–&gt;encoder–&gt;interaction做两句话的每个词之间的交互 extraction很像图像识别中的feature map，所以像用resnet做特征提取 交叉熵loss 《attention is all you need 》提出来一个muti-hot attention，就是对两句话做多次attention，就是说两组encoding之间有多个encoding weight。 更深的attention weight就可以包含更多信息。 模型模型分为五个部分：output、feature提取、interaction、encoding、embedding跟传统模型完全不一样 encoding layer分为三个部分：两层highway和selfatt输出一期进入fusegateencoding layer： （fuse gate作用是将两者信息动态结合） 现加两层highway network，在此之上做一次self-attention，整合信息全局信息，就是不用lstm去组合context信息。 feature extraction layerfeature extraction layer用的densenet（cvpr2017），不同的是除去batch-norm，每个dense block有8层convolution layer，每个convolution layer的output都是20个chanel的feature map，再叠加到之前的feature map上 三组densenet block 通过element wise interaction得到 回顾一下densenet结构densenet节省参数而又不损失准确率 实验数据：SNLI、mulitiNLI、quora question pair 切除分析做切除分析，每个切除分析都能加强对模型理解 交互空间中的其他应用 很多可以借鉴cv的地方，特定任务中有些好的神经网络结构会超过那些trick。如何融合数据集外的常识知识会变的非常常见的任务。 qa可以用在句子的reranking、相似度]]></content>
  </entry>
  <entry>
    <title><![CDATA[paper weekly1:DCN for qa]]></title>
    <url>%2F2017%2F10%2F28%2Fpaper-weekly1-DCN-for-qa%2F</url>
    <content type="text"><![CDATA[第一次的组内规定每周paper: DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING（https://arxiv.org/abs/1611.01604） 总体而言总体而言，是一个用于基于文档的问答的模型，允许对问题和文档进行互相依赖的表示，以便用文档的相关部分来回答问题。以下是模型可以生成的结果示例： 什么是阅读理解问题结合问题和文本段落二者的信息，生成一个关于文本段落各部分的注意力权重，对文本信息进行加权，得到最高权重的部分文本。 详细说明：DCN是一种端到端的，把问题和文档同时encoder（带coattention），以及确定文档中答案的开始词和结束词的动态定位decoder。 DOCUMENT AND QUESTION ENCODER假设问题中有n个单词，文档中有m个单词。还向每个向量表达中添加了一个哨兵（sentinel），以告诉模型不加入某些特定的单词。LSTM用于一次处理一个单词，隐藏状态用d或q表示. 提取lstm的每一时刻的隐层得到矩阵作为问题和文档的表示。 为了在文档和问题表示中有一些变化，对问句的表示做一个非线性变换。 模型第一部分是coattention encodercoattention 机制（见论文Hierarchical Question-Image Co-Attentionfor Visual Question Answering）：是一种双向的注意力，不仅要给阅读的文本段落生成一个注意力权重，还要给问句也生成一个注意力权重。 plus，协同注意力可以分为两种方式： rallel Co-Attention：将数据源A和数据源B的信息结合（Bilinear等方式），再基于结合的信息分别对两种数据源生成其对应的Attention。 Alternating Co-Attention：先基于数据源A的信息，产生数据源B的Attention，再基于加入Attention之后的数据源B的信息，去生成数据源A的Attention，类似交替使用两次传统的Attention。 其中符号：A：注意力权重，直接对文档和问句相乘后的矩阵分别按行和按列求Softmax，得到对问句中每个单词的Attention矩阵和文档中每个单词的Attention矩阵 C_Q：将Attention应用到问句中，问题中每个单词的注意力权重表示*文档中的所有单词。 Q__A_D：将Attention应用到文档中 C_Q__A_D： C_D：引入Co-Attention机制后的文档信息和文本信息的结合，将文档中每个单词的表示保存为QA_D和C_QA_D的总和的最终表示。 coattention encoder的最后步骤是将时间信息与我们刚做出来的注意力内容相结合。把文档信息D和C_D通过双向LSTM在时序上进行融合，双向LSTM每一时刻的隐层（U）作为总的encoder部分最后输出。 结果是矩阵U。（在获取Bi-LSTM的所有输出后，我们已经删除了前哨）。用该矩阵来预测答案在文档的起始位置和终止位置。 模型第二部分Dynamic Pointer Decoder动态迭代：对于模型输出的结果，不将它作为最终的结果，而是将它继续输入到模型中作为参考，迭代出新一轮的输出，经过多次迭代，直到超过迭代次数阈值。这在文本生成上有很多的应用，诸如唐诗生成等等。对于机器阅读理解，模型最终需要预测阅读文本的哪一个片段（Span）是问句的答案，我们则可以引入动态迭代的思想，先预测一个片段，再将预测输入回模型，反复迭代后，得到最终的预测片段。 迭代：输入：上一次的预测结果和Encoder信息以及历史预测信息，输出：下一次的预测结果 将每一次的预测结果都输入到一个LSTM中去保存该历史预测信息（h矩阵） 开始和结束位置由开始和结束的highway maxout network（HMN）的argmax决定。Maxout networks和highway networks很好用。 maxout和highway networks是具有相同结构但权重不同的网络。（hmn特别的地方是用Skip Connection并且使用Maxout作为激活函数） 可以将maxout网络视为强大的非线性模型，但是权重是ReLU的两倍。通过highway networks连接，能够选择性地控制信息流，能够建立从第一个最大层输出到最后一个最大层的连接。 整个Decoder部分可以概括为用HMN根据历史预测信息、上一次预测情况对每文档中每一个字作为起始位置（或终止位置）进行打分，用LSTM存储历史预测信息，将最后一次迭代得分最高的为最终答案。 训练损失函数是所有迭代的开始和结束位置的累积交叉熵。一旦开始和结束位置停止更改或设置最大次数（本文中为4），就停止迭代。 最大序列（文档）长度为600（字），模型所有隐层大小（LSTM units、Maxout layers、Linear layers）都设置为200 Dynamic Pointer Decoder的最大迭代次数为4，maxout pool size设置为16.在训练期间，网络中使用了dropout和ADAM优化。 另外：也可以使用一个简单的2层MLP而不是HMN]]></content>
  </entry>
  <entry>
    <title><![CDATA[沈向洋cncc]]></title>
    <url>%2F2017%2F10%2F27%2F%E6%B2%88%E5%90%91%E6%B4%8Bcncc%2F</url>
    <content type="text"><![CDATA[得语言者得天下 通过训练来学习描述（image caption）–&gt;对话就是智能对话就是智能（从回答到问答，从单轮对话到多轮对话） （做成系统：更多人用，更多数据） 从回答到问答https://arxiv.org/abs/1706.09789 从单轮对话到多轮对话结合上下文相关记忆、对话中的情感、目标用户的画像，结合三者共同作用于attention模型，从而优化seq2seq生成的回复 {encoder：用户上下文、ai上下文、当前用户输入}放进记忆LSTM中 {LSTM、用户画像、对话情感}提炼进attention model 再decoder生成 plus：长程对话必定有多次话题的转变，目标：转入期望值高的话题 对话就是智能–&gt; future人脑：eq+iq 操作系统➕软件-&gt;浏览器+搜索-&gt;APP-&gt;？]]></content>
  </entry>
  <entry>
    <title><![CDATA[贝贝网数据挖掘比赛经验]]></title>
    <url>%2F2017%2F10%2F22%2F%E8%B4%9D%E8%B4%9D%E7%BD%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[从贝贝网电商推荐比赛说起：有用户数据（userid）、商品数据（itermid）、行为数据。（最近数据挖掘比赛很火啊，京东杯、摩拜杯、蚂蚁杯 分类问题竞赛基于训练数据、提取特征训练模型，做分类基于用户前期行为，预测未来行为（有时序）比如电商推荐：有用户数据（userid）、商品数据（itermid）、行为数据 预测的是未来五天的对商品的label（0或者1买不买） 评分公式：模型预测0.4+userid和itermid对0.6 （两个模型：一个模型预测对每个商品会不会买，另一个模型预测用户最可能买哪个商品） 怎么构建训练集？–划窗思想优点：增加很多训练样本 用后五天购买过的ui对作为label往前寻找前n天，根据后五天的交互来赋值。 比如前面n天10万对交互购买，后五天有100对购买 一个模型负责预测用户，第二个模型预测ui对的label是1还是0 前期数据处理、特征工程这次比赛1000多维，通过树模型来筛选700多维特征，模型整体框架： 每一个训练集就是每一个时间窗口，这里取4个窗口 不是给多少数据就用多少，很多数据都是应该过滤掉的。 会设置采样比，有用特征可能没被才到。特征选择也很重要。 控制正负采样比。（根据评分公式） 总而言之的细节起始值的处理：数值型用平均数或者中位数；类别型要是有缺失值就是单别为一类，类别型一般要one-hot处理 异常值处理：统计中发现有一段时间数据分布明显 增删样本：把一看就不会购买的人（无效用户）直接抛弃。删样本可以用树模型来搞：树模型训练，每个样本分到叶子节点上（应该具有一致表现），看样本离群程度，选topk 上下采样来控制正负样本比（xgboost可以直接设参数，其他需要手动改，比如负样本随机采样百分之五十） 数值型数据的特征工程：树模型来搞，可以不需要做归一化，因为分裂节点标准都是暴力来选取。要是用线性模型或者梯度下降，还是需要做归一化（在高维空间是狭长的模型，不适合求最优解） 类别型数据的特征工程：树模型+one hot。但是如果某类别有成千上万，就不用onehot。（直接把树丢进去，比如有100类，分为0-99，直接丢树）当然如果用逻辑回归之后onehot效果之后会非常好。 特征选择：L1正则可以让特征稀疏，或者PCA降维，也可以树模型直接来选—训练完之后输出特征重要性 基于赛题场景设计特征：不是所有赛题都适合树模型。有些用FM可能也很好。 模型选择加入规则特征很重要，前期不上模型，或者在模型中加入规则函数（晚点上模型，前期先发现规则） lightgbm是nips2017才发，区别是按叶子来分裂…可以看泽康聚聚pdf FFM：因子分解机，台湾大学的库，线性模型基础上考虑交叉组合特征 模型融合深度学习来做分类问题其实效果不太好，不如FFM。纯结构化数据还是不适合深度学习，LSTM和CNN做分类还是不太好。 这部分一般是在比赛结束前几天搞一下，最常用是四种 重点1、选择合适的验证集，线下的验证集一定要选好，不然可能线下线上趋势不一样。（线上线下分布要基本一致，或者说，预测周末，验证集也找周末） 什么叫合适：训练出来的模型得到的分数的趋势变化和线上趋势要差不多，才有利于后期线下调试 2、数据挖掘比赛中模型创新很少，大部分时间在搞数据清洗，搞数据分析，搞特征工程，这三块至少占百分之七十。]]></content>
  </entry>
  <entry>
    <title><![CDATA[linear model]]></title>
    <url>%2F2017%2F10%2F18%2Flinear-model%2F</url>
    <content type="text"><![CDATA[There are three linear models:PLA(感知机)，Linear Regression(线性回归)，Logistic Regression(逻辑回归) Linear Scoring Functions=wx PLA: output:+1 or -1 (NP-HARD) Linear Regression： regression model，直接得到某个实数值，使用平方损失函数，得到连续二阶可微的凸函数，并且一阶微分为线性函数，可以直接求解最优参数，仅仅需要一次矩阵运算。 Logistic Regression:output某类的概率。使用对数损失函数也叫 cross error，得到连续二阶可微的凸函数，不过一阶微分函数非线性，很难直接求解，因此使用梯度下降的方式来迭代求解。 error Function 0/1: 0 iff ys &lt;0 平方: 当ys的绝对值比较大的时候， err也会很大。也有地方很接近与0/1损失函数 cross-error: 相对平滑稳定很多，更加接近与0/1损失 使ys为x，err为y PLA: 在线性可分的情况下，效果最好，可以有最小的E; 缺点是: 必须是线性可分，否则需要用Pocket改进算法，这样的话，求解优势不明显。 Linear Regression: 最优解求解最简单。缺点: 在|ys|比较大的时候，err这个bound太大。 Logistic Regression: 求解相对简单。缺点: 在ys&lt;0时， err偏大。 综上的话，在实际中做二分类的时候，经常先用Linear Regression 做一次，得到一个参数，将此参数作为Logistic Regression 或者 Pocket PLA的初始参数: w，进而再去使用梯度迭代的方式最优化，这样可以很大程度上加速迭代的过程，更快得到最终解。 还有一点是，相对于Pocket PLA来说，在实际中用的更多的还是Logistic Regression。 multi-class problemOVA （One Versus All） but data set’s size influence robust.so there comes OVO.]]></content>
  </entry>
  <entry>
    <title><![CDATA[SingleInstructionMultipleData]]></title>
    <url>%2F2017%2F09%2F27%2FSingleInstructionMultipleData%2F</url>
    <content type="text"><![CDATA[高性能计算中一个基础的东西–Single Instruction Multiple Data一条指令多个数据流 （使用同一条指令同时操作多个数据） 特点属于数据级别的并行(data level parallelism), 无需多线程参与(not concurrency) 没有通用的模板（不像mpi、openmpi）可以套用，没有cuda、不能用不同架构的cpu去套进模型去算，意思就是完全依赖于硬件 如开一个openmpi要有启动线程等的很大开销，但simd就没有，单线程的。那么它可以和多线程配合使用 怎么simd1、默认加编译选项自动向量化 2、检测到依赖会拒绝向量化，就使用OpenMP的#pragma simd 强行向量化，但是效果不好 3、推荐：Intel intrinsic (推荐) 4、内嵌汇编直接爆肝 __asm指令，用c语言写的时候把汇编嵌入进去]]></content>
  </entry>
  <entry>
    <title><![CDATA[what is vae]]></title>
    <url>%2F2017%2F09%2F20%2Fwhat-is-vae%2F</url>
    <content type="text"><![CDATA[vae的实现 papers：《Generating Sentences From a Continuous Spaces》. ICLR 2016 《Neural Variational Inference for Text Processing》. ICML 2016 《Language as a Latent Variable: Discrete Generative Models for Sentence Compression》. EMNLP 2016 《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》. AAAI 2017 1) VAE 可以看做是 Standard autoencoder 的 regularized version（在 autoencoder 的架构上引入随机 latent variable）2) VAE 从 data 学到的是在 latent space 的 region，而不是单个点。换句话说是 encode 学到了一个概率分布 q(z|x)3) 引入 KL divergence 让后验 q(z|x)接近先验 p(z)。这里的 motivation 在于如果仅用 reconstruction loss，q(z|x)的 variances 还是会很小（又和原有的单个点差不多了）要掌握变分推断和理解 reparametrization trick 。 VAE是Max Welling 和他的学生D. Kingma 提出的一种Generative Model. VAE本质上不是一个deep generative model. 因为它实质上只是学习一个两层的graphical model. h 是一组 latent variables, 我们有一个概率模型和 , 我们希望能够从一组unlabelled data 里面学习出. 这个问题看起来很简单，其实是graphical model特别是latent variable model里面的一个老大难问题，也非常基本。第一次接触这类问题的话，可能会惊讶于machine learning学界连这么简单的一个二层的latent variable model都训练不出来，也反映出概率计算到底是多么地困难。 如果受过一些训练，就会想到EM算法, 不断地做两件事: (1). 计算 (2) 但这个算法当然要求很高，必须l 是可计算的，换句话说 p(h|x) 形式必须简单。一般来说h只取有限个离散值才可以这么干(clustering)。过去20年unsupervised learning的主要发展就是clustering，但clustering是不足以解决AI问题的。 当然也可以做Monte Carlo EM, 也就是用MCMC sampling去逼近. 效果据说很差，因为每个data point 在每个update上都得sample一次。但现在计算资源如此丰富，以后也许不是问题 现在我们假设h 是连续随机变量(这是vae的核心假设)。目标是maximize log-likelihood. 一个很自然的想法来自 Variational inference. 首先我们有variational bound: Variational Inference 的想法是， 对于q(h|x),我们可以取一些简单的distribution family 进行逼近，进而最大化, 对于每一个数据点x. 比如最常见的mean field VI，就假设q 是factorized 的分布。如果p 再比较简单，就可以解析地解出B,然后做最优化.但有时候p复杂，积分还是求不出来，那就只有再靠sampling了。 上面这个gradient就可以用sampling 逼近了。 这个trick基本上起源于REINFORCE algorithm.但悲剧的是，这个estimator的variance相当大。所以并不实用。2012年Jordan的学生在这个式子的基础上加了一点variance reduction，然后就发了一篇ICML。 这里为啥不用variational inference呢？因为VI的distribution太局限了。一般都是mean field的，这样误差一般都会很大。假如h有很复杂的结构，给定x, 我们希望我们的逼近能capture h之间复杂的correlation，不希望简简单单假设他们是独立的。 Variational bound也可以写成 第一项是可以看做是reconstruction error. 这就是为啥被称为VAE. 先验概率p(h)往往非常简单，简单到第二项有时候可以算出来。 如果可以算出来，它的gradient estimator的variance就会进一步减小。 VAE做的事情是不对q做非常简单的假设。这样我们能够capture的distribution就复杂得多。 而假设我们有随机变量满足分布, 是一个random seed，是一个被参数化的光滑函数族，比如一个DNN. 为什么做这个变量代换呢？是因为上面的那个estimator的variance大的原因，是因为它需要sample from q，然后利用q的sample处的gradient修正q，这样两次产生的误差会叠加。但做了这个reparametrization，我们只需要sample一个分布非常稳定的random seed的distribution，比如N(0,1)所以noise小得多。 应用这个的trick,我们有对于任意的函数g, . 所以我们求这个东西的gradient，就有. 如果它是DNN决定的， 就可以backpropagation. 对做sample, 这个estimator的variance就小了很多。 比如我们假设满足的分布是一个正态分布，它的mean 和variance由被决定的两个DNN 算出来，那么我们就可以很简单地计算. 然后交替地优化和就可以得到结果。 VAE几乎是现存的最优秀的generative model之一，另外一个是GAN。 它的问题也非常明显。Real data, 比如natural images，它是非常高维的数据，但他一般处在一个low dimensional manifold上面。VAE使用Gaussian approximation去做inference，通常也假设p是gaussian的，这样缺点就是没办法capture这个low dimensional structure。因为gaussian不可能做到100%低维。 《Generating Sentences From a Continuous Spaces》. ICLR 2016motivation 在于作者为了弥补传统的 RNNLM 结构缺少的一些 global feature（其实可以理解为想要 sentence representation）。其实抛开 generative model，之前也有一些比较成功的 non-generative 的方法，比如 sequence autoencoders[1]，skip-thought[2]和 paragraph vector[3]。但随着 VAE 的加入，generative model 也开始在文本上有更多的可能性。Loss 的组成还是和 VAE 一样。具体模型上，encoder 和 decoder 都采用单层的 LSTM，decoder 可以看做是特殊的 RNNLM，其 initial state 是这个 hidden code z（latent variable），z 采样自 Gaussian 分布 G，G 的参数由 encoder 后面加的一层 linear layer 得到。这里的 z 就是作者想要的 global latent sentence representation，被赋予了先验 diagonal Gaussians，同时 G 就是学到的后验。 模型很简单，但实际训练时有一个很严重的问题：KL 会迅速降到 0，后验失效了。原因在于，由于 RNN-based 的 decoder 有着非常强的 modeling power，直接导致即使依赖很少的 history 信息也可以让 reconstruction errors 降得很低，换句话说，decoder 不依赖 encoder 提供的这个 z 了，模型等同于退化成 RNNLM（摊手）。顺便一提，本文最后有一篇 paper 也是为了解决这个问题。 先看这篇 paper 提出的解决方法：KL cost annealing 和 Word dropout。 1) KL cost annealing作者引入一个权重 w 来控制这个 KL 项，并让 w 从 0 开始随着训练逐渐慢慢增大。作者的意思是一开始让模型学会 encode 更多信息到 z 里，然后随着 w 增大再 smooth encodings。其实从工程/代码的角度看，因为 KL 这项更容易降低，模型会优先去优化 KL，于是 KL 很快就降成 0。但如果我们乘以一开始很小的 w，模型就会选择忽视 KL（这项整体很小不用降低了），选择优先去降低 reconstruction errors。当 w 慢慢增大，模型也慢慢开始关注降低 KL 这项了。这个技巧在调参中其实也非常实用。 2) Word dropout 既然问题是 RNN-based 的 decoder 能力太强，那我们就来弱化它好了。具体方法是把 input 的词替换成 UNK（我可能是个假的 decoder），模型被迫只能去多多依赖z。当然保留多少 input 也需要尝试，我们把全都不保留的叫做 inputless decoder，实验表明，inputless VAE 比起 inputless RNN language model 不知道好到哪里去了。受到 GAN 的启发，作者还提出了一个 Adversarial evaluation，用一半真一半假的数据作为样本训练出一个分类器，再对比不同模型生成的句子有多少能骗过这个分类器，这个 evaluation 被用在 Imputing missing words 这个任务上，VAE 的表现同样比 RNNLM 出色。最后，作者展示模型的确学到了平滑的 sentence representation。选取两个 sentence 的 code z1 和 z2，z1 和 z2 可以看做向量空间的两个点，这两个点连线之间的点对应的句子也都符合语法且 high-level 的信息也保持局部一致。 《Neural Variational Inference for Text Processing》. ICML 2016其实这篇 paper 和第一篇是一起投的 ICLR，后来转投了 ICML 2016，所以时间上其实和第一篇是一样的（两篇文章也有互相引用）。不同于第一篇，作者的出发点是构建一个 generative neural variational framework。为了证明 framework 的优越性，分别在 unsupervised 和 supervised 的任务上提出了两个模型，结果也很令人满意。 第一个任务是 unsupervised document modeling，模型叫 Neural Variational Document Model（NVDM）。h 和第一篇的 z 一样，在这里代表 latent document semantics，但 document 是以 bag-of-words 的形式（个人以为这里作者主要还是受到 LDA 的影响）。encoder 采用MLP，decoder 是一层 softmax。第二个任务是 supervised answer selection，模型叫 Neural Answer Selection Model（NASM）。文本的建模方式采用 LSTM（在第二个任务用 LSTM，第一个任务用词袋，可能为了证明普适性）。h 代表 latent question semantics。如上图所示，Zq 和 Za 用来表示 question 和 answer，y 代表 answer 是不是正确答案，用 Zq 和 Za 预测 y。那么 Zq 和 Za 是怎么得到的呢？Zq 延用 LSTM 的 last state，而 Za 则较为复杂，所谓脱离问题谈答案都是耍流氓，所以对 Za 建模时要显式的放入 question 的信息。可这里该怎么表示 question 呢？如果还用 Zq，模型很容易 overfitting。这里我们的 latent h 终于可以出场了，引入 h 不仅起到了 muti-modal 的效果，还让模型更 robust，再把基于 attention 的 c(a,h)和 answer 的 LSTM last state 组合得到 Za。这种做法对我们在寻找 representation 时有很好的借鉴作用。最后通过推导 variational lower bound 确定 h 的先验是 p(h|q)（第一个任务中先验是 p(h)）, 这里就不赘述了。 《Language as a Latent Variable: Discrete Generative Models for Sentence Compression》. EMNLP 2016这篇 paper 发表在 EMNLP 2016，同样出自第二篇 paper 的作者。传统的 VAE 是把数据 encode 成 continuous latent variable，这篇 paper 的贡献在于提出了一个 generative model 用来学到 language 的 discrete representation—一个带有 sequential discrete latent variable 的 VAE。所谓的 discrete latent variable 就是指一个单词，加上 sequential 其实就是一个句子，由于 VAE 本身是压缩数据的，换句话说是用短一点的句子来表示原来的句子，也就是句子压缩。我觉得作者的 intuition 在于每个句子可以有多个缩写，且都可以表示原句，有一点点 distribution 的意思，所以用 latent variable 很合适。原句和压缩句分别是 s 和 c ，模型整体是 encoder -&gt; compressor -&gt; decoder。我们分解开看，encoder -&gt; compressor 采用 pointer network[4]只从 s 里选取合适的词而不是整个词典，从而大大减少了 search space。compressor -&gt; decoder 是一个带 soft attention 的 seq2seq。这个模型的好处是不需要 label 数据，但是如果我们有足够的 label 数据（真实数据里 c 里的词可不仅仅来自 s），需要额外加个 softmax 从整个词典里选词，同时再定义一个 latent factor 判断是从 s（pointer network）还是从词典里选，更加符合任务需求。值得一提的是 Variational lower bound 里的 p(c)是 pre-train 好的 language model。因为 Language model 的一个特点是比较喜欢短句子，很适合句子压缩的场景。由于 reparameterisation trick 并不适用 discrete latent variable，作者还采用了 REINFORCE[5]的方法（凡是 discrete 的问题，GAN/VAE 都可以采用 REINFORCE）。 《A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues》. AAAI 2017这是第一篇把 VAE 的思想引入到 dialogue 的 paper。和普通的 VAE 区别在于 dialogue 的 reconstruction 是生成的下一句 utterance，而不是 input 自身。这篇 paper 的前身是 HRED[6]，HRED 的核心思想是，把 dialogue 看做是 two-level：dialogue 是 utterance 的组合，utterance 是 words 的组合。HRED 由 3 个 RNN 组成：encode RNN 把每个 utterance 变成 real-valued 的向量 u，context RNN 把每个 turn 里的 u 作为输入变成向量 c，最后把 c 交给 deocde RNN 生成下一个 utterance。VHRED 在 HRED 的基础上每个 turn 里引入一个 latent variable z，z 由 context RNN 的 c 生成。z 的意义比较笼统，sentiment/topic 怎么解释都行。模型的训练技巧如 KL annealing 等大量借鉴了第一篇 paper 的思想，特别要注意训练时的 z 从后验采样（保证 decode 的正确性），测试时再从先验采样（ KL 已经把分布拉近）。实验表明，latent variable 有助于生成更加 diverse 的回复。]]></content>
  </entry>
  <entry>
    <title><![CDATA[understand rnn from motivation]]></title>
    <url>%2F2017%2F09%2F20%2Funderstand-rnn-from-motivation%2F</url>
    <content type="text"><![CDATA[just scratch the surface N vs NThe length of input and output sequence must be equal char rnn：http://karpathy.github.io/2015/05/21/rnn-effectiveness/ N vs 1 Sequence to the Category 1 vs Nhttps://static.leiphone.com/uploads/new/article/740_740/201709/59a920bc1964b.jpg?imageMogr2/format/jpg/quality/90 the Category to the Sequence N vs MSeq2Seq encoder: Decoder: Attentionthe length of ‘c’ limits the model performance. Attention mechanism input different c in each different time to solve this problem. Decoder with attention: example: (aij is learn from the model.) tf]]></content>
  </entry>
  <entry>
    <title><![CDATA[say hello to nlp]]></title>
    <url>%2F2017%2F09%2F18%2Fsay-hello-to-nlp%2F</url>
    <content type="text"><![CDATA[first try to write in english &amp;&amp; want to make friends in our studio say hello to Natural Language Processing Introduction Natural language processing (NLP) is all about creating systems that process or “understand” language in order to perform certain tasks. So what are these tasks? For example: qa sentiment analysis image to text mappings machine translation name entity recognition traditional approachThe traditional approach to NLP involved a lot of domain knowledge of linguistics itself. Such as phonemes and morphemes. just try to understand the following word—“uninterested” “un” indicates an opposing or opposite idea and “ed” can specify the time period (past tense) But we are not skilled linguists dl approachrepresentation learning can be describe as a general term. Word Vectorswe have to represent each word as a d-dimensional vector. Let’s use d = 6.we want this six dimensional vector to represents the word and its context, meaning, or semantics. A coocurence matrix is a matrix that contains the number of counts of each word appearing next to all the other words in the data. So now using the matrix we have word vectors,but the question is that the matrix which would be extremely sparse (lots of 0’s) when the training data are very large. Definitely it has poor storage efficiency. so let us come to Word2Vec. Word2Vec The basic idea is that we want to store as much information as we can in this word vector while still keeping the dimensionality at a manageable scale (25 – 1000 dimensions). We want to predict the surrounding words of every word by maximizing the log probability of any context word given the current center word Word2Vec finds vector representations of different words by maximizing the log probability of context words given a center word and modifying the vectors through SGD(https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf、http://nlp.stanford.edu/pubs/glove.pdf) RNN Each of the vectors has a hidden state vector at that same time step (ht, ht-1, ht+1). Let’s call this one module=each of the vectors(x)+hidden state vector hidden state：about the hidden state vector at previous time step and the word vector.W is weight matrix. When we want a output.The last time is t. gated recurrent unitsWhy we need to capture long distance dependencies? During backpropagation, such as if the initial gradient is a small number (&lt; 0.25), then by the 3rd or 4th module, the gradient will have vanished. The GRU provides a different way of computing this hidden state vector h(t). Included:an update gate, a reset gate, and a new memory container. two gateas have different weights a new memory container: LSTM as an extension to the idea behind a GRU different in the number of gates that they have (GRU – 2, LSTM – 3) http://colah.github.io/posts/2015-08-Understanding-LSTMs/ Memory Networks详见另一篇博客 神经机器翻译 论文：https://arxiv.org/pdf/1609.08144v2.pdf]]></content>
  </entry>
  <entry>
    <title><![CDATA[smp2017]]></title>
    <url>%2F2017%2F09%2F16%2Fsmp2017%2F</url>
    <content type="text"><![CDATA[keynete价值&gt;Σ(paper+poster)价值 keynote1张潼 ai lab:基础研究+产品+开放基础研究：在acl和emnlp发了很多文章（和cv、speech相比nlp是更难的问题，还要花十年二十年的功夫） 产品：游戏+社交+音乐/视频 开放：ai open platform提供api tasks对话是nlp到底行不行的关键点 各种从哪里到哪里 text 到knowledge： 腾讯kgentity graph就是普通的关系 term graph就是对某个term的解释在qa和chatbot里面的应用 先爬data-&gt;抽取关系、事件、等等的抽取-&gt;整理和处理 其中有很多困难： 跟抽取有关的一个问题，pattern-based 有篇文章：用先验知识做w2v，更好的embedd 怎么做个性化推荐：需要和文本相关，词的打分，各种各样的分类（不同分类方法），有个点就是怎么构建用户兴趣，有很多approach比如text categorization（深度model：从浅层cnn到lstm计算慢到深层cnn） 舆情分析：如今日热点：全网热点，怎么把数据聚合起来，更好的去定义这个热点。腾讯的优点：有自己的数据。情感走势：虽然只是二分类，对某一热点的文章情感走势：emnlp17用很多attention去找很多关键词，再聚合。 deep 情感 parsing：从文本到情感树，把text变成公式（把小学数学题变成公式），虽然也是一颗树，两种方案：1、自动从树生成 grammar（text-free grammar），高准确率低recall， 2、更普遍的case，用seq2seq，code成seq 想问一个问题：怎么把公式生成题目，推荐题目 text generation：写文章（从kg到text）、翻译、照片翻译 翻译也做了很多工作，但是我还是对翻译不感兴趣，就没记录。 最困难：开放域的chat，现在都只是特定domain的，open domain还是没人解决。问题分析-&gt;各方面的merge-&gt;meta search+rbu+app+kg 重要的：context理解，知识的抽取和匹配（matching上面深度学习做的挺好），语言生成模型，更多复杂的（如独热学习） 想问一个问题：多个知识图谱？不同知识图谱的链接？结构？应该用什么方法去做kbqa！ keynote2周涛经济学、教育学、社会学传统区域经济评估：虚报、滞后 公开的数据来源：第一时间、无法造假 在经济学上面，计算机、大数据都只是辅助手段，并不适合切入这个领域， 教育：用行为数据预测问题 与成绩有很多关系因素，人格不一定能变化，但是行为是可以干预的，我们去研究可以干预的。 海量data：mooc、手机 他做的是预测，index：生活是不是有序（用actual 熵而不是香农熵）+努力程度 actual 熵：n是时间序列的长度（离散值），ganmai：congi往后数，最短的长度是该序列之前都没出现过的 做斯皮尔曼主成分分析和异常分析 这个人明显是做物理的特点：统计分析像数学建模一样 （给辅导员做报警） 对于社会经济学，找一个非常有用非常性感的问题很重要，也很有意思 keynote3秦兵社交媒体中的情感分析 情感分类：输入seq或篇章，输出情感类别传统做的话能达到80%-&gt;rnn在依存树基础上得到根节点的情感分类（基于句法分析）-&gt;cnn直接做全局和局部的上下文信息且速度快-&gt;得到情感词典、得到语言学约束加到损失函数中 篇章级具有语意组合性，层次化的建模，词到句子到篇章（词层和句层都加attention） （篇章级有结构性的划分问题） lstm+attention：给予评价对象的情感分析（局部和总体，属性和个体）、memory+attention（显式利用上下文） 情感抽取 三个任务：情感词语抽取、评价对象抽取、评价搭配抽取（二元组对应） 情感词的表示学习工作（基于词语，词向量反而不好，因为词向量依靠上下文）：情感词典 评价对象：传统crf序列标准-&gt;lstm+词向量+pos词性特征（emnlp2015）、依存结构 评价对象搭配：2011句法规则匹配、基于句子压缩（去无关干扰） 跨领域情感分析从源领域到目标领域，相当于迁移学习 问题：评价对象不同，评价表达不同，情感表达极性不同（程度不同） 2010spectral feature alignment聚类 2011icml svm+单层去噪自动编码器+堆叠去噪自动编码器 个性化情感分析基于用户用词习惯不同用户群体情感倾向是不同的，主观想法和个人身份立场不同 比如：打分个人有打分高和打分低的偏好 2015acl 基于认知理论的方法用户画像：属性维度、性格维度、行为维度 结合用户信息进行情感分析 基于网络结构的方法社交媒体上用户之间的连接关系、相同情感倾向性kdd唐杰 隐式情感分析事实型和修辞型（说不透） 中文情感表达方式复杂 事实型比如：桌子上有一层灰、西方文明的摇篮、收货只要20小时 基于规则和特征的意见挖掘模型（acl2011） 基于上下文的方法 修辞型反讽：大连理工林鸿飞的隐喻语料库 情感原因发现基于文本数据来源哈工大深圳徐睿峰 基于个体立场现有方法难以解决立场问题 基于群体立场民众情绪的自动归因（对焦点事件）、可能有子话题：比如沉船有人被救起 情感生成评论文本生成：affect-lm（2017acl）：前半句生成后半句attribute+attention（eacl） 情感回复生成！！！知识点！！！链接！！！！反问！！！在chatbot中的应用！！ 语文题目里面：任重而道远！！ 学生学习，不仅是答案，更重要的是为什么，答题套路，要讲人话！ keynote4唐杰 总结前人20世界前：社会学为主：1967六度分割、1973weak tie、1995结构度 20世纪左右：物理学：hits、pagerank、smallworld、scale free 21世纪：计算机学：link prediction、network evolution：复杂化发展（加时间加地点）densification、social influnence分析 2009computational social science（giles） 那什么是社会计算学：很多节点和边组成的社交图 信息1.0：data：像是给一个query把文档排序（google是1.0重要代表） 大数据来了：it时代公司发展云，传统公司跳进来把数据存进去 信息2.0：数据➕用户，像是信息推荐（如今日头条是2.0时代重要代表） 未来是什么：融合智能：数据语意➕用户语意=知识-&gt;智能http://oqnrd919g.bkt.clouddn.com/17-9-16/21529687.jpg 大数据需要知识，一定需要deep learning这个锄头去挖数据 以交互驱动 节点是用户，边是关系，以用户为核心和以边为核心两种建模方式 难的是：社会理论融合到概率图模型上，不仅需要how，更需要why 第二部分，社交影响的研究用户观点和影响 2012nature：即使不认识也会有影响 问题：应用这个影响：2009kdd，谁影响谁，双向会不一样，topical factor graph的模型 结果：谁对当前用户影响最大，学习算法（TAP算法）：问题节点需要搜索到周围其他节点，效率低无法分布式计算 问题：1、结构上的影响非常复杂，2、行为怎么去用dl学习 1、结构上的影响非常复杂aaai2017 不知道红色节点有没有边，先学习三个的，红色是正向蓝色是负向白色是目标， 在真实中，不一定是正向负向，会维数爆炸：fast sampling 可以预测微博转发结果 2、行为怎么去用dl学习 user➕embedd层 influence是一个传播的结果，而且有可能有噪音：用GAN模型化传播过程（aaai2017） 腾讯上的应用提高了非常多 nlp session主要是用社交媒体的数据做常见的nlp任务 seq2seq 翻译 中文分词北理 史学文 微博中的分词，外来语和拼写错误 先做中文分词：传统当做序列标注我们当做翻译，目标是带分隔符的序列才知道attention based encoderdecoder是2015提出的用最长公共子序列（LCS）来解决多语言的问题PKU、MSRA的分词数据 结果跟LTP作对比 NLPCC2016评测数据（分词） 结果显示跟最好的还是有一定差距 本文相当于定义新的任务（带有错误），输出分好词且无错误 SIGHAN2014有个拼写任务评测 结论：给预处理提供了一个新的想法 社交媒体数据中的实体集合扩展问题：社交媒体文本数据很多噪音、多语义 先提取候选的实体集合再对候选排序 特征：三个维度的，连词模式和前缀规则 可以用随机游走对候选答案进行排序 连词模式：用、联结或者@联结的通常是同一类别，总结出24种连词符号及其权重 综合排序模型 用map平均准确率来评估 DQN的开放域的多伦对话问题：1、万能回复，2、不考虑到对话的未来走向 最常用的seq2seq生成式（而不是检索式）所以由于最大化似然估计所以会有万能回复 没有建模多伦对话的整体过程所以没有考虑到未来走向 用强化学习来考虑问题的总体视角sar序列，最大化马尔科夫过程 DQN：状态S和动作A，迭代更新的训练犯法，详细见2013DQN 自编码器：输入和输出一样 价值的估计（MLP）输入特征向量，输出价值 事件演化图谱相关工作：统计脚本学习、时序关系抽取 抽象范化的事件 结构：树状、链状、环状 构建pair candidates，估计转移概率 知乎旅行答案数据 教育session刘三女牙（宏观）1、未来教育生态构建的核心问题个性化学习：学习是个人的：风格、速度、兴趣 老师和学生即是面对面也无法完全掌握学生状态 2、教育信息化 云计算搭台，大数据唱戏，人工智能（数据驱动）成明星 终身学习、自主学习 3、学习场景 不是预测是洞察，帮助孩子克服缺点 4、困难 数据 机器可以替代的是归纳推理，而不是创造 缩短15年教师成长期 荀恩东 语言教育语言教育技术太多了 大数据数据形式不是电子！ 个性化学习、协作学习、泛在学习 相关工作1、BCC汉字语料库（多样式的） 2、汉字书写评测系统 ppt和书，缺乏移动学习：3、app卡片汉语 问题还是在扫描件上面，还有汉字书写的评价问题 3、语文作文评分 唐杰 mooc互相影响是非常重要的，不可能一个人学习 问题：节课率低 chatbot和mooc结合 在mooc上知识体系丢失了（层级关系） 实现智能交互，学生和知识图结合来提高热情度 教育中不是交互，而是干预，不是问什么就答什么。 （助教的角色，问问题不仅回答，还推荐课） faq答案库：有很多回答器只是最后选择一个回答器 不一定要全都会回答，只是学习助手，他也不懂，做生态，邀请其他人回答。 “有三个人一起在看，邀请三个人来聊天！做生态” forums和结课率的关系 预测更多 从mooc中字幕抽取关键词，类似于pagerank，做概念图，从而推荐课程，课程流程 acl2017 七个相似度+随机森林分类器 三、小沐 交互和干预 1、课程推荐，首页，推荐视频的重点 2、干预，droup out keynote5 拓尔思舆情：数据量 对待数据垄断：还得爬，最好还得是从平台获取 舆情报告的智能化（ai写数据驱动的舆情报告）（清博） 舆情的发展 舆情的处理过程 未来：从单舆论场到多舆论场的融合（微博微信联合在一起）、从分析到预测（预测未来可能会发生什么，需要知识库推理）、综合人行为空间时间一起综合分析、从单语种到跨语种（全球化）、机器数据也是舆情、对重点事件做多维度分析（传播的指标） 评价指标模型 公关公司是舆情公司（拓尔思）的客户，排行指标中人为因素很重要 keynote6 刘铁岩ai是一个方法论，但看领域本身，在各个方面都有突飞猛进的发展1、图像 imagenet（关键点2015） 2、语音识别loud and clear2017 5.1% 3、nlp 翻译： 更复杂，因为包含高级语意 橙色人，绿色nn，蓝色传统 对话：小冰 左边是rl（下棋），右边是知识图谱 应用医疗是一个长期没有被开发的处女地，ct是对器官做切片处理，医师花半个小时做诊断是困难的，只有五分钟，所以可以辅助。 而且医疗资源匮乏，把知识图谱放到机器人里面，可以改变医疗资源不均匀的情况。 金融：量化数据是一个时序序列分析，非常重要的投资资源；人类基金经理很难很快从报告中提取信息；金融投资实际是一场博弈，有来自各方的各种诉求的参与；金融投资分秒必争 总而言之，平民化和网红化，但是很多方面还是短板。 ai和人类社会之间存在的鸿沟人的智能从何而来？回答：社会分工相互促进（互补），教育体系知识传承（不用出生就强化学习） dual learning和leanring to teach dual learning对偶学习学习是一件苦差事，问题：严重依赖于标注数据 那么，如何摆脱这种学习曲线缓慢的问题？ “听君一席话，胜读十年书”，利用对称性，主问题和对偶问题，相互给反馈信号，举例：机器翻译 原因：对称的任务有很强的概率联系 对偶推断，用对偶模型通过贝叶斯做的假主模型和真主模型做集成 leanring to teach因材施教高效的最小化损失函数 如何教：怎么植入先验知识，平衡有监督和无监督的比例，选择合适的损失函数，怎么评估模型 经典的机器学习就是静态的学生，meta leanring就是动态学生用强化学习（最优化），learning to teach是动态老师 “教学相长” 实验：动态老师来选择数据，去冗余，去下噪音，数据输入顺序 mini-batch sgd 实验结果： 分工合作、分工合作、学术选拨（对知识的强化学习有监管）、市场经济（有无形的手在操作） 走仿生学还是结果论？还是倾向后者，为啥要学习人类社会，因为感觉现在的机器学习算法太笨了，需要大量样本，并不知道是不是数据冗余了，像牛刀杀鸡，会走向垄断，所谓我们想要轻快学习，柯洁吃一个鸡蛋就能比赛，阿尔法狗却是一个小型发电站。 对于量子计算，仍然只是把有限的问题变更快，而ai不知道智能从何而来，量子并不知道有啥结合。 # dynamic network2016kdd 随机异常RCA 网络的改变，变化的原因，定位 早期的用消失的相关项， 情感分析 session李寿山苏大 文本情绪分析任务：产品评论、个人性格分析、精神状态识别 情感是对极性的评价，情绪是主观认知经验的通称 文本情感分析：句子level、篇章level、评价对象level 文本情绪分析任务:情感类别和情感原因|结果 三种解决语料库匮乏的问题： 语料库建设1、类别体系的定义（心理学：基本情绪和复合情绪） 基本情绪：快乐、愤怒、悲伤、恐惧 2、情绪体验者体系 作者、读者、某人 3、原因或者结果 情绪图标事实：社交网络中含有微表情或者表情包 解决方案：网络学习的方式 半监督的新闻读者和评论（对称性）acl13存在读者情绪和作者情绪，情绪是有关联的。两种视角做co-training 就把非标注信息的样本利用起来 coling16 当不满足co-training时，要改进 通过新定义转移学习 （情感分析半监督都算是前沿，短文本的半监督很有挑战性） 腾讯 ai平台部 基础技术中心舆情商业化 平台：（基础平台）-&gt;（指数分析平台、用户画像平台、舆情分析平台）-&gt;对接业务 技术问题：延时问题、歧义问题、数据量大、事件覆盖 事件热点三级发现策略：离线策略（主题分词、LDA语意聚类）、在线策略（层次聚类）、高危策略 （预定义高危事件模板，信息抽取实时上报） 歧义问题：有监督消除歧义，看做分类，每个语意一个类别，但是问题是数据太大要几十万个分类器。所以还是得用无监督消除歧义。构件实体知识库，核心是计算歧义实体的上下文和实体知识库的语义相似度 构建相似图：词为节点，边为其余弦相似度 舆情目标：有一段时期，对不同的时期有着不同的策略。 舆情组件：情绪（作者）、情感（读者）、观点提取 情绪分类建模：典型的短文本分类问题。分类模型+闭环迭代系统模型 情感分类：影响情感分类的三个关键点：评价对象、情感词语、对象和词语的相对位置 汗液做法：rule+句法分析&lt;svm+句法分析&lt;target+cnn&lt;target-lstm-a&lt;memnet； 改进：emnlp2017 recurrent attention network for aspect情感分析 观点提取 问题定义：实体观点和事件观点 用户： 问题：账号体系不统一、属性的错误、如何刻画对象（基础属性+层次化兴趣体系） 舆情商业化应用： 应用：分析用户反馈，优化产品需求，排序处理问题的优先级，分级处理问题 难点：异常问题必须在大量传播才有效，人力难以遍历信息，处理信息滞后 商业化探索：与新闻媒体结合 用采集的数据定制内容和题材策划并可视化分析结果 商业化探索：与影视剧制作结合 总结过去来预测未来 问题：用户画像和媒体画像 t大 jaijia 情感计算affect computing难题：怎么提取具有情感区分性的feature，social feature ，建模（large network） 表示学习session深度学习非常重要的一个应用。非结构化学习的一个表示 赵鑫跨主题的一些工作，推荐系统中的表示学习 对数据特征以及其结构的表示，数据的摘要。传统的矩阵分解和现在的embedd 推荐系统：评分预测和推荐五题基本模型：mf矩阵分解 评分预测：一个表格中的填空 更复杂的是基于context的推荐，基于页面的推荐 矩阵分解：做低纬的投影 等 六种方法 基于网络嵌入式将网络节点做低维表示 用random walk生成序列 line一阶和二阶都表示出来了 刻画出图的样子 推荐更近的节点 bpr是推荐系统里面的baselinepitf是tag的传统baseline 实验效果不错 很多带属性的信息中做随机游走很困难 wordembed 输入一串符号序列 在poi签到数据集做实验 签到地点和时间做排序得到两个序列 学习出user和location的表示 cbow：给顶点的信息，预测点的信息 用户关系刻画：skip-gram 线性加权融合两个信息 问题：序列很长，如不同月份会关联度很差 方法：按天切断，得到几个段上刻画的序列 段的信息也要加入 就是把三个jointly 实验：token2vec 计算所的工作 用学习的东西对传统模型做正则化 动机：重点刻画用户和iterm的关系，但是iterm之间相关性被忽略 而word2vec做适合刻画两个词之间的相关性，刚好 而结合方法就是一部分分解mf，另一部分分解pmi的矩阵，两个分解矩阵叠加在一起 transe知识图谱上补全和预测上很强对head实体加上relation等于tail实体 每个用户都生成了一个自己喜欢的很多的电影的序列 传统做hmm但是会遇到数据稀疏的问题，所以做factorized hmm 而transe要表示，先建系（先前的iterm+user=next iterm） metric leanring：做相似性，距离上的度量 学习一个距离函数，学习mahalanobis 距离矩阵，尽量连在一起的点距离很小，使没有链在一起的点距离大 pull loss就是使同类近，push loss就是使异类远 distance func和loss func 同时学习用户i和表示u 怎么引入用户的很多特征来改进表示，引入变化函数 引入正则化的项 多模态融合嵌入kg 每个iterm都是entity，把知识图谱上面信息用上来 三种信息：图片、文本、信息 transr 文本：输入文本输出也是文本 图片：前端cnn搭自编码器，并且表示是可以叠加的 应用结合图像和文字做推荐，图像的信息属于iterm，basemodel只有图片没有文本，把他考虑成多任务工作，加入文本模型。 推荐系统最近几年停滞了，并没有出现很多数据集下打败svd++，bpr，fm的模型，推荐系统本身是刻画相关度的问题，那么如何用dl的方法？ 效果得看数据 序列性很强用word embed，不强则用网络嵌入，或者结合？ 不同的映射空间怎么到一个映射空间？不能直接相加吧？ 自然语言表示学习 韩先培nlp两条路径：符号主义路线、sub-符号主义路径（直接学习文本表示，直接映射到我们的目的表示） 深度表示学习：句子-&gt;压缩低维的稠密向量 好的表示是什么？（很多困难） 从两个方面介绍nlp表示学习 词嵌入w-&gt;n维连续向量的函数 分布式假设：相似上下文具有相似语意（甚至是跨语言的） 大粒度短语句子1、bow就是简单的带权词向量相加除以平均数，简单高效 2、lstm 3、受到感受野而提出cnn，具有平移缩放不变性 表示学习的应用 分成四种任务 对于ai 那个评测，说不定cnn+lstm已经很好，如果要更好，可以结合一下翻译 语言和计算社会学推荐中知识表示 社交媒体中的网络表示 词汇表示，句子表示]]></content>
  </entry>
  <entry>
    <title><![CDATA[跨语言主题模型]]></title>
    <url>%2F2017%2F09%2F14%2F%E8%B7%A8%E8%AF%AD%E8%A8%80%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[传输文件]]></title>
    <url>%2F2017%2F09%2F09%2F%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[。 1df -h 1ls /dev/disk* 1sudo dd if=ubuntu-16.04.3-server-amd64.iso of=/dev/disk2]]></content>
  </entry>
  <entry>
    <title><![CDATA[郭少分享不完全记录]]></title>
    <url>%2F2017%2F09%2F09%2F%E9%83%AD%E5%B0%91%E5%88%86%E4%BA%AB%E4%B8%8D%E5%AE%8C%E5%85%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[“用TCMalloc或者jemalloc比自己实现的可能好十到一百倍” TCMalloc优点：快、减少了多线程程序中的锁争用情况、小对象的空间最优表现形式 跨度（Span）TCMalloc管理的堆由一系列页面组成。连续的页面由一个“跨度”（Span）对象来表示。由页面号索引的中央数组可以用于找到某个页面所属的跨度。 (跨度a占据了2个页面，跨度b占据了1个页面，跨度c占据了5个页面最后跨度d占据了3个页面) 在一个32位的地址空间中，中央阵列由一个2层的基数树来表示，其中根包含了32个条目，每个叶包含了 2^15个条目（一个32为地址空间包含了 2^20个 4K 页面，所以这里树的第一层则是用2^5整除2^20个页面）。这就导致了中央阵列的初始内存使用需要128KB空间（2^15*4字节），看上去还是可以接受的。 在64位机器上，我们将使用一个3层的基数树。 其中基数树是什么? Linux基数树（radix tree）是将指针与long整数键值相关联的机制，它存储有效率，并且可快速查询，用于指针与整数值的映射（如：IDR机制）、内存管理等。 Linux radix树最广泛的用途是用于内存管理，结构address_space通过radix树跟踪绑定到地址映射上的核心页，该radix树允许内存管理代码快速查找标识为dirty或writeback的页。Linux radix树的API函数在lib/radix-tree.c中实现。 radix树是通用的字典类型数据结构，radix树又称为PAT位树（Patricia Trie or crit bit tree）。Linux内核使用了数据类型unsigned long的固定长度输入的版本。每级代表了输入空间固定位数。 radix tree是一种多叉搜索树，树的叶子结点是实际的数据条目。每个结点有一个固定的、2^n指针指向子结点（每个指针称为槽slot），并有一个指针指向父结点。 Linux内核利用radix树在文件内偏移快速定位文件缓存页，图4是一个radix树样例，该radix树的分叉为4(22)，树高为4，树的每个叶子结点用来快速定位8位文件内偏移，可以定位4x4x4x4=256页，如：图中虚线对应的两个叶子结点的路径组成值0x00000010和0x11111010，指向文件内相应偏移所对应的缓存页。 Linux radix树每个结点有64个slot，与数据类型long的位数相同，图1显示了一个有3级结点的radix树，每个数据条目（item）可用3个6位的键值（key）进行索引，键值从左到右分别代表第1~3层结点位置。没有孩子的结点在图中不出现。因此，radix树为稀疏树提供了有效的存储，代替固定尺寸数组提供了键值到指针的快速查找。 大对象的分配一个大对象的尺寸(&gt; 32K)会被除以一个页面尺寸（4K）并取整（大于结果的最小整数），同时是由中央页面堆来处理的。中央页面堆又是一个自由列表的阵列。对于i &lt; 256而言，第k个条目是一个由k个页面组成的自由列表。第256个条目则是一个包含了长度&gt;= 256个页面的自由列表： k个页面的一次分配通过在第k个自由列表中查找来完成。如果该自由列表为空，那么我们则在下一个自由列表中查找，如此继续。最终，如果必要的话，我们将在最后一个自由列表中查找。如果这个动作也失败了，我们将向系统获取内存（使用sbrk、mmap或者通过在/dev/mem中进行映射）。 如果k个页面的一次分配行为由连续的长度&gt; k的页面满足了，剩下的连续页面将被重新插回到页面堆的对应的自由列表中。 其中内存共享机制mmap是什么?共享内存可以说是最有用的进程间通信方式，也是最快的IPC形式, 因为进程可以直接读写内存，而不需要任何数据的拷贝。（unlike管道和消息队列）并且共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的 UNIX访问文件的传统方法是用open打开它们,如下图两个进程同时读一个文件的同一页的情形，每个进程都要再执行一个存储器内的复制操作将已经被从磁盘读到高速缓冲区的数据再读到自己的地址空间 而mmap()系统调用使得进程之间通过映射同一个普通文件实现共享内存。普通文件被映射到进程地址空间后，进程可以向访问普通内存一样对文件进行访问，不必再调用read()，write（）等操作。 mmap()系统调用形式如下：void mmap ( void addr , size_t len , int prot , int flags , int fd , off_t offset )mmap的作用是映射文件描述符fd指定文件的 [off,off + len]区域至调用进程的[addr, addr + len]的内存区域, 如下图所示:参数解释： (参数fd为即将映射到进程空间的文件描述字，一般由open()返回，同时，fd可以指定为-1，此时须指定flags参数中的MAP_ANON，表明进行的是匿名映射（不涉及具体的文件名，避免了文件的创建及打开，很显然只能用于具有亲缘关系的进程间通信）。len是映射到调用进程地址空间的字节数，它从被映射文件开头offset个字节开始算起。prot 参数指定共享内存的访问权限。可取如下几个值的或：PROT_READ（可读） , PROT_WRITE （可写）, PROT_EXEC （可执行）, PROT_NONE（不可访问）。flags由以下几个常值指定：MAP_SHARED , MAP_PRIVATE , MAP_FIXED，其中,MAP_SHARED , MAP_PRIVATE必选其一，而MAP_FIXED则不推荐使用。offset参数一般设为0，表示从文件头开始映射。参数addr指定文件应被映射到进程空间的起始地址，一般被指定一个空指针，此时选择起始地址的任务留给内核来完成。函数的返回值为最后文件映射到进程空间的地址，进程可直接操作起始地址为该值的有效地址。) jemallochttp://brionas.github.io/2015/01/31/jemalloc%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/ 内存池的实现http://www.cnblogs.com/bangerlee/archive/2011/08/31/2161421.html slab 分配器https://www.ibm.com/developerworks/cn/linux/l-linux-slab-allocator/index.html c++实现内存分配器http://www.codeceo.com/article/efficient-cpp-memory-allocator.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[emnlp2017]]></title>
    <url>%2F2017%2F09%2F04%2Femnlp2017%2F</url>
    <content type="text"><![CDATA[文本摘要和情感分析、mt、文本分析及表示学习 文本摘要和情感分析1.异步多模态文本摘要介绍：同步·：同一时间语言和视频（字幕） 异步：文本新闻，视频新闻 多模态的异步：文本模态，视频模态，音频模态要考虑到：会有很多错误 视频：一系列图像集合，镜头内部语意相似（一系列帧的集合）视觉信息在新闻中占有什么角色若是文本摘要能覆盖这些视觉亮点就可以覆盖那些重点 镜头长的自然更重要一些 具体：重要性，可读性，覆盖视觉信息 salience for text：Lexrank模型：避免可读性差的出现在摘要中（语音识别和文本相比哪个好就用哪个） coverage for visual：采用text image matching model（cvpr2016）是一个文本图片匹配分类器但是还要提取子句，语意角色标注，（扔去时间地点信息） 目标函数：怎么优化有数学……上面如图 数据集：对比试验：有七个对比，肯定是图文匹配结果更好（加引导策略提升不大，引导策略对可读性有提升） 总结：指导策略来可读性高，多模态摘要（未来：低成本标注大量数据集） 2 多文本摘要保持句子间的亲疏度aim：选择更重要的句子、冗余性少一个摘要系统的目标函数考虑到aim 几十篇文档句子数量很多，但目标摘要句子少优点：用图排序（基于迭代，较为复杂）所以不需要大量数据集（区别深度模型） 更新调整随机游走模型中的非对称的转移矩阵 缺点：convergence，词袋模型 传统上随机游走模型：离散mm，加了一个只能转移到自己的节点 3.情感原因发现（coling2010 规则方法 等上述都没公开数据集）情感表达 emnlp2016公布新浪微博数据库 转化为树的分类问题（词语之间关系），只能处理句子，无法处理短语 新的框架：问答系统框架，情感相当阅读文本，情感相当问题，结果相当于答案，答案只有yes和no memory network（memnet 2015）作为基本模型，注意力的加权和但是权重由上下文和本文一起决定相当于卷积注意力加权方法conv-memnet 实验对隔壁传统：rbcbrb+cb+mlsvmword2veccnn多核c-memnet有很多提升 重新定义了测评指标来测短语级别） 引入卷积操作（ntcir emotion cause extraction shared task中英文的数据集） 4多语言的情感分类带情感信息的embedded（bilingual word2vec）中文自身和英文自身预测，互译预测 亚马逊购物评论数据集（…） 几个分类器，主要是LR回归得到分类（只是用GD去调收敛）：bilingual model：直接平行 单语言模型（双语数据）pivotn-driven bilingual model：无需平行语料universal multilingual model：到英语无法直接到达的语言，统一的模型 实验只是想知道能不能获取这样的信息，压缩到embedding空间后一个模型得到的统一的分类器 5情感词典的构建传统的是基于字典和基于语料 词义级和文档级监督信息 基于PMI的词语级监督文档级监督 预测和真实的差异，定义交叉熵rgd来最小化损失函数 由此得到情感表示来构建情感词典 情感词典（HIT 2015 tang）训练softmax分类器（正向和负向） 数据集是twtter 评估分为有监督（特征模板是LIBSVM工具包）和无监督 评估：semeval情感词典 结论：三种来源：预订的情感词典，PMI-SO硬 sentiment anotation，PMI-SO软sentiment annotation mt1. 神经网络phase翻译引入SMt（statistical）的短语来打败nmt（缺点：生成的东西跟原文不相关） 结合nmt和smt有word level和sentence level，那phase level？ 问题：忠实度、覆盖度 预先：smt把核心词提出来，smt写相关的phase，nmt读取那些相关的phase nn based balance 求概率NIST数据集关心的是生成的phase对我们有没有帮助 结果：NPVP：时态分不清楚QP上述都不好 不足：smt翻译有问题，chunking有错误抠不出来，word generation 结论：照片中某个表格smt得出的东西让nmt去玩 未来还能做什么：？ 2. 词语预测机制encoder decoder框架：输出生成initial state（含目标信息）decoder的隐藏层有 缺点：错误回传（某个词翻译错误）s1和s2没有直接的监督 上述不好，则提出新的，要有直接控制，回传机制要有监督后续信息就引入词语预测机制 词语预测不关注词语性质，只关注是否正确。训练过程中对initial state和hidden state都加入词语预测 initial state：某个公式最大化概率 hidden state：最大化……. 且不会影响解码（而且可以加快解码效率） 做法：先词语预测，选出最大的概率的词语，然后把它交给解码，（但注意一定要准确） 实验：LDC中英语料 这个模型加了dropout之后没作用？why 召回率很高，所以能包含将翻译的全部词语 但是一个句子每个词找最好的真的就最后结论最好吗其实就相当于多任务的学习。。 双向分层表示主要是关注encoder关注：长距离依赖不会编码的那么好，语法和语意的缺失 conv-tree-based encoder基于树的encoder的改进 工作：1，双向树的编码2.拓展到字词subword来解决oov问题3.树和叶有近似的词语 1， 双向树的编码自顶向下（标准gru） 2.拓展到字词subword来解决oov问题自己建一个自左向右两两结合的词法树，把它融入到句法树就既包含全局信息也包含局部信息，但是那么就会有重复的翻译 3.对原来的attention机制修改，把两部分的（红色和绿色）分开gating scalar就知道词向量占得多还是短语向量占得多 实验：数据集LDC不仅序列的上下信息，结构的上下文信息也能得到提升词法树的加入不会破坏翻译质量，而且树模型会涨一点贝塔的值是叶向量（节点）和词语向量的占比（节点），各有所长所以gating scale（自动去学权重）是最好的 无监督生成双语词典构建非平行的中英文本语料 前人训练前都需要一个小的词典（种子词典） 发现词向量不同语言空间上是近似的，线性映射联系起来想法：无监督，取消种子词典依赖 学习一个映射G，关键是距离度量earth mover distance （EMD）（离散分布） 想象为土堆和坑洞，最小化整体的方案，而且能得到一词多义 （Wasserstein distance） 两种优化方案1， gan2， 正交先固定g，求解t，再反过来，交替形式，并可以收敛 用WGAN的结果优化EMDOT 实验：五对语言 和用种子词典对比， plus：语言间距离决定因素：两种因素，语言形态和交互关系 信息抽取及自动问答1.关系抽取：知识图谱补全，问答，热点事件问题：难以多句话文本抽取语意 引入关系路径编码器 模型：text encoderpath encoderjoint model 实验做了很多种情况，加path之后都有提升 还自己构造了一个数据集 2.全局优化端对端nn句子级别的关系抽取：转化为序列 2014年的填表table-filling sequence n的平方的表 融入beam searching（acl2016）特征抽取上面有所不同（句法特征）双向LSTM从左向右和从右向左 3.异构知识理解和机器阅读常识阅读理解rocstories数据集，更需要非直接来自于文本的知识 知识获取：把知识表示成推理知识：1时间序列知识event narrative knowledge从文本中无监督得到2实体之间的语意知识 entity semantic knowledge3 实体间的相关关系 entity associate knowledge4 情感一致性规则 sentiment metric learning 来矫正推理规则 怎么去应用：文档和候选答案划分为元素（phase）：对推理合理性进行评估结果中出现的每个元素能找到依据 框架： 得到代价和概率，由此softmax 但是有问题1， 抽取的是词对2， 怎么形成一个统一的框架的3， 时间序列不一定能提出来4， 准确率取决于得到的模型的所用的文本的正误回答之间的距离好坏以及和要用到的文本之间的近似度 4法学5 聊天场景下的回复生成生成式回复是统计模型，那么统一的回复会概率很大但是很无聊 用外部的信号告诉它，gan框架生成器和判别器 文本是离散结构，图像是连续的，这样离散会导致不可导。rl框架是融合多个不同的模型，从而相互作用，但是rl不够直接，端到端模型应该紧密结合在一起。如何变成连续？提出embedding层，从而判别器能顺利传到生成器 取倒数第二层作为loss 百度贴吧作为数据集？好吗这样 主要是解决gan用在单轮对话上如何变为连续，如何定义loss函数（最小化最后一层的两个句子节点的差距） 结果上看上去很好 文本分析及表示学习1. gan对Twitter进行词性标注含有OOV问题 word2vec会丢失形态和结构信息 2. 中文0指代消解任务我吃了一个苹果。很甜0代词（ZP）能帮助替代句子中以前出现的词语ZP的先行词一般是名词短语 常见：作为分类任务，寻找一系列先行可能的词汇，分为可消解和不可消解可消解概率最大的就是我们要找到的 不足：没有利用语意信息（只是针对词法语法），因为很难被表示，没有描述形信息 需要借助句子其他信息：上下文，潜在的那些先行语（memory network逐层去做） 上下文：两层上下文LSTM从左到右和从右到左 潜在的那些先行语：之前是用平均和head word但是都有问题那么就用lstm，一个正向一个反向 就有了np的表示，那么怎样用于memory network呢？每个先行语作为memory，加权求和，每个都加attention，多层的attention （每个trick之后提升，实验要有） 3. ngram2vec得到更好的单词表示（结合工作，自己工作不多） 4. 面向习语的组合语意词的语意组合成句子语意 idioms习语有普通的词但是意思不同（比如撒狗粮） 用gate判断取组合还是非组合 实验如图先要有个习语表，对任何一个ngram都要看是否是习语，不连续的习语无法处理 情感分析的可视化：越红越正面，越蓝越负面圆形是组合，三角是 共享的方式做情感方面的组合任务，构造了一个数据集]]></content>
  </entry>
  <entry>
    <title><![CDATA[依存关系树]]></title>
    <url>%2F2017%2F09%2F04%2F%E4%BE%9D%E5%AD%98%E5%85%B3%E7%B3%BB%E6%A0%91%2F</url>
    <content type="text"><![CDATA[浅层句法分析和依存句法分析 浅层句法分析1、概念：识别句子中的一些相对简单的独立成分（语块） 2、任务：（1）语块的识别与分析；（2）语块之间的依存关系 3、语块的概念：包含位词、非递归短语、子句 4、名词短语句法分析：利用IOB来标记语块，或者块标注法标记语块，并且可以使用下面的方法来训练模型（1）使用SVM；（2）使用WINNOW；（3）使用CRF 依存句法分析1、思路 句法关联建立起词和词之间的关联，包括支配词和从属词。依存语法是指支配与被支配的关系。（依存语法认为动词是句子的中心，其他词都是由动词所支配。） 2、优点 简单，直接考虑词之间的关联；不过多强调句子中的固定词序；受深层语义结构驱动，依存本质是语义上的；形式化程度较短语结构更浅。 3、依存句法分析方法 包括判别式、生成式、确定性依存分析、基于约束满足的分析方法（规则方法）。 ①生成式：考虑联合分布最大时的依存语法树（需要以词和词之间是独立的作为前提条件） ②判别式：考虑条件概率最大（可以不考虑词和词之间的独立性） ③模仿人的认知过程，按照特定方向每次读入一个词。每读入一个词，就进行一次决策。优点：速度快缺点：不可回溯（局部最优）举例：Arc-eager算法（包含四种分析动作：Left-Arc、Right-Arc、Reduce、Shift），算法例子放在后面 4、评价指标 无标记依存正确率：测试集中找到其正确支配词的词所占总次数的百分比；带标记依存正确率：测试集中找到其正确支配词的词，并且依存关系类型也标注正确的词占总词数的百分比；依存正确率：测试集中找到正确支配词非根结点词占所有非根结点词总数的百分比；根正确率：测试集中正确根结点的个数与句子个数的百分比；或者是指测试集中找到正确根结点的句子数占总句子数百分比；完全匹配率：测试集中无标记依存结构完全正确的句子占句子总数的百分比。 5、依存分析树与短语结构树的关系 依存分析树可以由短语结构树得到，但是反过来不行（一颗依存分析树对应多种可能的短语结构树）。 转换步骤： 定义中心词抽取规则，产生中心词表；根据中心词表，为句法树中每个节点选择中心子结点；同一层的非中心子结点中心词依存到中心子结点的中心词上，下一层的中心词依存到上一层的中心词上。 6、Arc-eager算法举例 由于截图不是特别清楚，我们再用文字描述一遍。 左边的S表示栈，右边为待处理的已分词的句子。句子内容为“脚步声打断了我的沉思”。具体步骤描述如下： Left-Arc：将“脚步”存入栈中，之后取栈顶元素（“脚步”）和队头元素（“声”）比较，如果两者有依存关系，且“声”支配“脚步”，则进行Left-Arc操作； Shift：将已经完成的部分放入栈中（如下图）； Left-Arc：比对当前栈顶元素（“声”）和队列元素（“打断”）的依存关系，发现也是Left-Arc，那么进行Left-Arc操作； Shift； Right-Arc：“打断”支配“了”，为Right-Arc； Reduce：由于栈顶元素不是根结点，所以要进行规约，使得“打断”在栈顶； Shift：由于“我”和“打断”没有依存关系，所以将“我”移入栈顶； Left-Arc：“的”支配“我”； Shift； Left-Arc：“沉思”支配“的”； Right-Arc：“打断”支配“沉思”； Reduce：让“打断”在栈顶； End。]]></content>
  </entry>
  <entry>
    <title><![CDATA[kbqa]]></title>
    <url>%2F2017%2F09%2F03%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[KBQA: Learning Question Answering over QA Corpora and Knowledge Bases 知识图谱1、实体链接————&gt;实体识别+实体消歧 2、关系抽取————&gt;词性标注+语法分析+依存关系树+关系分类 KBQAKBQA: Learning Question Answering over QA Corpora and Knowledge Bases 评价标准评价标准：回召率（Recall），精确率（Precision)），F1-Score。而对话系统的评价标准以人工评价为主，以及 BLEU 和 Perplexity。 主流方法一、语义解析（Semantic Parsing）偏linguistic、将自然语言转化为 一系列形式化的逻辑形式（logic form），通过对逻辑形式进行自底向上的解析，得到一种可以表达整个问题语义的逻辑形式，通过相应的查询语句（类似 lambda-Caculus）在知识库中进行查询，从而得出答案。 下图红色部分即逻辑形式，绿色部分 where was Obama born 为自然语言问题，蓝色部分为语义解析进行的相关操作，而形成的语义解析树的根节点则是最终的语义解析结果，可以通过查询语句直接在知识库中查询最终答案 原始论文解读： 该方法来自斯坦福 Berant J, Chou A, Frostig R的Semantic Parsing on Freebase from Question-Answer Pairs，文章发表于2013年的EMNLP 1.什么是语义解析 知识库Freebase由大量的三元组组成，并且这些三元组的实体和实体关系都是形式化的语言 给定一个自然语言的问题：“Where was Obama born？”我们面临的第一个挑战，就是如何建立问题到知识库的映射？ 语义解析的思路是通过对自然语言进行语义上的分析，转化成为一种能够让知识库“看懂”的语义表示，进而通过知识库中的知识，进行推理（Inference）查询（Query），得出答案。 简而言之，语义解析要做的事情，就是将自然语言转化为一种能够让知识库“看懂”的语义表示，这种语义表示即逻辑形式（Logic Form）。 2.什么是逻辑形式 为了能够对知识库进行查询，我们需要一种能够“访问”知识库的逻辑语言，Lambda Dependency-Based Compositional Semantics (Lambda-DCS) 是一种经典的逻辑语言，它用于处理逻辑形式（在实际操作中，逻辑形式会转化 SPARQL query，可以在 Virtuoso engine 上对 Freebase 进行查询）。如果我们把知识库看作是一个数据库，那么逻辑形式（Logic Form）则可以看作是查询语句的表示。 我们用表示一个逻辑形式，用表示知识库，表示实体，表示实体关系（有的也称谓语或属性）。简单而言，逻辑形式分为一元形式（unary）和二元形式（binary）。对于一个一元实体，我们可以查询出对应知识库中的实体，给定一个二元实体关系，可以查到它在知识库中所有与该实体关系相关的三元组中的实体对。并且，我们可以像数据库语言一样，进行连接 Join，求交集 Intersection 和聚合 Aggregate（如计数，求最大值等等）操作。具体来说，逻辑形式有以下形式和操作： 有了上面的定义，我们就可以把一个自然语言问题表示为一个可以在知识库中进行查询的逻辑形式 比如对于问句“Number of dramas starring Tom Cruise?”它对应的逻辑形式是： 当自然语言问题转化为逻辑形式之后，通过相应的逻辑语言（转化为 SPARQL query）查询知识库就可以得到答案。那么，语义解析要如何把自然语言问题正确地转化为相应的逻辑形式呢？ 3.语义解析 KB-QA 的方法框架 语法分析的过程可以看作是自底向上构造语法树的过程，树的根节点，就是该自然语言问题最终的逻辑形式表达。整个流程可以分为两个步骤： ①词汇映射：即构造底层的语法树节点。将单个自然语言短语或单词映射到知识库实体或知识库实体关系所对应的逻辑形式。我们可以通过构造一个词汇表（Lexicon）来完成这样的映射。 ②构建（Composition）：即自底向上对树的节点进行两两合并，最后生成根节点，完成语法树的构建。这一步有很多种方法，诸如构造大量手工规则，组合范畴语法（Combinatory Categorical Grammars，CCG）等等，而今天这篇论文，采用了最暴力的方法，即对于两个节点都可以执行上面所谈到的连接 Join，求交 Intersection，聚合 Aggregate 三种操作，以及这篇文章独创的桥接 Bridging 操作（桥接操作的具体方式稍后会提到）进行结点合并。显然，这种合并方式复杂度是指数级的，最终会生成很多棵语法树，我们需要通过对训练数据进行训练，训练一个分类器，对语法树进行筛选。 自然语言转化为逻辑形式的流程如下图所示： 上图红色部分即逻辑形式，绿色部分 where was Obama born 为自然语言问题，蓝色部分为词汇映射（Lexicon）和构建（Composition）使用的操作，最终形成的语义解析树的根节点即语义解析结果。 接下来，我们还剩最后三个待解决的问题，如何训练分类器？如何构建词汇表？什么是桥接操作？ 训练分类器 ： 分类器的任务是计算每一种语法分析结果 d（Derivation）的概率，作者通过 discriminative log-linear model 进行 modeling，使用 Softmax 进行概率归一化，公式如下： 其中 x 代表自然语言问题,是一个从语法分析结果和 x 中提取出来的 b 维特征向量（该特征向量包含了构造该语法树所有操作的对应特征，每种操作的具体特征之后会提到）, 是 b 维的参数向量。 对于训练数据问题-答案对，最大化 log-likelihood 损失函数，通过 AdaGrad 算法（一种动态调整学习率的随机梯度下降算法）进行参数更新。 可以看出特征向量的训练实际上是一种弱监督训练（准确的说是一种远程监督，DistantSupervison）。 构建词汇表 ： 词汇表即自然语言与知识库实体或知识库实体关系的单点映射，这一操作也被称为对齐（Alignment）。我们知道自然语言实体到知识库实体映射相对比较简单，比如将“Obama was also born in Honolulu.”中的实体 Obama 映射为知识库中的实体 BarackObama，可以使用一些简单的字符串匹配方式进行映射。 但是要将自然语言短语如“was also born in”映射到相应的知识库实体关系，如 PlaceOfBirth， 则较难通过字符串匹配的方式建立映射。怎么办呢？没错，我们可以进行统计。直觉上来说，在文档中，如果有较多的实体对（entity1，entity2）作为主语和宾语出现在 was also born in 的两侧，并且，在知识库中，这些实体对也同时出现在包含 PlaceOfBirth 的三元组中，那么我们可以认为“was also born in”这个短语可以和 PlaceOfBirth 建立映射。 比如（“Barack Obama”，“Honolulu”）,（“MichelleObama”，“Chicago”）等实体对在文档中经常作为“was also born in”这个短语的主语和宾语，并且它们也都和实体关系 PlaceOfBirth 组成三元组出现在知识库中。 有了这样的直觉，我们再来看看这篇文章是怎么构建词汇表的，利用 ReVerbopen IE system 在 ClueWeb09（cmu的，还有12 年版本，ClueWeb12）上抽取 15 millions 个三元组构成一个数据集，如 (“Obama”, “was also born in”, “August 1961”)，可以看出三元组的实体和关系都是自然语言的形式，取出其中的一个三元组子集，对里面的每一个三元组的主语实体和宾语实体通过字符匹配的方式替换为知识库的实体，并使用 SUTime 对数据进行归一化。 如(“Obama”, “was also born in”, “August 1961”) 经过预处理后转化为 (BarackObama, “was also born in”, 1961-08)。 接着我们对每一个三元组中的自然语言短语两边的实体对（entity1，entity2）进行统计，注意，由于自然语言短语 r1 知识库实体关系 r2 的对应关系是多对多的，比如“was also born in”可能对应 PlaceOfBirth，也可能对应 DateOfBrith，我们需要对每一个 r1 进行区分，我们可以通过知识库查询到每一个实体的类型（type），比如 1961-08 的类型是 date 而 honolulu 的类型是 place，我们对 r1 两边的实体类型进行查询可以得到主语实体的类型 t1 和宾语实体的类型 t2，因此 r1 可以进一步表示为 r[t1,t2]，我们对其所在三元组两边的实体进行统计，得到实体对集合F(r[t1,t2])。 同样的，通过对知识库进行统计，对每一个知识库三元组中的实体关系 r2 也统计其两边的实体，可以得到实体对集合 F(r2)，通过比较集合 F(r[t1,t2]) 和集合 F(r2) 类似 Jaccard 距离（集合交集元素数目比集合并集元素个数）这样的特征来确定是否建立词汇映射，如下图所示： 图中绿色字体为 r1，蓝色字体为 r2。作者定义了词汇映射操作的三种特征（用于训练分类器），对齐特征（Alignmentfeatures），文本相似度特征（Textsimilarity features），和词汇化特征（Lexicalizedfeatures），具体内容如下表所示： 其中文本相似度特征中的 s2 指 r2 的 freebase name。 在实际使用中，我们可以通过词性标注（POS）和命名实体识别（NER）来确定哪些短语和单词需要被词汇映射（Lexicon），从而忽略对一些 skippedwords 进行词汇映射。并且，作者还建立了 18 种手工规则，对问题词（questionwords）进行逻辑形式的直接映射，如“where，how many”映射为 Type.Location 和 Count。 桥接操作 ： 完成词汇表的构建后，仍然存在一些问题。比如，对于 go，have，do 这样的轻动词（light verb）难以直接映射到一个知识库实体关系上，其次，有些知识库实体关系极少出现，不容易通过统计的方式找到映射方式，还有一些词比如 actress 实际上是两个知识库实体关系进行组合操作后的结果 (actor ∩ gender.female)。 作者最后提到这个问题有希望通过在知识库上进行随机游走 Random walk 或者使用马尔科夫逻辑 Markov logic 解决，因此我们需要一个补丁，需要找到一个额外的二元关系来将当前的逻辑形式连接起来，那就是桥接。 这里举个具体的例子，比如“Which college did Obama go to?” 假设“Obama” 和 “college” 可被词汇映射映射为 BarackObama 和 Type.University，这里”go to” 却难以找到一个映射，事实上，这里我们需要去寻找一个中间二元关系（即Education）使得上面的句子可以被解析为 (Type.University ∩ Education.BarackObama)，如下图所示： 具体来说，给定两个类型（type）分别为 t1 和 t2 的一元逻辑形式 z1 和 z2，我们需要找到一个二元逻辑形式 b，在 b 对应的实体对类型满足 (t1,t2) 的条件下生成逻辑形式，这就是桥接，由于这里有类型的限制，所以我们可以在知识库中相邻的逻辑关系中暴力搜索符合条件的二元关系 b。（注：在论文中还提到了另外两种需要进行桥接的场景，这里我们则不再赘述）。 同样的，作者也为桥接操作定义了相应的特征（为了分类器的训练），定义如下表所示： 对于构建（composition）的其他三种操作，连接Join，求交集Intersection和聚合Aggregate，作者也定义了相应的特征（为了分类器的训练），如下表所示： 至此，语法树的构建，分类器的训练，和分类器的输入——特征向量的构造方式我们都已经介绍完毕。最后我们再简单的介绍一下实验和实验结果。 4.实验结果 由于语义解析树的构建方式是指数级的，因此，在训练和测试的时候，作者执行了标准的自底向上的集束分析器（Beam-based bottom-up parser）。在这篇论文之前，KB-QA 流行的数据集是由 Cai and Yates (2013) 构建的 Free917，该数据集只包含了 917 组问题答案对，因此，作者构建了一个更大的 benchmark 数据集 WebQuestion，包含了 5810 组问题答案对，该数据集的构建方式我在揭开知识库问答 KB-QA 的面纱·简介篇中进行了简单介绍。 作者测试了仅使用Alignment和Bridging以及都使用下的正确率，如下表所示： 我们可以看出传统的语义解析方法还是存在大量的手工规则，也涉及到了一些 linguistic 的知识，对于没有传统 NLP 先验知识的朋友可能理解起来会稍微困难一些。 however，该方法有些什么缺陷？ 首先，词汇映射是整个算法有效（work）的基点，然而这里采用的词汇映射（尤其是关系映射）是基于比较简单的统计方式，对数据有较大依赖性。最重要的是，这种方式无法完成自然语言短语到复杂知识库关系组合的映射（如 actress 映射为。 其次，在答案获取的过程中，通过远程监督学习训练分类器对语义树进行评分，注意，这里的语义树实际的组合方式是很多的，要训练这样一个强大的语义解析分类器，需要大量的训练数据。我们可以注意到，无论是 Free917 还是 WebQuestion，这两个数据集的问题-答案对都比较少。 其他代表论文： Berant J, Chou A, Frostig R, et al. Semantic Parsing on Freebase from Question-Answer Pairs[C]//EMNLP. 2013, 2(5): 6. Cai Q, Yates A. Large-scale Semantic Parsing via Schema Matching and Lexicon Extension[C]//ACL (1). 2013: 423-433. Kwiatkowski T, Choi E, Artzi Y, et al. Scaling semantic parsers with on-the-fly ontology matching[C]//In Proceedings of EMNLP. Percy. 2013. Fader A, Zettlemoyer L, Etzioni O. Open question answering over curated and extracted knowledge bases[C]//Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014: 1156-1165. 二、信息抽取（Information Extraction）该类方法通过提取问题中的实体，通过在知识库中查询该实体可以得到以该实体节点为中心的知识库子图，子图中的每一个节点或边都可以作为候选答案，通过观察问题依据某些规则或模板进行信息抽取，得到问题特征向量，建立分类器通过输入问题特征向量对候选答案进行筛选，从而得出最终答案。 信息抽取的代表论文 Yao X, Van Durme B. Information Extraction over Structured Data: Question Answering with Freebase[C]//ACL (1). 2014: 956-966。 1.人类的思考方式 想一想，如果有人问你 “what is the name of Justin Bieber brother?” ，并且给你一个知识库，你会怎么去找答案？显然，这个问题的主题（Topic）词就是 Justin Bieber，因此我们会去知识库搜索 Justin Bieber 这个实体，寻找与该实体相关的知识（此时相当于我们确定了答案的范围，得到了一些候选答案）。 接下来，我们去寻找和实体关系 brother 相关的实体，最后得到答案。 而信息抽取的方法，其灵感就是来自于刚才我们的这种思考方式。 2.如何对问题进行信息抽取 还是这个例子，想想我们人类是怎么对这个问题进行信息抽取和推理的。首先，我们会潜意识地对这个句子结构进行分析，下图是 “what is the name of Justin Bieber brother?” 这个问句的语法依存树（Dependency tree），如果你对依存树不了解，可以把它理解成是一种句子成分的形式化描述方式。 我们首先通过依存关系 nsubj (what, name) 和 prep_of (name, brother) 这两条信息知道答案是一个名字，而且这个名字和 brother 有关，当然我们此时还不能判断是否是人名。 进一步，通过 nn (brother, justin bieber) 这条信息我们可以根据 justin bieber 是个人，推导出他的 brother 也是个人，综合前面的信息，我们最终推理出来我们的答案应该是个人名。（注：这里 nsubj 代表名词性主语，prep_of 代表 of 介词修饰，nn 代表名词组合）当确定了最终答案是一个人名，那么我们就很容易在备选答案中筛选出正确答案了。 我们刚才进行的步骤，实际上就是在对问题进行信息抽取，接下来，让我们看看这篇文章具体是怎么实施的。 首先我们要提取的第一个信息就是问题词（question word，记作 qword），例如 who, when, what, where, how, which, why, whom, whose，它是问题的一个明显特征 。 第二个关键的信息，就是问题焦点（question fucus，记作 qfocus），这个词暗示了答案的类型，比如 name/time/place，我们直接将问题词 qword 相关的那个名词抽取出来作为 qfocus，在这个例子中，what name 中的 name 就是 qfocus。 第三个我们需要的信息，就是这个问题的主题词（word topic，记作 qtopic），在这个句子里 Justin Bieber 就是 qtopic，这个词能够帮助我们找到 freebase 中相关的知识，我们可以通过命名实体识别（Named Entity Recognition，NER）来确定主题词，需要注意的是，一个问题中可能存在多个主题词。 最后，第四个我们需要提取的特征，就是问题的中心动词（question verb ，记作 qverb），动词能够给我们提供很多和答案相关的信息，比如 play，那么答案有可能是某种球类或者乐器。我们可以通过词性标注（Part-of-Speech，POS）确定 qverb。 通过对问题提取问题词 qword，问题焦点 qfocus，问题主题词 qtopic 和问题中心动词 qverb 这四个问题特征，我们可以将该问题的依存树转化为问题图（Question Graph），如下图所示： 具体来说，将依存树转化为问题图进行了三个操作： 1）将问题词 qword，问题焦点 qfocus，问题主题词 qtopic 和问题中心动词 qverb 加入相对应的节点中，如 what -&gt; qword=what。 2）如果该节点是命名实体，那就把该节点变为命名实体形式，如 justin -&gt; qtopic=person （justin 对应的命名实体形式是 person）。这一步的目的是因为数据中涉及到的命名实体名字太多了，这里我们只需要区分它是人名 地名 还是其他类型的名字即可。 3）删除掉一些不重要的叶子节点，如限定词（determiner，如 a/the/some/this/each 等），介词（preposition）和标点符号（punctuation）。 从依存树到问题图的转换，实质是就是对问题进行信息抽取，提取出有利于寻找答案的问题特征，删减掉不重要的信息。 3.如何构建特征向量对候选答案进行分类 在候选答案中找出正确答案，实际上是一个二分类问题（判断每个候选答案是否是正确答案），我们使用训练数据问题-答案对，训练一个分类器来找到正确答案。那么分类器的输入特征向量怎么构造和定义呢？ 特征向量中的每一维，对应一个问题-候选答案特征。每一个问题-候选答案特征由问题特征中的一个特征，和候选答案特征的一个特征，组合（combine）而成。 问题特征：我们从问题图中的每一条边 e(s,t)，抽取 4 种问题特征：s，t，s|t，和 s|e|t。如对于边 prep_of(qfocus=name，brother)，我们可以抽取这样四个特征：qfocus=what，brother，qfocus=what|brother 和 qfocus=what|prep_of|brother。 候选答案特征：对于主题图中的每一个节点，我们都可以抽取出以下特征：该节点的所有关系（relation，记作 rel），和该节点的所有属性（property，如 type/gender/age）。（注：我在揭开知识库问答KB-QA的面纱1·简介篇中对知识库中属性和关系的区别进行了讲解） 对于 Justin Bieber 这个 topic 我们可以在知识库找到它对应的主题图，如下图所示： 例如，对于 Jaxon Bieber 这个 topic 节点，我们可以提取出这些特征：gender=male，type=person，rel=sibling 。可以看出关系和属性都刻画了这个候选答案的特征，对判断它是否是正确答案有很大的帮助。 问题-候选答案特征：每一个问题-候选答案特征由问题特征中的一个特征和候选答案特征中的一个特征，组合（combine）而成（组合记作 | ）。 我们希望一个关联度较高的问题-候选答案特征有较高的权重，比如对于问题-候选答案特征 qfocus=money|node type=currency（注意，这里 qfocus=money 是来自问题的特征，而 node type=currency 则是来自候选答案的特征），我们希望它的权重较高，而对于问题-候选答案特征 qfocus=money|node type=person 我们希望它的权重较低。 接下来我们用 WebQuestion 作为训练数据，使用 Stanford CoreNLP 帮助我们对问题进行信息抽取。训练集中约有 3000 个问题，每个问题对应的主题图约含 1000 个节点，共计有 3 million 的节点和 7 million 种问题-候选答案特征。 作者用带L1 正则化的逻辑回归（logistic regression）作为分类器，训练每种问题-候选答案特征的权值（L1 正则化的逻辑回归很适合处理这种稀疏的特征向量，作者表示其效果好于感知机 Percptron 和支持向量机 SVM）。 训练完毕后，得到了 3 万个非零的特征，下表列出了部分特征和它相应的权值，可以看出问题特征和候选答案特征相关度较高时，其权值较高。 因此，在使用的时候，对于每一个候选答案，我们抽取出它的特征（假设有 k 个特征）后，再和问题中的每一个特征两两结合（假设有 m 个特征），那么我们就得到了 km 个问题-候选答案特征，因此我们的输入向量就是一个 km-hot（即 k*m 维为 1，其余维为 0）的 3 万维向量。 在提取候选答案的特征时，我们对每个实体提取了它的关系和属性，在论文中，作者还额外提取了一个更强力的特征，即每一个关系 R 和整个问题 Q 的关联度，可表示为概率的形式 P(R|Q)。那么这个概率如何求解呢？作者采用朴素贝叶斯，backoff model（即）的思想和假设，对于这种复合关系，people.person.parents，也采用 backoff 的思想（即）这个概率进行近似，即： 作者通过 freebase 知识库和两个数据集分别对上面的概率进行统计估算。第一个数据集是 Berant J, Chou A, Frostig R, et al. 中 Semantic Parsing on Freebase from Question-Answer Pairs 使用到的利用 ReVerbopen IE system 在 ClueWeb09 抽取的三元组数据集，包含了 15 million 个三元组，该数据集记作 ReverbMapping，第二个数据集是含 1.2 billion 对齐对（alignment pairs）的 CluewebMapping。 值得一提的是，这些数据中并不直接包含知识库中的关系 rel，那么要如何去估算 P(w|r) 呢？作者采用了一个近似的办法，如果一个数据集中的三元组包含的两个实体和知识库中的关系 r 包含的两个实体一样，就认为这个三元组中存在该关系 r，计数加一。 在两个数据集上分别完成统计和计算后，作者使用了 WebQuestion 进行了测试，分别计算给定每一个问题 Q，答案对应的 relation，它们的概率和其他 relation 相比，排名在 top1, top5, top10, top50, top100 和 100 之后这六种情况的百分比，如下表所示： 可以看出当使用 ClueWebMapping 这个超大数据集训练后，给定一个问题，每次选择概率最高的 rel 时，有 19% 的正确率。因此，作者对于主题图中的每一个候选答案，增加这样一种特征，即该候选答案对应的每一个关系 rel，其概率 P(rel|Q) 在所有关系中的排名情况（如 top1、top3、top5、top50、top100 等等），比如特征 income_relation_rank=top_3（这里因为 relation 是有方向的，所以用 income 前缀对方向加以区分)。 至此，我们已经搞清楚了这篇文章方法涉及到的所有元素：问题特征，候选答案特征，每个候选答案和问题的特征向量以及分类器。最后，我们再简单介绍下实验。 4.论文实验与总结 候选答案的主题图是根据问题中的主题词确定的，而一个问题可能包含多个主题词。作者先通过命名实体识别提取问题中的所有命名实体（如果提取不到一个命名实体，则使用名词短语代替），将所有命名实体输入到 Freebase Search API 中，选取返回排名最高的作为最终的主题词，使用 Freebase Topic API 得到相应的主题图。 当然使用 Freebase Search API 这个方法可能会错过真正和答案相关的主题词（topic），作者也测试了模型在真实的主题词（Gold Retrieval）下的 F1 score，结果如下： 相比Semantic Parsing on Freebase from Question-Answer Pairs 方法在 F1-score 下有较高的提升，到达了 42.0 的 F1-score。 信息抽取的办法，总体来说涉及到了不少 linguistic 的知识，比较符合人类的直觉。虽然也涉及到了很多手工和先验知识的东西，但个人认为它的思想还是很不错的。 作者在构造候选答案特征时，引入了和 P(R|Q) 相关的特征，这个思路是一个很好的思路，但是对 P(R|Q) 的估计方式总体来说还是比较粗暴（比如使用 backoff），个人认为可以使用 Deep Learning 的方法进行提升。 三、向量建模（Vector Modeling）和信息抽取的思想比较接近，根据问题得出候选答案，把问题和候选答案都映射为分布式表达（Distributed Embedding），通过训练数据对该分布式表达进行训练，使得问题和正确答案的向量表达的得分（通常以点乘为形式）尽量高，如下图所示。模型训练完成后则可根据候选答案的向量表达和问题表达的得分进行筛选，得出最终答案。 Question answering with subgraph embeddings（文章发表于 2014 年的 EMNLP 会议）： 问题一是如何将问题和答案映射到低维空间，显然我们不能仅仅将自然语言的问题和答案进行映射，还要将知识库里的知识也映射到这个低维空间中（否则我们就只是在做 QA 而非 KB-QA 了） 第二个问题是，如果做过类似工作（one-shot，imgae caption，word embedding 等）的朋友应该知道，使用这种方法是需要大量数据去训练这个低维空间的分布式表达的，而 KB-QA 中的 benchmark 数据集 WebQuestion 只含有 5800 多个问题答案对，这样的数据是难以训练好这种表达的。 问题的分布式表达：首先我们把自然语言问题进行向量化，作者将输入空间的维度 N 设置为字典的大小+知识库实体数目+知识库实体关系数目，对于输入向量每一维的值设置为该维所代表的单词（当然这一维也可能代表的是某个实体数目或实体关系，对于问题的向量化，这些维数都设置为 0）在问题中出现的次数（一般为 0 或 1 次），可以看出这是一种 multi-hot 的稀疏表达，是一种简化版的词袋模型（Bag-of-words model）。 我们用 q 代表问题，用 Φ(q) 代表 N 维的问题向量，用矩阵 W 将 N 维的问题向量映射到 k 维的低维空间，那么问题的分布式表达即 f(q)=WΦ(q)。 答案的分布式表达：我们想想可以怎样对答案进行向量化，最简单的方式，就是像对问题一样的向量化方式，使用一个简化版的词袋模型。由于答案都是一个知识库实体，那么这样的表达就是一个 one-hot 的表达，显然，这种方式并没有把知识库的知识引入到我们的输入空间中。 第二种方式，我们把知识库想象成一个图，图的节点代表实体，边代表实体关系。通过问题中的主题词可以定位到图中的一个节点，该节点到答案节点有一条路径，我们把该路径上的所有边（实体关系）和点（实体）都以 multi-hot 的形式存下来作为答案的输入向量。 我们这里只考虑一跳（hop）或者两跳的路径，如路径（barack obama, place of birth, honolulu）是一跳，路径（barack obama, people.person.place of birth, location.location.containedby, hawaii）是两跳。因此这种表示是一种 3-hot 或 4-hot 的表示。 第三种方式，让我们回想一下我们在揭开知识库问答KB-QA的面纱3·信息抽取篇介绍的信息抽取的方法，对于每一个候选答案，该答案所对应的属性（type/gender 等）和关系都是能够帮助我们判断它是否是正确答案的重要信息。 因此我们可以把每个候选答案对应的知识库子图（1 跳或 2 跳范围）也加入到输入向量中，假设该子图包含 C 个实体和 D 个关系，那么我们最终的表达是一种 3+C+D-hot 或者 4+C+D-hot 的表达。和信息抽取方法一样，我们也对关系的方向进行区分，因此我们输入向量的大小变为字典的大小+2*（知识库实体数目+知识库实体关系数目）。 同样的，我们用 a 表示答案，用 Ψ(a) 表示答案的输入向量，用矩阵 W 将问题向量映射到 k 维的低维空间，答案的分布式表达即 g(a)=WΨ(a)。 向量得分：最后我们用一个函数表征答案和问题的得分，我们希望问题和它对应的正确答案得尽量高分，通过比较每个候选答案的得分，选出最高的，作为正确答案。得分函数定义为二者分布式表达的点乘，即s(q,a)=f(q)*g(a)。 上述整个流程如下图所示： 训练分布式表达： 对于训练数据集D，我们定义 margin-based ranking 损失函数，公式如下： 做过 zero-shot 或者对 SVM 了解的朋友应该对这个式子不会陌生，其中 表示负样本集 中的一个负样本（错误答案），m 是一个值为 0.1 的 margin。最小化这个损失函数，意味着我们希望正确答案和问题的得分要比任意错误答案的得分高出一个 margin。 和训练 word embedding 一样，为减少计算量，我们通过采样的方式构造负样本，50% 来自随机挑选，50% 来自与问题主题词实体相连的其它路径。 由于 benchmark 数据集 WebQuestion 包含的样本数过少，作者还构造了其他几个数据集： Freebase：选取 freebase 中包含出现频率高于 5 次实体的三元组，得到一个知识库子集（含 2.2M 实体和 7K 关系），对于每一个三元组如（subject, type1.type2.predicate, object），我们通过自动化的方式，生成这样的问题答案对： Quesiton：“What is the predicate of the type2subject?” Answer：object ClueWeb Extractions：由于 Freebase 的三元组都是形式化语言，并不贴近自然语言，我们也用同样的方式将 ClueWeb 上提取出的三元组（subject, “text string”, object）通过少量模板作同样的变换（作者提取了 2M 对三元组）。 这样我们就在 WebQuestion 数据集的基础上，得到了一个新的扩展数据集，该数据集的例子如下表所示： 可以看出，扩增版的数据集，问题大多数都是自动构造的，缺乏多样性和真实性。怎么办呢？我们希望训练数据中问题的分布式表达尽量贴近它所类似的真实问题的分布式表达。 因此，作者在 WikiAnswers 中抓取了 2.2M 问题（不含答案），通过问题的分类标签，将它们分为了 350k 个类簇（可以理解为每个类簇里的自然语言问题它所表达的意思是一样的）。 如下表所示： 接下来我们就可以进行一个多任务学习（multi-task），让同一个类簇的问题得分较高，即s(q1,q2)=f(q1)*g(q2)，其训练方式和之前训练答案和问题得分是一样的。 至此，通过以上两种训练，我们的分布式表达就训练完毕了。 根据问题首先要确定候选答案，这里作者确定候选答案的方式和信息抽取略有不同。首先在从问题中主题词对应的知识库实体出发，通过 beam search 的方式保存10个和问题最相关的实体关系（通过把实体关系当成答案，用s(q,a)=f(q)*g(a)式子的得分作为 beam search 的排序标准）。 接下来选取主题词两跳范围以内的路径，且该路径必须包含这 10 个关系中的关系，将满足条件的路径的终点对应的实体作为候选答案，其中，1 跳路径的权值是 2 跳的 1.5 倍（因为 2 跳包含的元素更多）。 确定完候选答案后，选取s(q,a)=f(q)*g(a)得分最高的作为最终答案。 该方法在 WebQuestion 数据集上进行测试，取得了 39.2 的 F1-Score。 可以看出，相比信息抽取和语义解析的方法，该方法几乎不需要任何手工定义的特征（hand- crafted features），也不需要借助额外的系统（词汇映射表，词性标注，依存树等）。 相对来说，比较简单，也较容易实现，能取得 39.2 的 F1-score 得分（斯坦福 13 年的语义解析方法只有 35.7）也说明了该方法的强大性。通过自动化的方式扩展数据集和多任务训练也部分解决了实验数据不足的缺点。 然而，向量建模方法，是一种趋于黑盒的方法，缺少了解释性（语义解析可以将问题转化成一种逻辑形式的表达，而信息抽取构造的每一维特征的含义也是离散可见的），更重要的是，它也缺少了我们的先验知识和推理（可以看出其 F1-score 略低于 14 年使用了大量先验知识的信息抽取方法，该方法 F1-score 为 42.0），事实上，这也是现在深度学习一个比较有争议的诟病。 就这篇论文的向量建模方法来说，也存在一些问题，比如对问题的向量表示采用了类似词袋模型的方法，这样相当于并未考虑问题的语言顺序（比如 “谢霆锋的爸爸是谁？” 谢霆锋是谁的爸爸？ 这两个问题用该方法得到的表达是一样的，然而这两个问题的意思显然是不同的），且训练分布式表达的模型很简单，相当于一个两层的感知机。这些问题，可以通过深度学习来解决。 随着深度学习的加入，KB-QA 进入了一个新的时代。下一期，我们将进入深度学习篇，由于深度学习可以对传统的三种方法都可以进行提升，因此我们打算将深度学习篇拆成 2-3 篇来进行讲解，进一步揭开 KB-QA 的面纱，敬请期待本系列后续文章。 其他代表论文： Bordes A, Chopra S, Weston J. Question answering with subgraph embeddings[J]. arXiv preprint arXiv:1406.3676, 2014. Yang M C, Duan N, Zhou M, et al. Joint Relational Embeddings for Knowledge-based Question Answering[C]//EMNLP. 2014, 14: 645-650. Bordes A, Weston J, Usunier N. Open question answering with weakly supervised embedding models[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2014: 165-180. 上述方法集中在13-14年发布 深度学习使用卷积神经网络对向量建模方法进行提升： Dong L, Wei F, Zhou M, et al. Question Answering over Freebase with Multi-Column Convolutional Neural Networks[C]//ACL (1). 2015: 260-269. 使用卷积神经网络对语义解析方法进行提升： Yih S W, Chang M W, He X, et al. Semantic parsing via staged query graph generation: Question answering with knowledge base[J]. 2015. 注：该 paper 来自微软，是 ACL 2015 年的 Outstanding paper，也是目前 KB-QA 效果最好的 paper 之一。 使用长短时记忆网络（Long Short-Term Memory，LSTM），卷积神经网络（Convolutional Neural Networks，CNNs）进行实体关系分类： Xu Y, Mou L, Li G, et al. Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths[C]//EMNLP. 2015: 1785-1794. Zeng D, Liu K, Lai S, et al. Relation Classification via Convolutional Deep Neural Network[C]//COLING. 2014: 2335-2344.（Best paper） Zeng D, Liu K, Chen Y, et al. Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks[C]//EMNLP. 2015: 1753-1762. 使用记忆网络（Memory Networks），注意力机制（Attention Mechanism）进行 KB-QA： Bordes A, Usunier N, Chopra S, et al. Large-scale simple question answering with memory networks[J]. arXiv preprint arXiv:1506.02075, 2015. Zhang Y, Liu K, He S, et al. Question Answering over Knowledge Base with Neural Attention Combining Global Knowledge Information[J]. arXiv preprint arXiv:1606.00979, 2016. 以上论文几乎都使用了 Freebase 作为 knowledge base，并且在 WebQuestion 数据集上进行过测试，这里给出各种方法的效果对比图，给大家一个更加直观的感受。 kbqa数据集Benchmark 数据集——WebQuestion。 该数据集由 Berant J, Chou A, Frostig R, et al.在 13 年的论文 Semantic Parsing on Freebase from Question-Answer Pairs 中公开。 btwKBQA结合深度学习的基本思路是什么？代表方法有哪些？]]></content>
  </entry>
  <entry>
    <title><![CDATA[高恒:基于知识图谱的表示学习]]></title>
    <url>%2F2017%2F09%2F01%2F%E9%AB%98%E6%81%92-%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[知识图谱简介、表示学习意义 表示学习历史、基于知识图谱的表示学习 高恒：东南大学知识工程实验室（漆桂林） 表示学习意义在传统的表示学习中，在训练表示学习模型的过程中加入其他的增强信息会提高表示学习的效果通常情况下大家只会加入一些单一的信息，但知识图谱本身的结构信息往往会被忽略。 表示学习历史 基于知识图谱的表示学习我们在表示学习的研究中一直在考虑如何加入知识图谱的结构信息并以此为基础提出了一个新的表示学习方法。在整个研究过程中我们主要关注两个问题：1）如何加入结构信息 2）以及哪些结构信息是可以用来提升表示学习算法的。 单一的信息只是加强某一方面 但是却忽略了知识图谱的结构上的信息 1、首先要哪些数据可以用来描述实体 用一些周围的信息实体（和关系边）来确定描述唯一的实体 那么数据：目标实体的周围实体+连通路径约束关系 2、结构信息把子图放到表示学习里面去训练:①改造transe里面的势能函数，改成概率函数 ②三个不同的context 主语+宾语+关系，联合概率分布，使得三元组整体概率最大加入上采样和下采样，并且提供负例加入训练 最后通过实验证明在加入知识图谱的结构信息后可以提高表示学习在实体类型预测以及实体关系预测的效果，使得我们的算法达到了当前最先进的水平。 “””P.S.实体预测数据集：HITS@10、MEAN RANk、FB40k、WN18 transE在一对一很好，但是在多对一、多对多、一对多上面不太好，而加入图信息则效果都还可以 context比较稀疏的时候效果不太好}“”” 总结融合更多信息（结构、属性、路径、规则、多模态信息）会使表示学习准确率++ 知识图谱表示学习如何应用在传统推理规则任务（不一致检测） 时空、地点这种非三元组 多语言知识融合（NMT常见）会双方同时提高，类似平行语料库会提高实体凑趣 联合任务，识别实体+抽取关系同时做，nlp场景下可以尝试（指代消解任务中） IJCAI：一级铠]]></content>
  </entry>
  <entry>
    <title><![CDATA[matlab探索]]></title>
    <url>%2F2017%2F08%2F19%2Fmatlab%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[运算基础 clear clear x，y，z exp（a） e的a次方 矩阵基础a’ 转置 inv(a) 逆矩阵 .* 矩阵对应元素相乘法 a.^3 矩阵a的每个元素乘三次方 A = [a,a] 把两个矩阵横着合为一个 A = [a; a] 把两个矩阵竖着合为一个 A = magic(4) 四维魔幻方阵 A(4,2) 输出矩阵中某元素 A(1:3,2) 矩阵第一行到第三行的第二列元素 A(3,:) 第三行的所有元素 limit求极限limit(expr,x,a) 符号表达式中x趋近于a时的双向极限 limit(expr,a) 表达式中默认变量趋近于a时的双向极限 limit(expr,x,a,’right’) x从右边趋近于a时的极限 findind = find(X, k) or ind = find(X, k, ‘first’) X矩阵中的元素，从头开始查找，查找到第一个值为K的元素时，返回索引值 ind = find(X, k, ‘last’) 在X中查找，从后往前找，找到第一个值为K的元素，并返回索引值 [row,col] = find(A&gt;10) 在矩阵中找大于10的 [row,col] = find(X) 在矩阵中找x 用于创始化变量的 MATLAB 函数zeros(n)创建一个 n×n 零矩阵zeros(n,m)创建一个 n×m 零矩阵zeros(size(arr))创建一个与数组 arr 的零矩阵ones(n) 创建一个 n×n 元素全为 1 矩阵ones(n,m)创建一个 n×m 元素全为 1 矩阵eye(n) 创建一个 n×n 的单位矩阵eye(n,m) 创建一个 n×m 的单位矩阵length(arr) 返回一个向量的长度或二维数组中最长的那一维的长度 size(arr)返回指定数组的行数和列数 文件x=[0];save x.dat x -ascii”将会创建一个文件 x.dat load filename 矩阵乘除数组右除法A./B 数组左除法 A.\B 矩阵右除法 A/B 矩阵左除法 A\B 数组指数运算 A.^B 数学函数abs(x) 计算 x 的绝对值 字符转换函数char(x) 将矩阵中的数转化为字符,矩阵中的元素就不大于 127 double(x) 将子符串转化为矩阵 int2str(x) 将整数 x 转化为字符串形式 num2str(x) 将带小数点的数转化为一个字符型数组 str2num(x) 将字符串转化为数 查询自己 画图给图增加标题和坐标轴标签将会用到 title, xlabel, ylable 函数 grid on 代表在图象中出现网格线,grid off 代表去除网格线 如：1234x=0:1:10;y=x.^2-10*x+15;plot(x,y);title (&apos;Plot of y=x.^2-10*x+15&apos;); xlabel (&apos;x&apos;);ylabel (&apos;y&apos;); grid on; 创建一个 TIFF 格式的当前图象的图片,并保存在 一个叫 my_image.tif 的文件中1print –dtiff my_image.tif 联合作图:12x = 0:pi/100:2*pi; y1 = sin(2*x);y2 = 2*cos(2*x); plot (x,y1,x,y2); 关于自定义图像：第一方面指定轨迹的颜色, 第二方面指定符号的类型, 第三方面指定线的类型.（可以把重要的点突出，或者某一段是实线其他是虚线） 制作图例：如：显示了 f(x)=sin2x 和它的微分函数的图象 用黑实线代表 f(x),用红虚线代表它的微分函数。 图中有标题,坐标轴标签和网格线。123456x=0:pi/100:2*pi;y1=sin(2*x);y2=2*cos(2*x);plot(x,y1,&apos;k-&apos;,x,y2,&apos;b--&apos;);title(&apos; Plot of f(x)=sin(2x) and its derivative&apos;); xlabel(&apos;x&apos;);ylabel(&apos;y&apos;); legend(&apos;f(x)&apos;,&apos;d/dx f(x)&apos;) grid on; 对数尺寸： 打印数据既可以用对数尺度,也可以用线性尺度。在 x,y 轴上使用这两种尺度的一种或两种 可以组合形成 4 种不同的坐标系。每一种组合者有一个特定的函数。1.plot 函数的 x,y 均用线性尺度2.semilog 函数 x 轴用对数尺度,y 轴将用线性尺度3.semiloge 函数 x 轴用线性尺度,y 轴用对数尺度4.loglog 函数两坐标轴将会都用对数尺度。 在 legend 命令中 pos 的值不同使图像位置不同，如0，1，2 0是自动寻找最佳位置,至少不与数据冲突 ischar(a) a 是字符数组返回 1,否则返回 0isempty(a)a 是空数组返回 1,否则返回 0isinf(a)a 是无穷大,则返回 1,否则返回 0isnan(a) a 不是一个数则返 1,否则返回 0isnumeric(a) a 是一个数值数组返回 1,否则返回 0 在同一坐标系内画出多个图象 x = -pi:pi/20:pi; y1 = sin(x); y2 = cos(x); plot(x,y1,&apos;b-&apos;); hold on; plot(x,y2,&apos;k--&apos;); hold off; legend (&apos;sin x&apos;,&apos;cos x&apos;);]]></content>
  </entry>
  <entry>
    <title><![CDATA[夏令营总结]]></title>
    <url>%2F2017%2F08%2F04%2F%E5%A4%8F%E4%BB%A4%E8%90%A5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[时间总是很快的 最前面要说的话 感谢苟神感谢天恒巨巨感谢所有借过饭卡的人 ʕ •ᴥ•ʔ 开始从头说起当时提前一天来的，就开始了看ng的网课。 然后实现了第0个lab的优先队列 heap! 数据结构加深理解 hackinit八十进前十，第七，fake奖品 (workshop) 回来后， 爆肝赶进度，做完LLRBT实现set容器 看算法。看stl详解。 当时出现了一些bug，还是用printf debug法 接下来进入ml的学习 Linear algebra（特征分解、Moore-Penrose伪逆、迹运算）、概率与信息论（常用概率分布、Bernoulli分布、 Multinoulli分布、高斯分布、指数分布和Laplace分布、Dirac分布和经验分布）、数值计算（Hessian） 我对模型的总结监督：（对样本的适应性+学习结果的限制的合理性）的最优化 如果按照适用问题分类 函数： 部分model之间的关系 model的学习策略上和学习算法上对比： dl：卷积运算通过三个重要的思想: 稀疏交互(sparse interactions)、参数共享(parameter sharing)、等变表示(equivariant representa- tions)。 稀疏交互：使核的大小远小于输入的大小来达到的。举个例子, 当处理一张图像时,输入的图像可能包含成千上万个像素点,但是我们可以通过只占用几十个像素点的核来检测一些小的有意义的特征,例如图像的边缘。意味着我们需要存储的参数更少,而且提高了它的统计效率，更少的计算量。 参数共享：在一个模型的多个函数中使用相同的参数。传统权重矩阵的每一个元素只使用一次,当它乘以输入的一个元素后就再也不会用到了。在卷积神经网络中,核的每一个元素都作用在输入的每一位置上。参数共享保证了我们只需要学习一个参数集合,而不是对于每一位置都需要学习一个单独的参数集合。把模型的存储需求降低至 k 个参数,并且 k 通常要比 m 小很多个数量级。m大概等于n,故·k相对于 m × n 是很小的。总之存储需求和统计效率优于稠密矩阵的乘法运算。 平移等变：如果一个函数满足输入改变,输出也以同样的方式改变这一性质,我们就说是等变的。如果我们把输入中的一个事件向后延时,在输出中仍然 会有完全相同的表示,只是时间延后了。图像与之类似,卷积产生了一个 2 维映射 来表明某些特征在输入中出现的位置。如果我们移动输入中的对象,它的表示也会 在输出中移动同样的量。 池化） rnn实现cnn，word2vec，用tf实现rnn、LSTM把每一个词都转一个向量，然后文章就变成了稀疏矩阵，比如说如果是文本分类问题，就把每一个词对应的向量加在一起统计次数 word2vec有CBOW（类似于蒙特卡洛模拟，只需训练一个二元的分类模型）和skip-gram（目标词汇高频率）两种模式，选后者训练很快而且计算loss function效率高。 间隔太远的输入信息，rnn难以记忆，叫做longterm dependence是传统rnn硬伤。LSTM（1997）内部结构很复杂，包含四层，state在LSTM单元中流动时，每个单元有三个gates，可以对state储存和修改，来实现长程记忆。 bi-rnn普通的mlp对数据长度有限制，虽然rnn可以处理不固定长度的时序data，但是无法利用某个历史输入的未来信息。而bi-rnn将时序相反的两个rnn连接到同一个输出，可以同时获取历史信息和未来信息。 特别是在于需要content的时候非常有用，比如手写文字识别，前一个单词和后一个单词，比如上下文语境做mt任务。 问题在于虽然两个不同时序方向的rnn没有交集可以分开训练（不共用state），但是bptt不能同时更新状态和输出，要人工。 文本分类问题一传统 文本分词和去停用词+词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model）+常用的评价有文档频率、互信息、信息增益、χ²统计量+分类器 二深度 文本的分布式表示：词向量（word embedding）+词向量解决了文本表示的问题，文本分类模型则是利用CNN/RNN等深度学习网络及其变体解决自动特征提取（即特征表达）的问题。 5种CNN/RNN及其变体的模型： 1）fastText 2）TextCNN 3）TextRNNbi 4）TextRNN + Attention 5）TextRCNN（TextRNN + CNN）（aaai2015） 还有shell lab！1tsh&gt; hackweek同义词汇聚类 博客上的总结和笔记： 回顾和期望这一个月,基本上保持了日均在团队12个小时，当然也有经常通宵，经常爆肝。 不过却是挺喜欢十点到十点的生活 而且，更值得开心是在学自己喜欢的东西，在做自己喜欢的事，所以才挺有动力的。 囿于时间，未尽之处：Semi-Supervised Learning、PGM的实现、强化学习、评测比赛、prml 进一步学习，目标：ai lab 承诺：若有幸留下来，因下学期课较少，能保证周一至周五至少三个半天，周六周日必来。 来团队里面，可以和大家多交流，收获成长也会更快 再次谢谢学长们]]></content>
  </entry>
  <entry>
    <title><![CDATA[nlp&word2vec踩坑]]></title>
    <url>%2F2017%2F07%2F30%2Fnlp-word2vec%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[中文词法分析、中文文本分类、知识表示学习、词表示学习、文本挖掘 中文词法分析: THULAC：一个高效的中文词法分析工具包包括中文分词、词性标注功能。已经提供C++、Java、Python版本。 中文文本分类: THUCTC: 一个高效的中文文本分类工具提供高效的中文文本特征提取、分类训练和测试功能。 THUTag: 关键词抽取与社会标签推荐工具包GitHub - YeDeming/THUTag: A Package of Keyphrase Extraction and Social Tag Suggestion提供关键词抽取、社会标签推荐功能，包括TextRank、ExpandRank、Topical PageRank（TPR）、Tag-LDA、Word Trigger Model、Word Alignment Model等算法。 PLDA / PLDA+: 一个高效的LDA分布式学习工具包https://code.google.com/archive/p/plda/ 知识表示学习: 知识表示学习工具包GitHub - Mrlyk423/Relation_Extraction: KnowledgeBase Embedding包括TransE、TransH、TransR、PTransE等算法。 考虑实体描述的知识表示学习算法GitHub - xrb92/DKRL: Representation Learning of Knowledge Graphs with Entity Descriptions 词表示学习: 跨语言词表示学习算法Learning Cross-lingual Word Embeddings via Matrix Co-factorization 主题增强的词表示学习算法GitHub - largelymfs/topical_word_embeddings: A demo code for topical word embedding 可解释的词表示学习算法GitHub - SkTim/OIWE: Online Interpretable Word Embeddings 考虑字的词表示学习算法GitHub - Leonard-Xu/CWE 网络表示学习: 文本增强的网络表示学习算法GitHub - albertyang33/TADW: code for IJCAI2015 paper “Network Representation Learning with Rich Text Information” 文本挖掘： 国内一个NLP工具： 哈工大LTP：http://ir.hit.edu.cn/ 中科院分词ICTCLAS word2vec中的模型至今(2015.8)还是存在不少未解之谜，因此就有不少papers尝试去解释其中一些谜团，或者建立其与其他模型之间的联系，下面是paper list Neural Word Embeddings as Implicit Matrix Factorization Linguistic Regularities in Sparse and Explicit Word Representation Random Walks on Context Spaces Towards an Explanation of the Mysteries of Semantic Word Embeddings]]></content>
  </entry>
  <entry>
    <title><![CDATA[Activation Function]]></title>
    <url>%2F2017%2F07%2F22%2FActivation-Function%2F</url>
    <content type="text"><![CDATA[激活函数都有哪些？都长什么样？有哪些优缺点？ Sigmoid 能够把输入的连续实值“压缩”到0和1之间。 缺点： 1、Sigmoids saturate and kill gradients： sigmoid 有一个非常致命的缺点，当输入非常大或者非常小的时候，这些神经元的梯度是接近于0的，从图中可以看出梯度的趋势。所以，你需要尤其注意参数的初始值来尽量避免saturation的情况。如果初始值很大的话，大部分神经元可能都会处在saturation的状态而把gradient kill掉，这会导致网络变的很难学习。 2、Sigmoid 的 output 不是0均值： 这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如果数据进入神经元的时候是正的，那么w计算出的梯度也会始终都是正的。 tanh 与 sigmoid 不同的是，tanh 是0均值的 ReLUf(x)=max(0,x) ReLU 的优点： 1、使用 ReLU 得到的SGD的收敛速度会比 sigmoid/tanh 快很多。（因为它是linear，而且 non-saturating） 2、相比于 sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。 ReLU 的缺点： 举个例子：一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了。如果这个情况发生了，那么这个神经元的梯度就永远都会是0. 实际操作中，如果learning rate 很大，那么很有可能网络中的40%的神经元都”dead”了。 当然，如果设置一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。 Leaky-ReLU、P-ReLU、R-ReLULeaky ReLUs： 就是用来解决这个 “dying ReLU” 的问题的。与 ReLU 不同的是：12f(x)=αx，(x&lt;0)f(x)=x，(x&gt;=0) α 是一个很小的常数 Parametric ReLU： 对于 Leaky ReLU 中的α，通常都是通过先验知识人工赋值的。 见论文《Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification》 Randomized ReLU：在训练过程中，α 是从一个高斯分布 U(l,u) 中 随机出来的，然后再测试过程中进行修正（有点像dropout的用法）。 MaxoutMaxout出现在ICML2013上,见论文 怎么选用激活函数？使用 ReLU，要小心设置 learning rate，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.]]></content>
  </entry>
  <entry>
    <title><![CDATA[多种kernel对比]]></title>
    <url>%2F2017%2F07%2F21%2F%E5%A4%9A%E7%A7%8Dkernel%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[SVM关键是选取核函数的类型，主要有线性内核，多项式内核，径向基内核（RBF），sigmoid核。这些函数中应用最广的应该就是RBF核了，无论是小样本还是大样本，高维还是低维等情况，RBF核函数均适用，它相比其他的函数有一下优点： 1）RBF核函数可以将一个样本映射到一个更高维的空间，而且线性核函数是RBF的一个特例，也就是说如果考虑使用RBF，那么就没有必要考虑线性核函数了。 2）与多项式核函数相比，RBF需要确定的参数要少，核函数参数的多少直接影响函数的复杂程度。另外，当多项式的阶数比较高时，核矩阵的元素值将趋于无穷大或无穷小，而RBF则在上，会减少数值的计算困难。 3）对于某些参数，RBF和sigmoid具有相似的性能。 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况 “更多的时候，适合当前数据集的核函数不是现成就有的，是需要我们自己去编程实现的”]]></content>
  </entry>
  <entry>
    <title><![CDATA[svm中三个基础问题]]></title>
    <url>%2F2017%2F07%2F21%2Fsvm%E4%B8%AD%E4%B8%89%E4%B8%AA%E5%9F%BA%E7%A1%80%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[对偶问题\KTT条件\凸优化 什么是对偶问题任何一个求极大化的线性规划问题都有一个求极小化的线性规划问题与之对应，反之亦然，如果我们把其中一个叫原问题，则另一个就叫做它的对偶问题 对称型和非对称型 最优化与KTT条件最优化问题可以根据目标函数和约束条件的类型进行分类: 1). 如果目标函数和约束条件都为变量的线性函数, 称该最优化问题为线性规划; 2). 如果目标函数为变量的二次函数, 约束条件为变量的线性函数, 称该最优化问题为二次规划; 3). 如果目标函数或者约束条件为变量的非线性函数, 称该最优化问题为非线性规划. KTT条件是指在满足一些有规则的条件下, 一个非线性规划(Nonlinear Programming)问题能有最优化解法的一个必要和充分条件. 这是一个广义化拉格朗日乘数的成果. 一般地, 一个最优化数学模型的列标准形式参考开头的式子, 所谓 Karush-Kuhn-Tucker 最优化条件，就是指上式的最优点x∗必须满足下面的条件: 1). 约束条件满足gi(x∗)≤0,i=1,2,…,p, 以及,hj(x∗)=0,j=1,2,…,q 2). ∇f(x∗)+∑i=1pμi∇gi(x∗)+∑j=1qλj∇hj(x∗)=0, 其中∇为梯度算子; 3). λj≠0且不等式约束条件满足μi≥0,μigi(x∗)=0,i=1,2,…,p KKT条件第一项是说最优点x∗必须满足所有等式及不等式限制条件, 也就是说最优点必须是一个可行解, 这一点自然是毋庸置疑的. 第二项表明在最优点x∗, ∇f必须是∇g和∇h的线性組合, μ和λ都叫作拉格朗日乘子. 所不同的是不等式限制条件有方向性, 所以每一个μ都必须大于或等于零, 而等式限制条件没有方向性，所以λ没有符号的限制, 其符号要视等式限制条件的写法而定. 凸优化问题同时满足如下两个条件的优化问题称为凸优化： 1）目标函数(objective fucntion)是凸函数； 2）可行集合(feasible set)必须是凸集； 即在凸集上寻找凸函数的全局最值的过程称为凸优化。]]></content>
  </entry>
  <entry>
    <title><![CDATA[先验概率和后验概率]]></title>
    <url>%2F2017%2F07%2F18%2F%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E5%92%8C%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[基础知识2333 先验概率：事件发生前的预判概率。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率，如P(x),P(y)。 后验概率：事件发生后求的反向条件概率；或者说，基于先验概率求得的反向条件概率。概率形式与条件概率相同。 条件概率：一个事件发生后另一个事件发生的概率。一般的形式为P(x|y)表示y发生的条件下x发生的概率。 贝叶斯公式：1P(y|x) = ( P(x|y) * P(y) ) / P(x) 这里： P(y|x) 是后验概率，一般是我们求解的目标。 P(x|y) 是条件概率，又叫似然概率，一般是通过历史数据统计得到。一般不把它叫做先验概率，但从定义上也符合先验定义。 P(y) 是先验概率，一般都是人主观给出的。贝叶斯中的先验概率一般特指它。 P(x) 其实也是先验概率，只是在贝叶斯的很多应用中不重要（因为只要最大后验不求绝对值），需要时往往用全概率公式计算得到。 实例：假设y是文章种类，是一个枚举值；x是向量，表示文章中各个单词的出现次数。 在拥有训练集的情况下，显然除了后验概率P(y|x)中的x来自一篇新文章无法得到，p(x),p(y),p(x|y)都是可以在抽样集合上统计出的。 最大似然理论：认为P(x|y)最大的类别y，就是当前文档所属类别。 即Max P(x|y) = Max p(x1|y)p(x2|y)…p(xn|y), for all y 贝叶斯理论：认为需要增加先验概率p(y)，因为有可能某个y是很稀有的类别几千年才看见一次，即使P(x|y)很高，也很可能不是它。 所以y = Max P(x|y) * P(y), 其中p(y)一般是数据集里统计出来的。]]></content>
  </entry>
  <entry>
    <title><![CDATA[最优化方法总结]]></title>
    <url>%2F2017%2F07%2F18%2F%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[梯度下降法（Gradient Descent） 牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods） 共轭梯度法（Conjugate Gradient） 启发式优化方法 解决约束优化问题——拉格朗日乘数法 1. 梯度下降法（Gradient Descent）以线性回归算法来对三种梯度下降法进行比较 (假设函数h (损失函数J 1. 批量梯度下降法BGD更新每一参数时都使用所有的样本来进行更新,具体步骤： (1) 损失函数对θ求偏导，得到每个θ对应的的梯度：(2) 由于是最小化风险函数，所以按照每个参数θ的梯度负方向来更新每个θ： 具体的伪代码形式为： repeat{ （for every j=0, … , n）} 优点：全局最优解；易于并行实现； 缺点：当样本数目很多时，训练过程会很慢。 它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果样本数目m很大，那么可想而知这种方法的迭代速度很慢。所以，这就引入了随机梯度下降。 2. 随机梯度下降法SGD将上面的损失函数写为： 利用每个样本的损失函数对θ求偏导得到对应的梯度，来更新θ： 具体的伪代码形式为： Randomly shuffle dataset； repeat{for i=1, … , m{ }}优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。 3. 小批量梯度下降法MBGD算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这是小批量梯度下降法 MBGD在每次更新参数时使用b个样本（b一般为10），其具体的伪代码形式为：b=10, m=1000.Repeat{for i=1, 11, 21, 31, … , 991{(for every j=0, … , nn) }} 2. 牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods）使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。 求解: 我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f (x) = 0的解。迭代公式：由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。 从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。 2）拟牛顿法拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。 具体步骤:(Bk是一个对称正定矩阵) 首先构造目标函数在当前迭代xk的二次模型： 取这个二次模型的最优解作为搜索方向，并且得到新的迭代点： 要求步长ak 满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hesse矩阵Bk 代替真实的Hesse矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk 的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型： 具体地，要求 从而得到 这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。 3. 共轭梯度法（Conjugate Gradient）绿色为梯度下降法，红色代表共轭梯度法 4. 启发式优化方法5. 解决约束优化问题——拉格朗日乘数法]]></content>
  </entry>
  <entry>
    <title><![CDATA[分类器性能评估方法对比]]></title>
    <url>%2F2017%2F07%2F14%2F%E5%88%86%E7%B1%BB%E5%99%A8%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[对于分类器，或者说分类算法，评价指标主要有precision，recall，F-score1，ROC和AUC。06icml)The Relationship Between Precision-Recall and ROC Curves https://www.52ml.net/19370.html 精准率和召回率系统检索到的相关文档（A）（TP） 系统检索到的不相关文档（B）(FP) 相关但是系统没有检索到的文档（C）(FN) 不相关且没有被系统检索到的文档（D）(TN) 则：召回率R：用检索到相关文档数作为分子，所有相关文档总数作为分母，即R = A / ( A + C ) 准确率P：用检索到相关文档数作为分子，所有检索到的文档总数作为分母．即P = A / ( A + B ). F1F1是对准确率和召回率的综合，是模型准确率和召回率的一种调和平均，它的最大值是1，最小值是0。 F1-score(F1-分数)：2×准确率×召回率/（准确率+召回率） ROC和AUCROC和AUC是一回事，通常用于二分类的评价 ROC:ROC曲线的横坐标为false positive rate（FPR,假正率），纵坐标为true positive rate（TPR，真正率，召回率） TPR:TPR = TP /（TP + FN） TNR:TNR = TN /（TN + FP） FPR:FPR = FP /（FP + TN） FNR:FNR = FN /（TP + FN） AUC:被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。 首先AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。 ROC曲线越接近左上角，该分类器的性能越好。 下图是ROC曲线和Precision-Recall曲线5的对比： （图片来自论文(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线. (a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。]]></content>
  </entry>
  <entry>
    <title><![CDATA[kd tree]]></title>
    <url>%2F2017%2F07%2F14%2Fkd-tree%2F</url>
    <content type="text"><![CDATA[kNN算法每次在查询k个最近邻的时候都需要遍历全集才能计算出来，可想而且如果训练样本很大的话，代价还是很大的 优化：kd tree该树的功能就是在高维空间下进行一个快速的最近邻查询。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;#include &lt;string&gt;#include &lt;cmath&gt;using namespace std; struct KdTree&#123; vector&lt;double&gt; root; KdTree* parent; KdTree* leftChild; KdTree* rightChild; //默认构造函数 KdTree()&#123;parent = leftChild = rightChild = NULL;&#125; //判断kd树是否为空 bool isEmpty() &#123; return root.empty(); &#125; //判断kd树是否只是一个叶子结点 bool isLeaf() &#123; return (!root.empty()) &amp;&amp; rightChild == NULL &amp;&amp; leftChild == NULL; &#125; //判断是否是树的根结点 bool isRoot() &#123; return (!isEmpty()) &amp;&amp; parent == NULL; &#125; //判断该子kd树的根结点是否是其父kd树的左结点 bool isLeft() &#123; return parent-&gt;leftChild-&gt;root == root; &#125; //判断该子kd树的根结点是否是其父kd树的右结点 bool isRight() &#123; return parent-&gt;rightChild-&gt;root == root; &#125; &#125;; int data[6][2] = &#123;&#123;2,3&#125;,&#123;5,4&#125;,&#123;9,6&#125;,&#123;4,7&#125;,&#123;8,1&#125;,&#123;7,2&#125;&#125;; template&lt;typename T&gt; vector&lt;vector&lt;T&gt; &gt; Transpose(vector&lt;vector&lt;T&gt; &gt; Matrix) &#123; unsigned row = Matrix.size(); unsigned col = Matrix[0].size(); vector&lt;vector&lt;T&gt; &gt; Trans(col,vector&lt;T&gt;(row,0)); for (unsigned i = 0; i &lt; col; ++i) &#123; for (unsigned j = 0; j &lt; row; ++j) &#123; Trans[i][j] = Matrix[j][i]; &#125; &#125; return Trans; &#125; template &lt;typename T&gt; T findMiddleValue(vector&lt;T&gt; vec) &#123; sort(vec.begin(),vec.end()); auto pos = vec.size() / 2; return vec[pos]; &#125; //构建kd树 void buildKdTree(KdTree* tree, vector&lt;vector&lt;double&gt; &gt; data, unsigned depth) &#123; //样本的数量 unsigned samplesNum = data.size(); //终止条件 if (samplesNum == 0) &#123; return; &#125; if (samplesNum == 1) &#123; tree-&gt;root = data[0]; return; &#125; //样本的维度 unsigned k = data[0].size(); vector&lt;vector&lt;double&gt; &gt; transData = Transpose(data); //选择切分属性 unsigned splitAttribute = depth % k; vector&lt;double&gt; splitAttributeValues = transData[splitAttribute]; //选择切分值 double splitValue = findMiddleValue(splitAttributeValues); //cout &lt;&lt; &quot;splitValue&quot; &lt;&lt; splitValue &lt;&lt; endl; // 根据选定的切分属性和切分值，将数据集分为两个子集 vector&lt;vector&lt;double&gt; &gt; subset1; vector&lt;vector&lt;double&gt; &gt; subset2; for (unsigned i = 0; i &lt; samplesNum; ++i) &#123; if (splitAttributeValues[i] == splitValue &amp;&amp; tree-&gt;root.empty()) tree-&gt;root = data[i]; else &#123; if (splitAttributeValues[i] &lt; splitValue) subset1.push_back(data[i]); else subset2.push_back(data[i]); &#125; &#125; //子集递归调用buildKdTree函数 tree-&gt;leftChild = new KdTree; tree-&gt;leftChild-&gt;parent = tree; tree-&gt;rightChild = new KdTree; tree-&gt;rightChild-&gt;parent = tree; buildKdTree(tree-&gt;leftChild, subset1, depth + 1); buildKdTree(tree-&gt;rightChild, subset2, depth + 1);&#125;//逐层打印kd树void printKdTree(KdTree *tree, unsigned depth)&#123; for (unsigned i = 0; i &lt; depth; ++i) cout &lt;&lt; &quot;\t&quot;; for (vector&lt;double&gt;::size_type j = 0; j &lt; tree-&gt;root.size(); ++j) cout &lt;&lt; tree-&gt;root[j] &lt;&lt; &quot;,&quot;; cout &lt;&lt; endl; if (tree-&gt;leftChild == NULL &amp;&amp; tree-&gt;rightChild == NULL )//叶子节点 return; else //非叶子节点 &#123; if (tree-&gt;leftChild != NULL) &#123; for (unsigned i = 0; i &lt; depth + 1; ++i) cout &lt;&lt; &quot;\t&quot;; cout &lt;&lt; &quot; left:&quot;; printKdTree(tree-&gt;leftChild, depth + 1); &#125; cout &lt;&lt; endl; if (tree-&gt;rightChild != NULL) &#123; for (unsigned i = 0; i &lt; depth + 1; ++i) cout &lt;&lt; &quot;\t&quot;; cout &lt;&lt; &quot;right:&quot;; printKdTree(tree-&gt;rightChild, depth + 1); &#125; cout &lt;&lt; endl; &#125;&#125;//计算空间中两个点的距离double measureDistance(vector&lt;double&gt; point1, vector&lt;double&gt; point2, unsigned method)&#123; if (point1.size() != point2.size()) &#123; cerr &lt;&lt; &quot;Dimensions don&apos;t match！！&quot; ; exit(1); &#125; switch (method) &#123; case 0://欧氏距离 &#123; double res = 0; for (vector&lt;double&gt;::size_type i = 0; i &lt; point1.size(); ++i) &#123; res += pow((point1[i] - point2[i]), 2); &#125; return sqrt(res); &#125; case 1://曼哈顿距离 &#123; double res = 0; for (vector&lt;double&gt;::size_type i = 0; i &lt; point1.size(); ++i) &#123; res += abs(point1[i] - point2[i]); &#125; return res; &#125; default: &#123; cerr &lt;&lt; &quot;Invalid method!!&quot; &lt;&lt; endl; return -1; &#125; &#125;&#125;//在kd树tree中搜索目标点goal的最近邻//输入：目标点；已构造的kd树//输出：目标点的最近邻vector&lt;double&gt; searchNearestNeighbor(vector&lt;double&gt; goal, KdTree *tree)&#123; /*第一步：在kd树中找出包含目标点的叶子结点：从根结点出发， 递归的向下访问kd树，若目标点的当前维的坐标小于切分点的 坐标，则移动到左子结点，否则移动到右子结点，直到子结点为 叶结点为止,以此叶子结点为“当前最近点” */ unsigned k = tree-&gt;root.size();//计算出数据的维数 unsigned d = 0;//维度初始化为0，即从第1维开始 KdTree* currentTree = tree; vector&lt;double&gt; currentNearest = currentTree-&gt;root; while(!currentTree-&gt;isLeaf()) &#123; unsigned index = d % k;//计算当前维 if (currentTree-&gt;rightChild-&gt;isEmpty() || goal[index] &lt; currentNearest[index]) &#123; currentTree = currentTree-&gt;leftChild; &#125; else &#123; currentTree = currentTree-&gt;rightChild; &#125; ++d; &#125; currentNearest = currentTree-&gt;root; /*第二步：递归地向上回退， 在每个结点进行如下操作： (a)如果该结点保存的实例比当前最近点距离目标点更近，则以该例点为“当前最近点” (b)当前最近点一定存在于某结点一个子结点对应的区域，检查该子结点的父结点的另 一子结点对应区域是否有更近的点（即检查另一子结点对应的区域是否与以目标点为球 心、以目标点与“当前最近点”间的距离为半径的球体相交）；如果相交，可能在另一 个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点，接着递归进行最 近邻搜索；如果不相交，向上回退*/ //当前最近邻与目标点的距离 double currentDistance = measureDistance(goal, currentNearest, 0); //如果当前子kd树的根结点是其父结点的左孩子，则搜索其父结点的右孩子结点所代表 //的区域，反之亦反 KdTree* searchDistrict; if (currentTree-&gt;isLeft()) &#123; if (currentTree-&gt;parent-&gt;rightChild == NULL) searchDistrict = currentTree; else searchDistrict = currentTree-&gt;parent-&gt;rightChild; &#125; else &#123; searchDistrict = currentTree-&gt;parent-&gt;leftChild; &#125; //如果搜索区域对应的子kd树的根结点不是整个kd树的根结点，继续回退搜索 while (searchDistrict-&gt;parent != NULL) &#123; //搜索区域与目标点的最近距离 double districtDistance = abs(goal[(d+1)%k] - searchDistrict-&gt;parent-&gt;root[(d+1)%k]); //如果“搜索区域与目标点的最近距离”比“当前最近邻与目标点的距离”短，表明搜索 //区域内可能存在距离目标点更近的点 if (districtDistance &lt; currentDistance )//&amp;&amp; !searchDistrict-&gt;isEmpty() &#123; double parentDistance = measureDistance(goal, searchDistrict-&gt;parent-&gt;root, 0); if (parentDistance &lt; currentDistance) &#123; currentDistance = parentDistance; currentTree = searchDistrict-&gt;parent; currentNearest = currentTree-&gt;root; &#125; if (!searchDistrict-&gt;isEmpty()) &#123; double rootDistance = measureDistance(goal, searchDistrict-&gt;root, 0); if (rootDistance &lt; currentDistance) &#123; currentDistance = rootDistance; currentTree = searchDistrict; currentNearest = currentTree-&gt;root; &#125; &#125; if (searchDistrict-&gt;leftChild != NULL) &#123; double leftDistance = measureDistance(goal, searchDistrict-&gt;leftChild-&gt;root, 0); if (leftDistance &lt; currentDistance) &#123; currentDistance = leftDistance; currentTree = searchDistrict; currentNearest = currentTree-&gt;root; &#125; &#125; if (searchDistrict-&gt;rightChild != NULL) &#123; double rightDistance = measureDistance(goal, searchDistrict-&gt;rightChild-&gt;root, 0); if (rightDistance &lt; currentDistance) &#123; currentDistance = rightDistance; currentTree = searchDistrict; currentNearest = currentTree-&gt;root; &#125; &#125; &#125;//end if if (searchDistrict-&gt;parent-&gt;parent != NULL) &#123; searchDistrict = searchDistrict-&gt;parent-&gt;isLeft()? searchDistrict-&gt;parent-&gt;parent-&gt;rightChild: searchDistrict-&gt;parent-&gt;parent-&gt;leftChild; &#125; else &#123; searchDistrict = searchDistrict-&gt;parent; &#125; ++d; &#125;//end while return currentNearest;&#125;int main()&#123; vector&lt;vector&lt;double&gt; &gt; train(6, vector&lt;double&gt;(2, 0)); for (unsigned i = 0; i &lt; 6; ++i) for (unsigned j = 0; j &lt; 2; ++j) train[i][j] = data[i][j]; KdTree* kdTree = new KdTree; buildKdTree(kdTree, train, 0); printKdTree(kdTree, 0); vector&lt;double&gt; goal; goal.push_back(3); goal.push_back(4.5); vector&lt;double&gt; nearestNeighbor = searchNearestNeighbor(goal, kdTree); vector&lt;double&gt;::iterator beg = nearestNeighbor.begin(); cout &lt;&lt; &quot;The nearest neighbor is: &quot;; while(beg != nearestNeighbor.end()) cout &lt;&lt; *beg++ &lt;&lt; &quot;,&quot;; cout &lt;&lt; endl; return 0;&#125; 构造的kd树如下 利用kd树搜索最近邻 输入：已构造的kd树；目标点x; 输出：x的最近邻 1.在kd树中找出包含目标点x的叶结点：从根结点出发，递归的向下访问kd树，若目标点x的当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止。 2.以此叶结点为“当前最近点” 3.递归地向上回退，在每个结点进行以下操作： （a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”; (b)当前最近点一定存在于某结点一个子结点对应的区域，检查该子结点的父结点的另一子结点对应区域是否有更近的点（即检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的球体相交）；如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点，接着递归进行最近邻搜索；如果不相交，向上回退 4.当回退到根结点时，搜索结束，最后的“当前最近点”即为x的最近邻点。]]></content>
  </entry>
  <entry>
    <title><![CDATA[kNN算法]]></title>
    <url>%2F2017%2F07%2F14%2FkNN%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1、knn?k-近邻算法采用测量不同特征值之间距离的方法进行分类.思想很简单：如果一个样本的特征空间中最为临近（欧式距离进行判断）的K个点大都属于某一个类，那么该样本就属于这个类。这就是物以类聚的思想。优点：精度高、对异常值不敏感、无数据输入假定缺点：计算复杂度高、空间复杂度高适用数据范围：数值型和标称型 通过计算新数据与训练数据特征值之间的距离，然后选取K（K&gt;=1）个距离最近的进行分类判断 1.1 关键1、数据的所有特征都要做可比较的量化。若是数据特征中存在非数值的类型，必须采取手段将其量化为数值。另外，样本有多个参数，每一个参数都有自己的定义域和取值范围，他们对distance计算的影响也就不一样，so样本参数必须做一些scale处理，usually所有特征的数值都采取归一化处置。 2、需要一个distance函数以计算两个样本之间的距离。距离的定义有很多，如欧氏距离、余弦距离、汉明距离、曼哈顿距离等等.about相似性度量的方法可参考‘漫谈：机器学习中距离和相似性度量方法’。usually选欧氏距离作为距离度量，但是这是只适用于连续变量。在文本分类这种非连续变量情况下，汉明距离可以用来作为度量。btw，如果运用一些特殊的算法来计算度量的话，K近邻分类精度可显著提高，如运用大边缘最近邻法或者近邻成分分析法。 3，确定K的值K是一个自定义的常数，K的值也直接影响最后的估计，一种选择K值得方法是使用 cross-validate（交叉验证）误差统计选择法(就是数据样本的一部分作为训练样本，一部分作为测试样本，比如选择95%作为训练样本，剩下的用作测试样本。)通过训练数据训练一个机器学习模型，然后利用测试数据测试其误差率。 cross-validate（交叉验证）误差统计选择法就是比较不同K值时的交叉验证平均误差率，选择误差率最小的那个K值。例如选择K=1,2,3,… ， 对每个K=i做100次交叉验证，计算出平均误差，然后比较、选出最小的那个。 2、原理存在一个样本数据集合（知道所属的对应分类）。输入没有标签的数据后，将这个没有标签的数据的每个特征与样本集中的数据对应的特征进行比较，然后算法提取样本中特征最相似的数据（最邻近）的分类标签。我们只选择样本数据集中前 k 个最相似的数据，通常 k 是不大于 20 的整数。最后，选择 k 个最相似数据中出现次数最多的类别，作为新数据的分类。 3.1机器学习实战上的电影例子用 K-近邻算法来分来爱情片和动作片 即使不知道未知电影属于哪种类型，我们也可以通过某种方法计算出来。首先要计算未知电影与样本集中其他电影的距离，计算方法很简单，即欧式空间距离(Euclidean Distance) 现在我们得到了样本集中所有电影与未知电影的距离，按照距离递增排序，可以找到 k 个距离最近的电影，例如 k=3 K-近邻算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片。 3.2knn流程距离计算所需要的值，最好是结构化的数据。 KNN算法的过程为:选择一种距离计算方式, 通过数据所有的特征计算新数据与已知类别数据集中的数据点的距离按照距离递增次序进行排序，选取与当前距离最小的k个点对于离散分类，返回k个点出现频率最多的类别作预测分类；对于回归则返回k个点的加权值作为预测值 使用算法：首先需要输入样本数据和待分类数据，然后运行k-近邻算法判定待分类数据分别属于哪个分类，最后应用计算出的分类执行后续的处理 4.python实现knn4.1python导入数据将下面的代码保存到名为 kNN.py 的文本文件中： 12345678from numpy import *def createDataSet():dataSet = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) # 创建一个2x2的数组labels = [&apos;A&apos;, &apos;A&apos;, &apos;B&apos;, &apos;B&apos;] # 创建一个长度为4的列表return dataSet, labels 1234&gt;&gt;&gt; import kNN&gt;&gt;&gt; group,labels=kNN.createDataSet()&gt;&gt;&gt; kNN.classify0([0,0],group,labels,3) 4.2实现knn123456789101112131415161718192021from operator import itemgetter# inVec为待分类向量，dataSet和labels为数据集，k是最近点的个数def classify0(inVec, dataSet, labels, k):numberOfLines = dataSet.shape[0] # 获得数据集样本数量diffMat = tile(inVec, (numberOfLines, 1)) - dataSet # 将数据集中每个点都与待分类点相减，即各个特征相减squareDiffMat = diffMat**2 # 求差的平方squareDistance = squareDiffMat.sum(axis=1) # 求差的平方的和distances = squareDistance**0.5 # 对平方和开方得到距离# 对距离进行排序，argsort()函数默认按升序排列，但只返回下标，不对原数组排序sortedDistIndicies = distances.argsort()classCount = &#123;&#125; # 用于保存各个类别出现的次数for i in range(k): # 统计最近的 k 个点的类别出现的次数label = labels[sortedDistIndicies[i]]classCount[label] = classCount.get(label, 0) + 1# 对类别出现的次数进行排序，sorted()函数默认升序sortedClassCount = sorted(classCount.iteritems(), key=itemgetter(1), reverse=True)return sortedClassCount[0][0] # 返回类别出现次数最多的分类名称 1、shape()函数：返回数组的尺寸信息12&gt;&gt;&gt; x = tile((1,2),(3,2))&gt;&gt;&gt; x.shape[0] 2、tile()函数1&gt;&gt;&gt; tile([1,2],(3,2)) 3、sum()函数12345678&gt;&gt;&gt; x = tile((1,2),(3,2))&gt;&gt;&gt; xarray([[1, 2, 1, 2], [1, 2, 1, 2], [1, 2, 1, 2]])&gt;&gt;&gt; x.sum(axis=0)array([3, 6, 3, 6])&gt;&gt;&gt; x.sum(axis=1) 4、argsort()函数，返回排序后的原来位置的索引12&gt;&gt;&gt; v = [1, 4, 2, 3]&gt;&gt;&gt; argsort(v) 5、sorted()函数，按参数 key 排序1234567891011&gt;&gt;&gt; d = &#123;&apos;a&apos;:2,&apos;b&apos;:1,&apos;c&apos;:6,&apos;d&apos;:-2&#125;&gt;&gt;&gt; d&#123;&apos;a&apos;: 2, &apos;c&apos;: 6, &apos;b&apos;: 1, &apos;d&apos;: -2&#125;&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; sorted(d.iteritems(),key=itemgetter(1),reverse=True)``` ## 4.3测试分类器# 5.knn改进配对算法## 5.1从文本中解析数据 def file2matrix(filename): f = open(filename) # 打开文件 dataSet = f.readlines() # 读取文件的全部内容 numberOfLines = len(dataSet) # 获得数据集的行数 returnMat = zeros((numberOfLines, 3)) # 创建一个初始值为0，大小为 numberOfLines x 3 的数组 classLabelVector = [] # 用于保存没个数据的类别标签 index = 0 for line in dataSet: # 处理每一行数据 line = line.strip() # 去掉行首尾的空白字符,(包括’\n’, ‘\r’, ‘\t’, ‘ ‘) listFromLine = line.split() # 分割每行数据，保存到一个列表中 returnMat[index, :] = listFromLine[0:3] # 将列表中的特征保存到reurnMat中 classLabelVector.append(int(listFromLine[-1])) # 保存分类标签 index += 1 return returnMat, classLabelVector123```&gt;&gt;&gt; reload(kNN) &gt;&gt;&gt; datingDataMat, datingLabels = kNN.file2matrix(&apos;datingTestSet2.txt&apos;) 5.2分析数据使用 Matplotlib 创建散点图123456&gt;&gt;&gt; import matplotlib&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt; fig = plt.figure()&gt;&gt;&gt; ax = fig.add_subplot(111)&gt;&gt;&gt; ax.scatter(datingDataMat[:,1], datingDataMat[:,2])&gt;&gt;&gt; plt.show() Matplotlib 库提供的 scatter 函数支持个性化标记散点图上的点。12345&gt;&gt;&gt; from numpy import *&gt;&gt;&gt; fig = plt.figure()&gt;&gt;&gt; ax = fig.add_subplot(111)&gt;&gt;&gt; ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*array(datingLabels),15.0*array(datingLabels))&gt;&gt;&gt; plt.show() 将每年获得的飞行常客里程数作为 x 轴，玩视频游戏所耗时间百分比作为 y 轴1234&gt;&gt;&gt; fig = plt.figure()&gt;&gt;&gt; ax = fig.add_subplot(111)&gt;&gt;&gt; ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*array(datingLabels),15.0*array(datingLabels))&gt;&gt;&gt; plt.show() 另外，为了方便画图，不用每次都输入上面几行代码，我们可以写一个函数 showPlots 来完成同样的功能(将代码添加到 kNN.py 中)： 12345678from numpy import *import matplotlib.pyplot as pltdef showPlots(x, y, labels): # x:x轴数据，y:轴数据，labels:分类标签数据fig = plt.figure()ax = fig.add_subplot(111)ax.scatter(x ,y, 15.0*array(labels), 15*array(labels))plt.show() 重新载入 kNN.py ，输入如下代码即可画图：12&gt;&gt;&gt; reload(kNN)&gt;&gt;&gt; kNN.showPlots(datingDataMat[:,0], datingDataMat[:,1], datingLabels) 5.3归一化处理这种不同取值范围的特征值时，我们需要对数据进行归一化处理(权重一样，但是数值范围不同） 12345678910def autoNorm(dataSet):minVals = dataSet.min(0) # minVals保存每列最小值maxVals = dataSet.max(0) # maxVals保存每列最大值ranges = maxVals - minVals # ranges保存每列的取值范围normedDataSet = zeros(shape(dataSet))numberOfLines = dataSet.shape[0]normedDataSet = dataSet - tile(minVals, (numberOfLines, 1))normedDataSet = normedDataSet / tile(ranges, (numberOfLines, 1))return normedDataSet, ranges, minVals 1234&gt;&gt;&gt; reload(kNN)&gt;&gt;&gt; normMat, ranges, minVals = kNN.autoNorm(datingDataMat)&gt;&gt;&gt; normMat&gt;&gt;&gt; 5.4测试分类器的错误率=错误分类次数/分类测试总次数 123456789101112131415def datingClassTest():testRatio = 0.10 # 测试比例datingDataMat, datingLabels = file2matrix(&apos;datingTestSet2.txt&apos;) # 获得原始数据normedMat, ranges, minVals = autoNorm(datingDataMat) # 归一化m = normedMat.shape[0] # 原始数据行数numTestVecs = int(m*testRatio) # 测试数据行数errorCount = 0 # 错误分类计数器for i in range(numTestVecs): # 测试classifierResult = classify0(normedMat[i,:], normedMat[numTestVecs:m,:], datingLabels[numTestVecs:m], 4)print &quot;The classifier came back with: %d, the real answer is: %d&quot;\% (classifierResult, datingLabels[i])if(classifierResult != datingLabels[i]):errorCount += 1print &quot;The total error rate is: %f&quot; % (errorCount/float(numTestVecs)) 12&gt;&gt;&gt; reload(kNN)&gt;&gt;&gt; kNN.datingClassTest() 5.5完整系统12345678910def classifyPerson():resultList = [&apos;not at all&apos;, &apos;in small doses&apos;, &apos;in large doses&apos;]percentTime = float(raw_input(&quot;percentage of time spent playing video games: &quot;))ffMiles = float(raw_input(&quot;frequent flier miles earned per year: &quot;))iceCream = float(raw_input(&quot;liters of ice cream consumed per year: &quot;))datingDataMat, datingLabels = file2matrix(&apos;datingTestSet2.txt&apos;)normedMat, ranges, minVals = autoNorm(datingDataMat)inVec = array([ffMiles, percentTime, iceCream])classifierResult = classify0((inVec-minVals)/ranges, normedMat, datingLabels, 4)print &quot;You will probably like this person&quot;, resultList[classifierResult-1] 12&gt;&gt;&gt; reload(kNN)&gt;&gt;&gt; kNN.classifyPerson() 6.手写识别系统书本配备了两个数据集，一个是存储在文件夹 trainingDigits 内的训练集，大约2000个样本；另一个是存储在 testDigits 文件夹内的测试集，大约900个样本。如下图所示。两组数据没有重叠，尺寸均为 32行 x 32列。文件夹 trainingDigits 和文件夹 testDigits 的内容都是如下形式的样列： 将 32x32 的文本处理为一个尺寸为 1x1024 向量12345678def img2vector(filename):returnVec = zeros((1,1024)) # 用于保存1x1024的向量f = open(filename) # 打开文件for i in range(32): # 读取每一行并转换为1x1024的向量lineStr = f.readline()for j in range(32): # 处理第i行j列的一个字符returnVec[0,32*i+j] = int(lineStr[j]) # 字符需要强制类型转换成整数return returnVec 检查12345&gt;&gt;&gt; reload(kNN)&gt;&gt;&gt; testVec = kNN.img2vector(&apos;testDigits/0_13.txt&apos;)&gt;&gt;&gt; testVecarray([[ 0., 0., 0., ..., 0., 0., 0.]])&gt;&gt;&gt; testVec[0,0:32] 6.2 测试算法1234567891011121314151617181920212223242526from os import listdirdef handwritingClassTest():print &quot;Loading data...&quot;hwLabels = [] # 保存手写数字的分类标签trainingFileList = listdir(&apos;trainingDigits&apos;) # 得到文件夹trainingDigits下的所有文件名m = len(trainingFileList) # 训练集样本的个数trainingMat = zeros((m, 1024)) # 保存训练集for i in range(m):filenameStr = trainingFileList[i] # 得到文件名classNum = int(filenameStr.split(&apos;_&apos;)[0]) # 得到样本的分类标签hwLabels.append(classNum)trainingMat[i,:] = img2vector(&apos;trainingDigits/%s&apos; % filenameStr)testFileList = listdir(&apos;testDigits&apos;) # 得到文件夹testDigits下的所有文件名errorCount = 0 # 错误分类计数器mTest = len(testFileList) # 测试集样本个数for i in range(mTest): # 开始测试filenameStr = testFileList[i]classNum = int(filenameStr.split(&apos;_&apos;)[0])testVect = img2vector(&apos;trainingDigits/%s&apos; % filenameStr)classifierResult = classify0(testVect, trainingMat, hwLabels, 3)print &quot;The classifier came back with: %d, the real answer is: %d&quot;\% (classifierResult, classNum)if(classifierResult != classNum):errorCount += 1print &quot;The total number of errors is: %d&quot; % errorCountprint &quot;The total error rate is: %f&quot; % (errorCount/float(mTest)) 输入下面的命令对分类器进行测试（k=3）12&gt;&gt;&gt; reload(kNN)&gt;&gt;&gt; kNN.handwritingClassTest() 如果选择最近的一个作为分类标签，那么准确率会非常的高：123&gt;&gt;&gt; reload(kNN)&lt;module &apos;kNN&apos; from &apos;kNN.pyc&apos;&gt;&gt;&gt;&gt; kNN.handwritingClassTest() 总结1、kNN缺陷是无法给出任何数据的基础结构信息。而决策树能够解决这个问题，并且速度很快。2、另一个缺点就是既占空间速度又慢]]></content>
  </entry>
  <entry>
    <title><![CDATA[unique_summer_lab_0的书签]]></title>
    <url>%2F2017%2F07%2F05%2Funique-summer-lab-0%E7%9A%84%E4%B9%A6%E7%AD%BE%2F</url>
    <content type="text"><![CDATA[完成第一个lab中的一些网页书签 优先队列的申明方式和成员函数 github上面优先队列的实现1 github上面优先队列的实现2 左倾红黑树java的pdf 左倾红黑树的官方文档 左倾红黑树的github实现 红黑树实现set 介绍set容器 红黑树实现map和set]]></content>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2017%2F07%2F02%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[看了吴恩达课之后总结（1）Logistic Regression的基本原理（2）Logistic Regression的具体过程，即是先选取预测函数，求解Cost函数和J(θ)，梯度下降法求J(θ)的最小值，以及递归下降过程的向量化（vectorization）（3）对比《机器学习实战》中的实现代码 ,回答阅读LogisticRegression部分遇到的疑惑(question:一般都是用梯度下降法求损失函数的最小值，为何这里用梯度上升法呢？书中说用梯度上升发，为何代码实现时没求梯度的代码？) Stanford机器学习公开课（https://www.coursera.org/course/ml）(图片公式来源于csdn) 基本原理Logistic Regression和Linear Regression的原理是相似的，描述为这样的过程：（1）找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。（2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。（3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。 具体过程构造预测函数Logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。根据刚刚说的的步骤，需要先找到一个预测函数（h），显然，该函数的输出必须是两个值（分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为： 对应的函数图像是一个取值在0和1之间的S型曲线 接下来需要确定数据划分的边界类型，对于图2和图3中的两种数据分布，显然图2需要一个线性的边界，而图3需要一个非线性的边界。接下来我们只讨论线性边界的情况 下面是slide 对于线性边界的情况，边界形式如下 构造预测函数为： hθ(x)函数的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为： 构造Cost函数Andrew Ng在课程中直接给出了Cost函数及J(θ)函数如式（5）和（6），但是并没有给出具体的解释，只是说明了用这个函数来衡量h函数预测的好坏是合理的。 实际上这里的Cost函数和J(θ)函数是基于最大似然估计推导得到的。下面详细说明推导的过程。（4）式综合起来可以写成： 取似然函数为： 对数似然函数为： 最大似然估计就是要求得使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为（6）式，即： 因为乘了一个负的系数-1/m，所以J(θ)取最小值时的θ为要求的最佳参数。 梯度下降法求J(θ)的最小值求J(θ)的最小值可以使用梯度下降法，根据梯度下降法可得θ的更新过程： 式中为α学习步长，下面来求偏导： 上式求解过程中用到如下的公式： 因此，（11）式的更新过程可以写成： 因为式中α本来为一常量，所以1/m一般将省略，所以最终的θ更新过程为： 另外，补充一下，3.2节中提到求得l(θ)取最大值时的θ也是一样的，用梯度上升法求（9）式的最大值，可得： 观察上式发现跟（14）是一样的，所以，采用梯度上升发和梯度下降法是完全一样的，这也是《机器学习实战》中采用梯度上升法的原因。 3.4 梯度下降过程向量化关于θ更新过程的vectorization，Andrew Ng的课程中只是一带而过，没有具体的讲解。《机器学习实战》连Cost函数及求梯度等都没有说明，所以更不可能说明vectorization了。但是，其中给出的实现代码确是实现了vectorization的，图4所示代码的32行中weights（也就是θ）的更新只用了一行代码，直接通过矩阵或者向量计算更新，没有用for循环，说明确实实现了vectorization，具体代码下一章分析。文献[3]中也提到了vectorization，但是也是比较粗略，很简单的给出vectorization的结果为： 且不论该更新公式正确与否，这里的Σ(…)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization，不像《机器学习实战》的代码中一条语句就可以完成θ的更新。下面说明一下我理解《机器学习实战》中代码实现的vectorization过程。约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值： 约定待求的参数θ的矩阵形式为： 先求x.θ并记为A： 求hθ(x)-y并记为E： g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知hθ(x)-y可以由g(A)-y一次计算求得。再来看一下（15）式的θ更新过程，当j=0时： 同样的可以写出θj， 综合起来就是： 综上所述，vectorization后θ更新的步骤如下：（1）求A=x.θ；（2）求E=g(A)-y；（3）求θ:=θ-α.x’.E,x’表示矩阵x的转置。也可以综合起来写成：http://img.blog.csdn.net/20131213085438093前面已经提到过：1/m是可以省略的。 代码分析图4中是《机器学习实战》中给出的部分实现代码。 sigmoid函数就是前文中的g(z)函数，参数inX可以是向量，因为程序中使用了Python的numpy。gradAscent函数是梯度上升的实现函数，参数dataMatin和classLabels为训练数据，23和24行对训练数据做了处理，转换成numpy的矩阵类型，同时将横向量的classlabels转换成列向量labelMat，此时的dataMatrix和labelMat就是（18）式中的x和y。alpha为学习步长，maxCycles为迭代次数。weights为n维（等于x的列数）列向量，就是（19）式中的θ。29行的for循环将更新θ的过程迭代maxCycles次，每循环一次更新一次。对比3.4节最后总结的向量化的θ更新步骤，30行相当于求了A=x.θ和g(A)，31行相当于求了E=g(A)-y，32行相当于求θ:=θ-α.x’.E。所以这三行代码实际上与向量化的θ更新步骤是完全一致的。总结一下，从上面代码分析可以看出，虽然只有十多行的代码，但是里面却隐含了太多的细节. 这里有一篇另外的实现方法http://blog.csdn.net/zouxy09/article/details/20319673]]></content>
  </entry>
  <entry>
    <title><![CDATA[聊天系统]]></title>
    <url>%2F2017%2F07%2F02%2F%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[自动聊天system的解决方案和技术挑战 emmmm 图灵测试：人机对话无法判断对方，即持续聊天，任何query给出合适replies，考虑到用户个体差异 相关的概念：问答系统（给一个明确的答案factoid，或者why、how等等）、对话系统（通过对话完成一个事情，没有闲聊，科研圈子小）、自动客服FAQ 技术基因why和how形的问答、community qa、corpus mining、faq mining 共同点：有大量的qa数据，配以准确的匹配&amp;排序算法，那么问答系统是可能的 重要结论：文本的相似性和相关性（更重要）是两个不同需求 短文本相关性很难计算（常识上的同义词） 词语贡献\机器翻译（问题和恢复是一种另类的翻译对）|LDA主题建模\meta-data 两条主要路线一条好走大家都能走，一条看起来很难走但是沿途风景美 1、基于ir的 建立一个索引，query问答库的匹配，用文本相似度找，把候选答案再做一个相关性计算，排序得到最相关的，框架上就相当于检索系统 细化的三层的逻辑架构 首先计算query和question关系，找到和query最相似的问题后再通过问题找到答案；再做一个排序，考虑多轮语意相关性和主题相关性；第三层考虑非文本相关的需求，比如考虑用户差异，再排序 基于生成模板中文句子可以直接输出英文，而且机器翻译和聊天有潜在关联，则同样sequence to sequence，用深度学习做encoder到decoder 用基于ir的方法可以使句子可读性比较好，query保证回复的多样性，debug很容易。挑战是要rank，事情比较复杂。 基于生成model式的，构建模型很容易，直接训练，不需要维护问答库，可读性很差，回复总会是说“我也觉得是这样” 难度：上下文、外部知识、个性元素、评价 深度学习从浅入深列举，对抗学习23333 短文本的语意相关性计算 词向量很重要，算夹角表示距离 但是按维数相加，会产生损失，所以应该是产生句子向量，上面就是两种2014年的paper，一种是做卷积，后一种是词语之间的关系在二维上排开，细化计算语义关系产生矩阵再做卷积。即是词语向量输入，句子向量输出。 如上图，上下文引入，要有对话历史。通过序列化的gru模型，两个层面的计算，一个是两个词语之间形成embedding，另一方面gru形成句子级别的向量矩阵，形成句子之间的矩阵。两个平行的矩阵，分别做卷积。2016年 如上图，看句子是否相关。在cnn上面做attention机制，query在语义背景下，使用了两种方法，一种是得到模糊的语义背景，一种是？？？？然后用这两种关系排除result 如上图，GOOgle的方法，把作者的东西引进来，算，然后分解。上下文情况下人工排序 如上图，外部知识如何引入 生成式attention的方法引入 上图，强化学习的引入，关键是怎么给model以奖惩。三个维度，一生成的model能引出更多样性的回复，推动聊天进展，强相关于topic 上图，id和词语向量放在一起，每个词生成都考虑人是谁 上图，对抗学习，引入文本生成。假如一句话不能判断是谁说的，就说明生成器很好。图形的生成是可导的，但是词语的选择这一步就是不可导的，那么关联就没有了？用强化学习绕过不可导。或者说语意向量是可导的，这样来关联两个生成器。 评价 对话轮数是有区别的，可以赋权重 挑战训练集不统一（reddit更好） 情感交流，这是提高用户体验到关键 全nlp：问答、机器翻译、自动文摘、语音的自动合成与识别、ir、文本分类 信息检索：相关性、扩展以后的相关性、个性化检索 特征取得要强 emmmmm]]></content>
  </entry>
  <entry>
    <title><![CDATA[talk with 李云涛]]></title>
    <url>%2F2017%2F06%2F27%2Ftalk-with-%E6%9D%8E%E4%BA%91%E6%B6%9B%2F</url>
    <content type="text"><![CDATA[和李云涛的晚上trip他是本科两年修完四年的课，要上那种给分好事情少的老师，去听好老师的课。 微博关注：爱可可爱生活 各种评测比赛不好的地方：就是简单的把各种模型拼凑、boosting找最好的，没有创新 实习一定要先了解，他的三段实习：第一个在照片社交APP（nice）做数据岗（接触到了很多大型的数据，对数据量有了非常深入的理解）七个月，第二个在电影推荐做算法（感觉浪费了时间）五个月，第三个摩拜旗下的单车做数据。 做大公司的逻辑业务开发不好（如果不直接找工作），不会有收获。做大公司的算法方面，需要具体而言，大多数都是专精，往下挖，不太好。但是也有大公司算法岗是那种apply model to some certain aspect，那就可以。保险还是推荐去start up实习，既可以从0开始自己去摸索很多东西,也可以接触面就很广。 他毕业论文是做新闻和微博的文本摘要，就是先提取topic，压缩关键句子，然后再把关键词变成一个逻辑上的句子。现在是做信息检索，信息检索涉及面很广，nlp啥的都会涉及到，也好找工作一些。他的张岩老师就是从数据库转到信息检索。 关于ai，因为谁都不知道原理，所以会用tf就行，别深究。外界火，但是学术界就那样，好用的时候就用。 关于组会，一开一整天，从十点到八点。首先是十点到一点，每周一个人讲论文和学习收获分享。这个人先要给老师审核十篇论文，精读五篇，然后做ppt讲三个小时。三点开始每个人半小时汇报一周情况和下周安排分配。老师给的东西不强求你做完。 关于读论文，其实一般是是读reference提到的文章和google关键词搜索找的文章。然后要做到看标题就知道大概讲的是什么用的是什么model。 kaggle上的评测比赛。 关于数据的准确性的评估，应该是门学问。一看数据量，二实验人的背景来判断数据的可信度，另外还有些不记得了 关于写论文，short反而好中一些，他说他之前被拒的原因是model太复杂评审不能理解。要找准论文切入点，他的标题是老板定的，这个会议本来是不在中国的，而且没有deadline，有点水。会议不查重，有很多句子是抄的。一些固定的单词用法。model也可以由别人的现成的改。]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来教育]]></title>
    <url>%2F2017%2F06%2F25%2F%E6%9C%AA%E6%9D%A5%E6%95%99%E8%82%B2%2F</url>
    <content type="text"><![CDATA[emmmmm….“今天的教育和老师不生活在未来，未来的学生将活在过去。”对于有些教师来说，“互联网+教育”是“天堂”，技术为他们创造出新的教育模式带了无限可能；可对于有些教师来说，“互联网+教育”是“荒漠”，微课、慕课等较为成熟的教学方式也只是听说。身处技术革新越来越快，且越来越深刻影响人们学习、工作和生活的时代，教师的工作形态会发生什么样的变化？余胜泉教授认为：互联网、大数据时代要求教师从面向群体的知识传授型教育，转变为精确了解学生个体情况后设计个性化学习方案。教师不再是知识的传授者，而是学生个性化发展的引路人。赵勇教授非常赞同余胜泉教授的观点，他认为未来的教师不再是剪裁学生个性的“园丁”，而是促进学生各具特色的发展，像“自然保护区”的负责人，教师要注重每个孩子的个性化成长，帮助每个孩子发现他们的天赋与激情，并把天赋、激情与努力转变成对别人有帮助、有价值的事业。同时赵勇教授还认为，教师群体自身必须要多元化，教师要回归到人，而不再是传递知识的机器，教师的工作不再局限于教学的本身，而是从多角度助力学生的发展。在“互联网+”和人工智能的时代背景下，余胜泉教授认为人机结合是未来教师工作的新形态，教师应该利用外部的工具或者智能来发展自身智慧，正如目前未来教育高精尖创新中心在北京市通州区利用大数据支持区域性教学改革的试点中，我们力争实现全学习过程数据的采集，知识与能力结构的建模，学科优势的发现与增强，学习问题的诊断与改进。在减轻教师繁重且重复的工作负担的同时提高教育质量和教育公平。比如说用自动批改作业取代部分人工批改作业，通过收集分析个人和群体的大数据实现精准的课堂教学和教育管理。未来教育高精尖创新中心的人工智能教师“小艾”（在研项目）向两位教授提出犀利的问题：人工智能教师会取代人类教师吗？赵勇教授认为：在短期内人工智能肯定取代不了人类，人类的任务是发现问题，而机器是解决问题，机器可以预测普遍性但是无法预测特殊性。余胜泉教授同样认为，科技不会取代老师。但是会用科技的老师可能会取代不会用科技的老师；人工智能不会取代老师，但是善于利用人工智能的老师，可能会取代对此一无所知的老师。未来教师将基于大数据为学生提供个性化的教学服务，在线上、线下数据的引导下，在学生的学习过程中呈现陪伴、监督、管理、督促相结合的工作形态。]]></content>
  </entry>
  <entry>
    <title><![CDATA[读大话设计模式]]></title>
    <url>%2F2017%2F06%2F22%2F%E8%AF%BB%E5%A4%A7%E8%AF%9D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[大话设计模式类似于活字印刷：可维护，可复用，可拓展—-oop让业务逻辑和界面逻辑分开 oop三大特性：继承、封装、多态 封装每个对象包含他能进行操作所需要的所有信息，通过类的实例来实现 继承“is-a”关系 1、子类有父类非private的属性和功能 2、子类有自己的属性和功能（可拓展） 3、子类可以以自己的方式实现父类的功能（方法重写） 多态不同的对象可以执行相同的动作，但是要通过他们自己的实现代码来执行 1、子类以父类身份 2、子类在工作中以自己的方式实现 3、子类特有的属性和方法不能使用 virtual方法：在该成员的返回类型之前加上virtual，来使子类的实例完全接替来自父类的类成员。通常虚拟的是方法。 子类使用override：将父类实现替换为他自己的实现，即方法重写 对象的申明是父类，实例化的对象是子类 多态的原理：当方法被调用的时候，无论对象是否被转化为其父类，都只有位于对象继承链最末端的方法会被调用。也就是说，虚方法是按照其运行类型而非编译时类型进行动态调用的。 类和类之间的关系（uml类图）继承、依赖、聚合、合成、关联、实现接口 抽象策略模式和简单工厂不是类越多越好，累的划分是为了封装，但分类的基础是抽象，具有相同属性和功能的对象的抽象集合才是类 简单工厂模式：case0，case1，case2解决对象的创建问题 抽象策略模式：定义了算法家族，分别封装起来，让他们之间可以相互替换，此模式让算法的变化，不会影响到算法的客户（封装变化点） 结合：简单工厂不一定是一个单独的类，可以和策略模式的context结合 （参数是一个字符串） 选择所用的具体实现的职责由客户端对象承担，并转给策略模式的context对象 通常字段是private，首字母小写or“_”；通常属性是public，首字母大写。 get和set就像封闭的房子的纱窗]]></content>
  </entry>
  <entry>
    <title><![CDATA[联创面试笔记]]></title>
    <url>%2F2017%2F06%2F20%2F%E8%81%94%E5%88%9B%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[吸取经验 堆跟栈的区别栈（操作系统）：由操作系统自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。 堆（操作系统）： 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收，分配方式倒是类似于链表。 如何反转单链条 安卓四大组件Android四大基本组件分别是Activity，Service服务,Content Provider内容提供者，BroadcastReceiver广播接收器。 生命周期？ python修饰器在面向对象（OOP）的设计模式中，decorator被称为装饰模式。OOP的装饰模式需要通过继承和组合来实现，而Python除了能支持OOP的decorator外，直接从语法层次支持decorator。Python的decorator可以用函数实现，也可以用类实现。 decorator可以增强函数的功能，定义起来虽然有点复杂，但使用起来非常灵活和方便。 动态规划的用法 1背包问题 2一共有n个管道，每个管道的价格和流量不同，一个设备由不同的管道构成，每种管道有多种选择，他们的价格或流量不同，结果应选出价格之和最小与最小管道流量最大的组合。 作者：知乎用户链接：https://www.zhihu.com/question/23995189/answer/35392247来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。首先，动态规划是一种算法。那么，何谓算法？计算机书籍中不难找到其严谨的学术定义，大众可以简单理解为“解决某一类问题的核心思想”。先谈动态规划的意义——望文生义，“动态”规划对应“动态”的问题：你并不知道问题的规模会有多大，而不论是个位数还是百万级，都能以较快速度(动态规划是一种泛用性算法，而泛用性算法与特定算法相比往往存在性能差距)将结果正确计算出来。这是对于计算机科学最直观的意义，当然我认为其对人生亦有一定指导意义，但那是见仁见智的事了。动态规划这一思想的实质其实是以下两点：1.分析问题，构造状态转移方程2.以空间换时间让我们结合一个简单例子来理解一下：以乘法计算为例，乘法的定义其实是做n次加法，请先忘掉九九乘法表，让你计算99，如何得到81这个解？计算910呢？9999……以及9n呢？1.分析问题，构造状态转移方程“状态转移方程”的学术定义亦可简单找到（比如置顶答案），略去不表。光看“方程”二字，可以明白它是一个式子。针对以上问题，我们构造它的状态转移方程。问题规模小的时候，我们可以容易得到以下式子：90=0；91=0+9；92=0+9+9；……可以得到：9n=0+9+…+9(总共加了n个9)。严谨的证明可以使用数学归纳法，略去不表。现在，定义dp(n)=9n,改写以上式子：dp(0)=90=0；dp(1)=91=dp(0)+9；dp(2)=92=dp(1)+9；……作差易得：dp(n)=dp(n-1)+9；这就是状态转移方程了。可以看到，有了状态转移方程，我们现在可以顺利求解9n（n为任意正整数）这一问题。2.以空间换时间虽然能解，但当n很大时，计算耗时过大，看不出状态转移方程dp(n)=dp(n-1)+9与普通方程9n=0+9+…+9(总共加了n个9)相比没有任何优势。这时，如果dp(n-1)的结果已知，dp(n)=dp(n-1)+9只需计算一次加法，而9*n=0+9+…+9(总共加了n个9)则需计算n-1次加法，效率差异一望即知。存储计算结果，可令状态转移方程加速，而对普通方程没有意义。以空间换时间，是令动态规划具有实用价值的必备举措。 条件随机场条件随机场(Conditional random fields)，是一种判别式图模型，因为其强大的表达能力和出色的性能，得到了广泛的应用。从最通用角度来看，CRF本质上是给定了观察值集合(observations)的马尔可夫随机场。在这里，我们直接从最通用的角度来认识和理解CRF，最后可以看到，线性CRF和所谓的高阶CRF，都是某种特定结构的CRF。 TensorFlow的Tensor是啥TensorFlow是Google在2015年11月份开源的人工智能系统（Github项目地址），是之前所开发的深度学习基础架构DistBelief的改进版本，该系统可以被用于语音识别、图片识别等多个领域。 官网上对TensorFlow的介绍是，一个使用数据流图(data flow graphs)技术来进行数值计算的开源软件库。数据流图中的节点，代表数值运算；节点节点之间的边，代表多维数据(tensors)之间的某种联系。你可以在多种设备（含有CPU或GPU）上通过简单的API调用来使用该系统的功能。TensorFlow是由Google Brain团队的研发人员负责的项目。 什么是数据流图(Data Flow Graph) 数据流图是描述有向图中的数值计算过程。有向图中的节点通常代表数学运算，但也可以表示数据的输入、输出和读写等操作；有向图中的边表示节点之间的某种联系，它负责传输多维数据(Tensors)。图中这些tensors的flow也就是TensorFlow的命名来源。 节点可以被分配到多个计算设备上，可以异步和并行地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。 卷积神经网络中可调设的参数每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或Softmax），并且在神经网络中我们实现的各种技巧和要点依旧适用于卷积神经网络。 面向过程和面向对象的区别 面向过程就是分析出解决问题所需的步骤，面向对象则是把构成问题的事物分解成对象，抽象出对象的目的并不在于完成某个步骤，而是描述其再整个解决问题的步骤中的行为。 面向过程的思维方式是分析综合，面向对象的思维方式是构造。 例如C语言解决问题时，一般是先定义数据结构，然后在构造算法。而是用Java面向对象求解时则是先抽象出对象，构造一个“封闭”的环境，这个环境中有定义的数据和解决问题的算法。 面向过程的设计更具挑战性，技巧性，面向对象主要在于对象抽象的技术性，一旦完成抽象，任何人都可以做后面的工作了。 从代码层结构上来说的话，面向对象和面向过程的主要区别就是数据是单独存数还是与操作存储 在一起。面向对象提供了数据的封装后，是的对某一操作而言，数据的访问变得可靠了。 面向过程就是将coding当做一件事，一步一步完成，面向对象就是将coding当做一件事物，需要 做什么的时候由事物(对象)本身的行为去完成。 总的来说： 面向对象是将事物高度抽象化。面向过程是一种自顶向下的编程。面向对象必须先建立抽象模型，之后直接使用模型就行了。 进程和线程线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 进程是指在系统中正在运行的一个应用程序；线程是系统分配处理器时间资源的基本单元，或者说进程之内独立执行的一个单元。 对于操作系统而言其调度单元是线程。一个进程至少包括一个线程，通常将该线程称为主线程。一个进程从主线程的执行开始进而创建一个或多个附加线程，就是所谓基于多线程的多任务。 进程与线程的重要区别在于线程不能够单独执行，它必须运行在处于活动状态的应用程序进程中。 每个进程至少需要一个线程。 进程由两部分构成：进程内核对象，地址空间。线程也由两部分组成：线程内核对象，操作系统用它来对线程实施管理。线程堆栈，用于维护线程在执行代码时需要的所有函数参数和局部变量。 进程是不活泼的。进程从来不执行任何东西，它只是线程的容器。线程总是在某个进程环境中创建的，而且它的整个寿命期都在该进程中。如果在单进程环境中，有多个线程正在运行，那么这些线程将共享单个地址空间。这些线程能够执行相同的代码，对相同的数据进行操作。这些线程还能共享内核对象句柄，因为句柄表依赖于每个进程而不是每个线程存在。 进程使用的系统资源比线程多得多。实际上，线程只有一个内核对象和一个堆栈，保留的记录很少，因此需要很少的内存。因此始终都应该设法用增加线程来解决编程问题，避免创建新的进程。但是许多程序设计用多个进程来实现会更好些。 线程是一种操作系统对象，代表着一个进程中要被执行的代码的路径。每一个WIN32应用程序至少有一个线程–通常称为住线程或默认线程–但应用程序可以自由地创建其他线程来执行其他任务！进程是程序的一次动态执行过程，它对应了从代码加载、执行到执行完毕的一个完整过程，这个过程也是进程本身从产生、发展到消亡的过程线程是比进程更小的执行单位。一个进程在其执行过程能够中，可以产生 多个线程，形成多条执行线索。每条线索，即每个线程也有它自身的产生、存在和消亡过程，也是一个动态的概念。一个程序应该只有一个进程吧，但是可以拥有多个线程。可以说，一个执文件被运行后，就可以称为是一个进程了。但是进程只是存在内存中，实际上他是不会做任何事情的。这个时候，起作用的就是线程了。线程是程序的执行者，一个程序至少有一个线程，但是在多线程的操作系统中，可以有一个以上的线程。 其实我们可以把线程看成是我们排队买肯德鸡吃（循环的排队，一直排下去，知道我不想买了，退出）。每人都有机会到达队伍的最前端去买东西，这个就好比是线程，都有机会被程序执行。但是线程真正起作用的时候，就是我们在队伍的最前端买东西到东西买完后，这一段时间，这是线程真正执行的阶段。]]></content>
  </entry>
  <entry>
    <title><![CDATA[ccnu-cs自招机经]]></title>
    <url>%2F2017%2F06%2F10%2Fccnu-cs%E8%87%AA%E6%8B%9B%E6%9C%BA%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[题目解析和思路 题目集萃 搜索引擎怎么实现 怎么设计一个图书馆智能导引系统 你是钢铁侠，设计钢铁战衣 阿法狗 运动手环的发展前景 你怎么设计一个ofo类似的共享单车 怎么设计一个滴滴打车类似的APP 思考方向华师计算机院是分为三个专业，计算机科学技术，软件工程，物联网。所以大体上可以把题目分为这三类专业解决的事情。 计算机科学技术计算机科学技术就是以算法为主，包括现在火的神经网络、深度学习，聚类、隐式马科夫链、各类算法；可以做算法工程师。这些东西很广，要是感兴趣可以自己查一下。你可以就说“对算法很感兴趣，对acm竞赛很感兴趣，对machine learning（机器学习）很感兴趣”。 软件工程软件工程就是做基于一些平台的编程开发，比如android的app，Windows软件或者是web应用程序，对应的编程语言分别是Java、C#（读作c sharp）、web的前端JavaScript和后端python。 应该不涉及到专业的技术方面，所以在设计APP时要从多方面入手：比如说 1、需求分析（注意归根到底：用户使用者）（关键在于如何去挖掘用户需求？调查问卷？这就要你自己好好想想了） 2、团队建设（前端、后端、算法、产品经理、设计师、架构师各个职位齐全） 3、软件的稳定问题（专业名词：鲁棒性），如何实现数据安全 4、如何运营维护（讲人话：怎么宣传） 物联网这方面我认识不深，大体上就是偏硬件，单片机、嵌入式、RFID啥的，感兴趣的自己了解。 如果是机器人相关，可以带一句，“了解到华师机器人团队刚拿了全国第五，希望有幸能进实验室学习参加比赛为学院争光” 最重要的形式无领导小组讨论，所以最重要的就是，多发言多发言多发言。其次就是吐词清晰，我当时身边一个河南男生就是说话太紧张用方言根本听不懂，不过这里武汉的学弟学妹们就占了优势啦，所以别紧张 如何装逼地自我介绍在开始讨论之前每个人都有自我介绍，腹稿要打好，要有层次一二三，可以用手比划一二三，眼镜看着老师。高中生普遍的问题：紧张。真的没啥好紧张的 关于未来规划可以看我博客里的另一篇《计算机专业导论》 所以你要突出的是你的智力没问题，你的情商没问题，你对计算机很感兴趣，就ok了。 当然，可以说些老师爱听的话，比如“要大学准备每天去图书馆追求高绩点，做习主席说的有追求的青年，想去acm，赞同学院的以赛促学的方针，认为我们学校非常好桂子山很美，计算机院全校工科学院第一（😁）” 装逼扯什么呢？大数据、区块链、云计算、机器学习、深度学习神经网络cnn、nlp自然语言处理，这些算法相关无论它问啥题目都能涉及到 其他（人生规划）希望你早一点拥有自己的规划，不一定以后会实现，但是一步步走，你会发现你已经超过同龄人很多 如何思考呢？大学毕业无非三条路，就业找工作，国内读研（保研考研），国外读ms/phd（托g论文绩点） 那么你的一切所做的事情就要和你的规划相关了，人时间是有限的，青春更是有限的，什么狗屁学生会社团我建议你还是别浪费时间了，因为有更有趣的事情等着你做 如果想直接就业赚钱，很负责告诉你，你只有一条路，本科期间选定开发方向之后（前端还是后端还是安卓ios），就不断做项目工程开发，药不能停止，工程不能停，github欢迎你 如果想国内读研，唉小伙子你是学计算机的，还是国内算了吧，真的不值得 国外读研，ms和phd是有很大区别的，这个就要说很多了，下次有机会再说。但是四样东西是都要的，那么这四样就是你四年的所有追求，托福，gre，绩点，论文。 总而言之，无论怎样，不要做一些无聊的事情荒废时间，总之不要跟着学校老师走，跟着课程走的人只有死路一条。（当然了，不跟着老师走一样可以考年级前几） 其他（暑假读书建议），额你以后就知道学计算机会买很多很多很多很多很多很多书我大一买的书已经摞起来超过我一米八的身高了 暑假必买：《编码》、《Unix/Linux大学教程》、《C Primer Plus(第6版)(中文版)》或者《明解C语言》（如果觉得C语言不用买的请和老板沟通，我评估一下）《离散数学及其应用》（有勇气的可以买英文版，给你点赞） 必买：mac pro（记住：是为了编程必须买，至于为什么可以私戳我，然后觉得贵了我可以介绍人给你便宜2000） 选购：《黑客与画家》、《写给大家看的设计书》、《只是为了好玩》、《浪潮之巅》、《鸟哥的Linux私房菜》、《征服C指针》 这里首先看《Unix/Linux大学教程》和《编码》还有C语言，C语言的练习代码要推到Github上。《Unix/Linux大学教程》的可以先看第一、二、十一、十二、二十一、二十四、二十五这几章]]></content>
  </entry>
  <entry>
    <title><![CDATA[bop资格赛弃坑之路]]></title>
    <url>%2F2017%2F06%2F04%2Fbop%E8%B5%84%E6%A0%BC%E8%B5%9B%E5%BC%83%E5%9D%91%E4%B9%8B%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[bop资格赛弃坑之路由于这次资格赛涉及了过多的高级操作，就算会写bot也过不了资格赛，就比较无语，但是还是学到了些东西的 题目资格赛任务题是基于文档的问答任务（Document-based Question Answering task, DBQA），它是对于给定的一篇文档（Document）和一个从文档中提出的自然语言问题(Question)，参赛队伍需要使用提供的数据集训练模型算法，让模型可以回答问题，回答时仅限于从组成该文档的句子中选出能回答该问题的句子（Answer Selection in Question Answering）。鼓励参赛队伍发挥算法创造力并使用各种资源来训练模型，比如句子匹配模型（Sentence Matching Model），以使模型能准确地回答问题。 训练集和开发集 基于文档的问答系统DBQA给出了训练数据开发数据测试数据，训练数据给出了三元组，问题是同样的问题，七句话来自同一个篇章。第一列是答案标签，第六行是1：是答案，其他不是答案：是0。任务是：给你一个篇章再给一个问题，选出篇章中的一句来保证这是当前问题的答案。根据答案标签来训练问题答案句匹配模型，但是实际测试时只会给当前的篇章和问题，没有答案标签，要排序，排出最相关的一句话 提示：1、数数，重复的字词很多，就有问题答案关系 2、词向量。每个句子都可以转化成词向量，表示当前词的语言。然后看词向量上的距离 3、深度学习工具，做模型上的训练，使问题和正确答案相关性非常强，以实现答案抽取 一个句子匹配模型的神经网络的构造 胡老师用深度学习做文本领域的自动问答，分析输入问题基于文本的内容来给出答案 最土的做法：端对端的基于神经网络的自动问答模型，需要基于LSTM模型及其变体，比如有需要有web记忆模块的功能；再比如说用gru编码给定的文本信息作为知识，在编码给定信息的时候要用词向量表示文本，然后用gru表示给定的问题 然后用attention机制来表示问题和需要记忆的答案和情景之间的交互来生成答案，所以是一个需要用动态神经网络机制来实现的端对端的联合训练的神经网络系统。 要做的话建议用双向的LSTM模型来同时建立问题和文档的联合表示，然后通过一个分类器来预测答案。 TensorFlow谷歌就开源了其用来制作AlphaGo的深度学习系统Tensorflow 12345$ sudo apt-get install python-pip python-dev$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl($ pip install --upgrade pip) ！！！ 看看 attention机制真正火起来应该算由于是google mind团队的这篇论文《Recurrent Models of Visual Attention，他们在RNN模型上使用了attention机制来进行图像分类。随后，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》 [1]中，使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行，他们的工作算是是第一个提出attention机制应用到NLP领域中。接着类似的基于attention机制的RNN模型扩展开始应用到各种NLP任务中。最近，如何在CNN中使用attention机制也成为了大家的研究热点 《Recurrent Models of Visual Attention》他们研究的动机是受到人类注意力机制的启发。人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分. 该模型是在传统的RNN上加入了attention机制（即红圈圈出来的部分），通过attention去学习一幅图像要处理的部分，每次当前状态，都会根据前一个状态学习得到的要关注的位置l和当前输入的图像，去处理注意力部分像素，而不是图像的全部像素。这样的好处就是更少的像素需要处理，减少了任务的复杂度。可以看到图像中应用attention和人类的注意力机制是很类似的，接下来我们看看在NLP中使用的attention。 Attention-based RNN in NLP他们把attention机制用到了神经网络机器翻译（NMT）上，NMT其实就是一个典型的sequence to sequence模型，也就是一个encoder to decoder模型，传统的NMT使用两个RNN，一个RNN对源语言进行编码，将源语言编码到一个固定维度的中间向量，然后在使用一个RNN进行解码翻译到目标语言，传统的模型如下图： 这篇论文提出了基于attention机制的NMT，模型大致如下图：图中我并没有把解码器中的所有连线画玩，只画了前两个词，后面的词其实都一样。可以看到基于attention的NMT在传统的基础上，它把源语言端的每个词学到的表达（传统的只有最后一个词后学到的表达）和当前要预测翻译的词联系了起来，这样的联系就是通过他们设计的attention进行的，在模型训练好后，根据attention矩阵，我们就可以得到源语言和目标语言的对齐矩阵了。具体论文的attention设计部分如下：可以看到他们是使用一个感知机公式来将目标语言和源语言的每个词联系了起来，然后通过soft函数将其归一化得到一个概率分布，就是attention矩阵。从结果来看相比传统的NMT（RNNsearch是attention NMT，RNNenc是传统NMT）效果提升了不少，最大的特点还在于它可以可视化对齐，并且在长句的处理上更有优势。 Effective Approaches to Attention-based Neural Machine Translation他们的工作告诉了大家attention在RNN中可以如何进行扩展，这篇论文对后续各种基于attention的模型在NLP应用起到了很大的促进作用。在论文中他们提出了两种attention机制，一种是全局（global）机制，一种是局部（local）机制。 作者的实验结果是局部的比全局的attention效果好。 这篇论文最大的贡献我觉得是首先告诉了我们可以如何扩展attention的计算方式，还有就是局部的attention方法。 Attention-based CNN in NLP随后基于Attention的RNN模型开始在NLP中广泛应用，不仅仅是序列到序列模型，各种分类问题都可以使用这样的模型。那么在深度学习中与RNN同样流行的卷积神经网络CNN是否也可以使用attention机制呢？《ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs》这篇论文就提出了3中在CNN中使用attention的方法，是attention在CNN中较早的探索性工作。 传统的CNN在构建句对模型时如上图，通过每个单通道处理一个句子，然后学习句子表达，最后一起输入到分类器中。这样的模型在输入分类器前句对间是没有相互联系的，作者们就想通过设计attention机制将不同cnn通道的句对联系起来。 第一种方法ABCNN0-1是在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。具体的计算方法如下。第二种方法ABCNN-2是在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化，原理如下图。第三种就是把前两种方法一起用到CNN中，如下图这篇论文提供了我们在CNN中使用attention的思路。现在也有不少使用基于attention的CNN工作，并取得了不错的效果。 总结Attention在NLP中其实我觉得可以看成是一种自动加权，它可以把两个你想要联系起来的不同模块，通过加权的形式进行联系。目前主流的计算公式有以下几种： 通过设计一个函数将目标模块mt和源模块ms联系起来，然后通过一个soft函数将其归一化得到概率分布。目前Attention在NLP中已经有广泛的应用。它有一个很大的优点就是可以可视化attention矩阵来告诉大家神经网络在进行任务时关注了哪些部分。不过在NLP中的attention机制和人类的attention机制还是有所区别，它基本还是需要计算所有要处理的对象，并额外用一个矩阵去存储其权重，其实增加了开销。而不是像人类一样可以忽略不想关注的部分，只去处理关注的部分。 !!!! numpyNumpy是Python的一个科学计算的库，提供了矩阵运算的功能，其一般与Scipy、matplotlib一起使用。其实，list已经提供了类似于矩阵的表示形式，不过numpy为我们提供了更多的函数。 在 numpy 包中我们用数组来表示向量，矩阵和高阶数据结构。他们就由数组构成，一维就用一个数组表示，二维就是数组中包含数组表示。 入门 计算 矩阵的余弦看相似度 SVDsingular value decomposition是线性代数中一种重要的矩阵分解，在信号处理、统计学等领域有重要应用。 奇异值分解在某些方面与对称矩阵或厄米矩陣基于特征向量的对角化类似。svd gru模型GRU模型与LSTM模型设计上十分的相似，LSTM包含三个门函数（input gate、forget gate和output gate)，而GRU模型是LSTM模型的简化版，仅仅包含两个门函数（reset gate和update gate）。reset gate决定先前的信息如何结合当前的输入，update gate决定保留多少先前的信息。如果将reset全部设置为1，并且update gate设置为0，则模型退化为RNN模型。 从上面GRU模型和LSTM模型的定义可总结出区别如下 1：GRU包含2个门函数、LSTM包含三个门函数。 2：GRU模型没有output gate，因此它不需要计算输出。 3：LSTM中input gate和forget gate的作用分别为控制输入的信息和控制先前的信息。而GRU中由update gate同时控制输入和先前的信息，即公式中变量z。reset gate直接应用于先前的隐藏状态的控制，即公式中变量f。这样LSTM中reset的作用由GRU中reset和update gate共同完成。 4：输出不再需要加入一个非线性函数。 LSTM模型和GRU模型在应用中的选择 1：从上面的区别可以看出，GRU模型的参数相对更少，因此训练的速度会稍快，从实验中也可以得出该结论。 2：当你的训练数据足够多的时候，LSTM模型会表现的更好。 实验步骤 1：本次实验采用insuranceQA数据，你可以在这里获得。实验之前首先对问题和答案按字切词，然后采用word2vec对问题和答案进行预训练（这里采用按字切词的方式避免的切词的麻烦，并且同样能获得较高的准确率）。 2：由于本次实验采用固定长度的GRU，因此需要对问题和答案进行截断（过长）或补充（过短）。 3：实验建模Input。本次实验采用问答对的形式进行建模（q，a+，a-），q代表问题，a+代表正向答案，a-代表负向答案。insuranceQA里的训练数据已经包含了问题和正向答案，因此需要对负向答案进行选择，实验时我们采用随机的方式对负向答案进行选择，组合成（q，a+，a-）的形式。 4：将问题和答案进行Embedding（batch_size, sequence_len, embedding_size）表示。 5：对问题和答案采用相同的GRU模型计算特征（sequence_len, batch_size, rnn_size）。 6：对时序的GRU特征进行选择，这里采用max-pooling。 7：采用问题和答案最终计算的特征，计算目标函数（cosine_similary）。参数设置 1:、这里优化函数采用论文中使用的SGD（采用adam优化函数时效果会差大概2个点）。 2、学习速率为0.1。 3:、训练100轮，大概需要6个小时的时间。 4、margin这里采用0.15，其它参数也试过0.05、0.1效果一般。 5、这里训练没有采用dropout和l2约束，之前试过dropout和l2对实验效果没有提升，这里就没有采用了。 6、batch_size这里采用问题30字、答案100字。 7、rnn_size为150（继续调大没有明显的效果提升，而且导致训练速度减慢） 8、目标函数采用cosine_similary。 实验效果对比 QA_CNN：0.62左右 QA_LSTM：0.66左右 QA_BILSTM：0.68左右 QA_GRU :0.6378左右 QA_BIGRU ：0.669左右 注：这里分别实验了单向的GRU算法、双向的GUR算法、单向的LSTM和双向的LSTM算法。单向GRU/LSTM的算法只能捕获当前词之前词的特征，而双向的GRU/LSTM算法则能够同时捕获前后词的特征，实验证明双向的GRU/LSTM比单向的GRU/LSTM算法效果更佳。LSTM算法性能稍优于GRU算法，但是GRU算法训练速度要比LSTM算法快。实际使用可以根据自己的要求做出权衡。 !!!!!！！！ gru模型算法和qa LSTM模型nlpir汉语分词系统网址可以线上演示 word2vecword2vec是Google于2013年开源推出的一个用于获取词向量的工具包 从官方的介绍可以看出word2vec是一个将词表示为一个向量的工具，通过该向量表示，可以用来进行更深入的自然语言处理，比如机器翻译等。 N-gram模型通过上面的语言模型计算的例子，大家可以发现，如果一个句子比较长，那么它的计算量会很大； 牛逼的科学家们想出了一个N-gram模型来简化计算，在计算某一项的概率时Context不是考虑前面所有的词，而是前N-1个词； 当然牛逼的科学家们还在此模型上继续优化，比如N-pos模型从语法的角度出发，先对词进行词性标注分类，在此基础上来计算模型的概率；后面还有一些针对性的语言模型改进，这里就不一一介绍。 通过上面简短的语言模型介绍，我们可以看出核心的计算在于P(wi|Contenti)，对于其的计算主要有两种思路：一种是基于统计的思路，另外一种是通过函数拟合的思路；前者比较容易理解但是实际运用的时候有一些问题（比如如果组合在语料里没出现导致对应的条件概率变为0等），而函数拟合的思路就是通过语料的输入训练出一个函数P(wi|Contexti) = f(wi,Contexti;θ)，这样对于测试数据就直接套用函数计算概率即可，这也是机器学习中惯用的思路之一。 1234567// One-hot Representation 向量的维度是词表的大小，比如有10w个词，该向量的维度就是10wv(&apos;足球&apos;) = [0 1 0 0 0 0 0 ......]v(&apos;篮球&apos;) = [0 0 0 0 0 1 0 ......]// Distributed Representation 向量的维度是某个具体的值如50v(&apos;足球&apos;) = [0.26 0.49 -0.54 -0.08 0.16 0.76 0.33 ......]v(&apos;篮球&apos;) = [0.31 0.54 -0.48 -0.01 0.28 0.94 0.38 ......] 词向量表示自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。 最直观的就是把每个词表示为一个很长的向量。这个向量的维度是词表的大小，其中绝大多数元素为0，只有一个维度的值为1，这个维度就代表了当前的词。这种表示方式被称为One-hot Representation。这种方式的优点在于简洁，但是却无法描述词与词之间的关系。 另外一种表示方法是通过一个低维的向量（通常为50维、100维或200维），其基于“具有相似上下文的词，应该具有相似的语义”的假说，这种表示方式被称为Distributed Representation。它是一个稠密、低维的实数向量，它的每一维表示词语的一个潜在特征，该特征捕获了有用的句法和语义特征。其特点是将词语的不同句法和语义特征分布到它的每一个维度上去表示。这种方式的好处是可以通过空间距离或者余弦夹角来描述词与词之间的相似性。 神经网络概率语言模型神经网络概率语言模型（NNLM）把词向量作为输入（初始的词向量是随机值），训练语言模型的同时也在训练词向量，最终可以同时得到语言模型和词向量。 Bengio等牛逼的科学家们用了一个三层的神经网络来构建语言模型，同样也是N-gram 模型。 网络的第一层是输入层，是是上下文的N-1个向量组成的(n-1)m维向量；第二层是隐藏层，使用tanh作为激活函数；第三层是输出层，每个节点表示一个词的未归一化概率，最后使用softmax激活函数将输出值归一化。 得到这个模型，然后就可以利用梯度下降法把模型优化出来，最终得到语言模型和词向量表示。 word2vec的核心模型word2vec在NNLM和其他语言模型的基础进行了优化，有CBOW模型和Skip-Gram模型，还有Hierarchical Softmax和Negative Sampling两个降低复杂度的近似方法，两两组合出四种实现。无论是哪种模型，其基本网络结构都是在下图的基础上，省略掉了隐藏层； 分词处理由于word2vec处理的数据是单词分隔的语句，对于中文来说，需要先进行分词处理。这里采用的是中国自然语言处理开源组织开源的ansj_seg分词器` 分词处理之后的文件内容如下所示： 下载实践这里我没有从官网下载而是从github上的svn2github/word2vec项目下载源码，下载之后执行make命令编译，这个过程很快就可以结束。 123456789101112./word2vec -train ../corpus_out.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1// 参数解释-train 训练数据 -output 结果输入文件，即每个词的向量 -cbow 是否使用cbow模型，0表示使用skip-gram模型，1表示使用cbow模型，默认情况下是skip-gram模型，cbow模型快一些，skip-gram模型效果好一些 -size 表示输出的词向量维数 -window 为训练的窗口大小，5表示每个词考虑前5个词与后5个词（实际代码中还有一个随机选窗口的过程，窗口大小&lt;=5) -negative 表示是否使用负例采样方法0表示不使用，其它的值目前还不是很清楚 -hs 是否使用Hierarchical Softmax方法，0表示不使用，1表示使用 -sample 表示采样的阈值，如果一个词在训练样本中出现的频率越大，那么就越会被采样-binary 表示输出的结果文件是否采用二进制存储，0表示不使用（即普通的文本存储，可以打开查看），1表示使用，即vectors.bin的存储类型 处理结束之后，使用distance命令可以测试处理结果，以下是分别测试【足球】和【改革】的效果： word2vec的模型是基于神经网络来训练词向量的工具； word2vec通过一系列的模型和框架对原有的NNLM进行优化，简化了计算但准确度还是保持得很好； word2vec的主要的应用还是自然语言的处理，通过训练出来的词向量，可以进行聚类等处理，或者作为其他深入学习的输入。另外，word2vec还适用于一些时序数据的挖掘，比如用户商品的浏览分析、用户APP的下载等，通过这些数据的分析，可以得到商品或者APP的向量表示，从而用于个性化搜索和推荐。 深度学习word2vec笔记之基础篇 [word2vec词向量训练及中文文本相似度计算] (http://blog.csdn.net/eastmount/article/details/50637476) 所以突然发现是可以做到的，然而没时间了，我要下周就期末考试了，只能很惨的弃坑，不过这个坑要是小学期有时间一定填上]]></content>
  </entry>
  <entry>
    <title><![CDATA[toefl conversation problem]]></title>
    <url>%2F2017%2F05%2F28%2Ftoefl-conversation-problem%2F</url>
    <content type="text"><![CDATA[about english 一、跑题/插入内容eaasy读作at seeoffer help=volunteer绝不会是相同的词，否则鄙视干扰项 ok,well,ohhh,see,right,now,then,alright表示停顿，上一个语意群已经结束 1、首段若是跑题，跑题内容鄙必是干扰项let me know if you have any questions.在此之前出现的所有内容和今天主旨没有任何关联 so what i can do for you ,so what ca i do for you也是一样 2、首段若是跑题，可以设置为第二题问跑题目的3、若是跑题，跑题信息可设置为重听题，问跑题信息和跑题原因本来是在聊a，然后学生突然说道b，b说完又回到a，说今天还是说a 跑题提示词，本来是在说专业内容，first i have to ask,how many page do we left 二、主旨题中设置为被否定的信息吃火锅不吃、吃烧烤不吃、吃pizza不吃 get the word out=帮助宣传 三、混淆目的主旨和内容主旨学生最初来是因为作业不会做———目的主旨，内容主旨———没上课的原因是去照顾朋友 目的主旨———why does see the professor 内容主旨——mainly about\内容主旨——main ideadrop off my graduation form交]]></content>
  </entry>
  <entry>
    <title><![CDATA[人民的名义]]></title>
    <url>%2F2017%2F05%2F20%2F%E4%BA%BA%E6%B0%91%E7%9A%84%E5%90%8D%E4%B9%89%2F</url>
    <content type="text"><![CDATA[谈谈人名的名义 总体感触第一集中，季检察长在抓人之前一定要汇报给书记。第五集中，出了工人对抗拆迁的大事，高育良打电话，要求祁同伟赶快赶往京州市处理，祁同伟的思虑。这些都是受制于政治规矩的考虑。（什么人可以参加什么会议，什么人可以在会议上发表看法；要抓谁，不抓谁，怎么抓；经谁批准，由谁指挥；地方牵头还是中央直接来）这样写实的场面，还有官商结交时候的规矩。比如丁义珍和一群人吃饭，商人的巴结，官方的“接待”。 第二部分令我感触的是，这些身居党国要津的官员的发家史。陈海，其父亲是陈岩石，原检察院检察长，是一个老革命。其老师是高育良，原某政法大学的政法系主任，现任是汉东省省委副书记兼政法委书记。李达康，其现居京州市市委书记，是原省委书记的下属，其妻欧阳菁，是京州市城市银行的副行长。祁同伟的岳父，是原省政法委书记。而他自己又是是高育良的学生。陆亦可，现为某市反贪局侦察大队一处处长，其母亲是一位法官。……以我所见，仅在我省的小市里头，“门生故吏，同窗发小，父子母女，夫妻翁婿”在整个政府体系里头运作，比比皆是。 有亲缘友缘关系，意味着是有亲缘友缘面子，这个面子，小了说，是个安排，是个调动，中了说，是个项目，是笔资金，大了说，就是一条人命。这些个“门生故吏，同窗发小，父子母女，夫妻翁婿”的关系，是从“改开”以来，政治人才选拔过程的演变中出现的，尤其是以当时政法大学师生和农村干部提拔为主要选择方式中出现的，（所以，侯勇饰演的巨贪小官说的“农民的儿子”，那不是虚写，很大程度上是这些恢复高考后考上大学的学生和农村干部提拔的写实）。像这些官吏，看在他们老师/父亲/岳父/老领导等面子上，“勉为其难”给人家办事的，这个数目不是少，而是很多（这也是从严打以来，底层怕干事的主要因素）；而在“改开”洪流下被冲毁的基层社会，不仅把这种“办事”看成是帮助，看成是荣耀，而且还把“不办事”的看成是迂腐，看成是清高，看成是“不识时务”。想要在这样环境中，当一个想干事又不“办事”的好官，却只能“清静无为”已经是好的了。“挡了别人财路”，却没有被别人通过设计“阴谋”，双规入狱，简直可以称得上是“人生赢家”。可是，不是所有人都不会忘记初心，也不是所有人能够守住自己。就算是依照某乎最流行的利己主义来看，帮那些求着“办事”的人，在“合理范围”内帮衬一下，拿点“朋友之礼”，难道也不应该？“改开”初期，大家都没钱的时候，吃得饱饭的人，想的是让以后自己孩子也吃饱饭；到了后来，大家一部分人有钱的时候，吃的好饭的人，想的是让以后自己一家都能吃好饭。不管是在这些要津上的“老狐狸”们，还是望着这些“狐狸”的“花猫”们，都觉着“难道这个理错了吗？”于是，以前的行业规矩，还是以后的行业规矩，这就是蔡成功们、高小琴们屡屡能在丁义珍们，胡玉贵们身上得手的重要原因。 当然，舍去这个，还有祁同伟们关心的“副省长的位置”，李达康们关心的“市里的GDP”。“高位”和“GDP”,总结起来，就是两个字“政绩”。从千禧年以来，国家大动向是说“稳定压倒一切”，是要“一心一意谋发展”，于是，政绩（GDP）就是成为检验“真理的唯一标准”。在丁义珍外逃的时候，李达康回到京州市，想到的不仅仅是丁义珍的腐败问题，想到的更加是京州市的投资商们逃走不逃走的问题。为什么想到这个问题，往大了说，往脸面上说，那叫关系人生群众的生活。往小了说，往实际上讲，那叫关于个人能力和政治前途。这种政治前途，是很多富有干劲的“老干部”们向往的，尤其是“改开”以后上来的这批人以及这批人所提拔起来的人。他们身居要津，看下属，不是看过程，而是看结果（形势变了以后，也开始注重过程了），不是看计划，而是看财务报表。为什么？潜移默化形成的政治规矩，就是除了GDP，什么都是虚的。你干再对底层人民的胃口，没有GDP，那么对不起，这叫“虚誉”。 当然，说了这么多，作为码农这种无产阶级的我们，大约是体会不到这些东西，也不想体会这些东西。我也就说这些说到这里。我接下来想说一说本剧所展现的“反腐”。 反腐从开始赵处长的反腐，到后来丁义珍的反腐，展现的不仅仅是些许理想化的反腐过程，而恰恰把怎么腐败的方式展现了出来。侯勇饰演的赵处，前任就是倒在了贪腐上，按照一般的政治逻辑，上去的人基本是“不敢腐不想腐”，可是他还是腐败了。为什么？只不过一次应酬的机会，让丁义珍硬生生的把银行卡送到了他的手里。——你不想腐，那就阴谋诡计设计你，让你腐败。你不敢腐，那就柳暗花明罗织你，逼你腐败。李达康的妻子欧阳菁，是城市银行的副行长，丁义珍从她那“拿钱走账”；陆毅饰演的猴子，发小蔡成功来求他办事的时候，先拿着烟酒，美其名曰土特产，送给他；猴子收下一瓶酒，推掉其他的之后，蔡成功瞄上了猴子的儿子，下次登门拜访，拿着溜溜球和游戏机（谢评论区指正）给了小猴子（侯浩然）。——你丈夫不会腐，那就联系你夫人；你大人不能腐，那就叮上小孩子。总之，在传统的“人情社会”里头，形形色色的社会关系中，总能被有心人以有心的方式“成功搞一波”公器私用（比如，赵东来手下的警车，轻易被“借”出去载假警察们）。中下层的政商生态，在这部剧里头，写的是淋漓尽致。当然，除去这些，还有展现的“工人们”自发的对抗那一场戏，也很值得展开。但是这场戏深究下去，已经牵涉到了和贪腐“没什么关系”的“国改私”，我也就说道这里为止了。 祁同伟为何自杀祁同伟最后认为自己是输给了命运，所以他说汉东那么多贪官，为什么只追着我。他不觉得他是罪有应得才会这样，或者说，罪有应得的人多了，为啥被怼的是我？他根本不信正义和公平，如果有这些他当年就不会下跪了。所以没有对与错，只有输与赢，他输了。他觉得他输的不是侯亮平，不是沙瑞金，是老天。所以想让我祁同伟冲你们这些官二代低头认罪，做梦！和我祁同伟付出的相比，你们一路躺着上来的，你们有什么资格审判我！我抛弃了理想，道德，爱情，尊严以及一切，可最后还是无法改变命运，我去你妈的老天爷！关于祁同伟的争论我要表达一下看法。有人说我为罪犯辩解，我并没有。同情祁同伟，不代表认为他无罪。我再同情他，也无法接受他犯下罪行后不受惩罚。他的死是应该的。他自己也知道自己的罪什么大功也抵消不了，必死。但他为什么死前不服呢，他不服的是老天不公平。为什么自己受害时没有正义，而自己害人时正义却到了。祁同伟曾经是个好学生，高育良也曾经是个好老师。公权力的一次滥用，动摇了高育良，彻底扭曲了祁同伟。我做好人，却时刻受苦，他们都做坏人，却享福不受罚，试问，谁还愿意做好人？在这里该思考的是，人是如何变坏的，被什么因素影响的。不解决这个因素，祁同伟高育良杀不完也抓不完。 来看看情节 侯亮平刚下飞机与祁同伟交谈的时候，说到祁同伟谋害陈海，祁同伟大喊：“我没办法！”。 侯亮平转而却说： 没错，你不是这样的人。你不愿意失去巨大的财富。 那个瞬间，祁同伟无语了，只能无奈看天，长叹一口气，甩出一个没人能看见的白眼。那感觉，好气又好笑，只不过现在笑不出来。猴子，你是来认真聊天的吗？我知道你是来劝降的，但也别把老子说的这么低端行吗？随后，侯亮平开启嘴炮模式，对祁同伟不断进行着再教育。而祁同伟心里不断想着的却是，猴子啊猴子，倘若谈判专家都像你这样讲话，那所有的坏人都只剩两个选择，要么铤而走险，要么畏罪自杀，唉…… 你怎么不说说我心爱的高小琴？怎么不说说我那再也见不着的孩子？老子他妈的连老天爷都不放在眼里的人，你给我说什么人民？说什么大风厂？说什么党的形象？ 终于，侯亮平说出了那句终结一切的话： 看着信誓旦旦、一脸正气的侯亮平，祁同伟终于相信，他的这位同门师弟，真的不是针对他。曾经，祁同伟认为，侯亮平这种完美的人，是不存在的（当然，现实中就是不存在的），他一定是戴了某种不易看穿的面具。因为看不透这面具，所以他才不明白，为什么这位师弟始终要盯着自己的老学长不放。可是后来才发现，侯亮平抓他，是为了正义，不是针对他。最后跟他说那么多“废话”，是由衷的情感流露，也不是针对他。祁同伟不想再解释了。一来，他知道，固执的侯亮平，是不可能听进去任何解释的。二来，他也不愿让单纯的侯亮平看到更多的他无法想像的黑暗。就像《天下无贼》里那样，最后刘德华搭上自己的命，也要把王宝强的包悄悄的还回去，还不让他知道。这到底是为了满足老婆刘若英的愿望？还是真的希望傻根能永远活在一个天下无贼的世界里？ 这些都不重要了。重要的是，他最后这么做了。后人有的会说，刘德华犯傻，放着命不要也要满足一个傻小子的白日梦。有的会说，刘德华是良心发现，变了好人了。现在祁同伟，放下了本想杀侯亮平的枪，把最后一颗子弹打进自己的喉咙。我想成为那个，说他是死前对侯亮平心软了的人。 =====================猴子，倘若当年的我，也能如你一般保持着单纯到现在，会是什么样呢？恐怕早就被权力玩儿死了吧。但是你不一样，你命好，你有背景，所以你可以单纯。就算有实实在在的证据证明你违法，也有人保护你。你想抓不可能被抓的人，也有人挺着你。唉，多希望你能一直单纯下去，别走了我的老路。这辈子，我已经这样了。但是想叫我这个师兄给你低头，你还不够格呢。老子连老天爷都不放在眼里，哪里又轮得到你小子来教训我？今天老子就是要死个惨烈，让你小子长个记性，以后抓坏人，别一本正经的胡说八道，要了解他，要理解他，要站在他的立场看问题，要……罢了罢了，想这些还他妈的有何用？以你的本事，还用我瞎操什么心？呵呵，可笑我总是对人说要胜天半子，却没想到真的有祭棋这一天。来吧！祭棋就他妈祭吧！去你妈的吧！ 祁同伟祁同伟到最后一刻才意识到自己错的有多厉害。直到侯亮平开着伤害免疫光环站在他面前时，祁同伟才意识自己的错误。如果当时，他祁同伟一脚踹开了高老师的教室大门，跪在了小师妹钟小艾面前…… 梁书记能提早进政协，陈海就不用出车祸，侯亮平也能在金山县司法所摆正自己的位置，高老师和吴老师也能在大学里做一对的神仙眷侣，侯勇老师也能多吃几碗炸酱面，就连和陆处长相亲的基建处长也不会被人查…… 然后再去江都哭个坟，等赵书记进了政协，说不定就是祁沙配了。 所以呀，祁同伟应该在想：贼老天，就输了半子。补上一颗，一定能行吧。]]></content>
  </entry>
  <entry>
    <title><![CDATA[编程之美lecture]]></title>
    <url>%2F2017%2F05%2F18%2F%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BE%8Electure%2F</url>
    <content type="text"><![CDATA[lecture from 微软 自动问答系统简介（段楠）介绍典型的自动问答系统，包括基于知识库的自动问答（KB-QA）、基于文本的问答（Text-QA）和基于FAQ的问答（Community-QA）等。此外，还会对本次大赛问答任务中涉及到的技术做简要说明。 基于自己学校的网站打造机器人，问题回复和信息抽取，做人机交互。包含很多功能和模块。 任务，方法，涉及到的问答方法作介绍并基于真实实例 系统基于给定的数据库，回答人类提出的问题。 问题类型： 答案是实体，答案是定义，答案是yes/no，答案是意见，比较类问题 早期qa system是专业特定领域的，后来计算能力的飞跃，允许问答系统到开发领域 三大类： 基于知识的问答系统KBQA系统的数据库是结构化的，但是知识库是一些实体之间的关系，实体之间的关系是以抽象的标签的形式存在的 关键是两点：一是能不能把问题中实际到的实体检测出，另一是问题中提到的实体的关系检测出来 应用：微软的必应bing，可以直接直接结构化的查寻，优点：直接拿到答案，节省用户去结果网页中进行信息再定位 从学校主页上挖掘 主语+谓词+宾语（结构化的三元组），可以回答这样的问题：清华大学校长是谁，先做实体的识别，知识库中知识是以实体和关系来存储。实体识别：清华大学，上下文究竟提到的是关于实体的哪个关系的识别：校长关系，答案检索：&lt;清华大学，校长，邱勇&gt; 那么可是如何做基于给定问题的实体识别和关系识别： 从基本的方法开始，从一些百科类网站来获取实体名称，用上述的知识对输入的问题来标注，来标注问题中究竟提到了哪些关系。其他问题：同一实体有很多不同说法名称，一些细致的地方就要做一些扩展关系识别： 一类是可以创造基于问题模板的关系识别方法（谁是什么什么创始人—&gt;创始人。人工撰写拓展问题模板或者搜索日志中抽取。要问题模板很全才能完全覆盖）。 一类是创造基于关系关键字的方法，创办人–从其他地方抓取关键字都表示了创办人的意思。提示：如果有很多满足当前关系的实体对，可以去海量去文本中挖掘句子，这个句子的条件是同时包含了满足这个关系的某一个实体对。若两个实体出现在一个句子中，这两个实体间又在知识库中存在某关系，那这个句子的其他部分就在对知识库里的这个关系做陈述。通过类似方式来抓取一些知识库中关键多对应的不同的关键字来用于检测问的问题是提到知识库中的哪个关系 基于文档的问答系统DBQA很多知识是无法经过人类整理编辑录入知识库的，哪怕是google的freebase也是不全面的。需要利用其他一些非结构化的知识，可以基于文档中的句子来进行搜索 应用：问题对应的搜索返回网页中包含的句子和段落很好的回答问的问题，直接就返回了结果 预选赛中的一个任务：给出了训练数据开发数据测试数据，训练数据给出了三元组，问题是同样的问题，七句话来自同一个篇章。第一列是答案标签，第六行是1：是答案，其他不是答案：是0。任务是：给你一个篇章再给一个问题，选出篇章中的一句来保证这是当前问题的答案。根据答案标签来训练问题答案句匹配模型，但是实际测试时只会给当前的篇章和问题，没有答案标签，要排序，排出最相关的一句话。 再比如校园机动车公告网页，第四句话是非常相关的，作为答案输出。对文档内容做处理。 提示：1、数数，重复的字词很多，就有问题答案关系 2、词向量。每个句子都可以转化成词向量，表示当前词的语言。然后看词向量上的距离 3、深度学习工具，做模型上的训练，使问题和正确答案相关性非常强，以实现答案抽取 基于社交型（问题答案对）问答形态FAQ search像百度知道，奖励机制，点赞来提取出高质量的问题和答案对，用历史答案来回复现在的高相似度的问题。 应用：微软小冰，进入公众号后，索引整理该公众号的历史文章，用户会问文章中含有的问题（一般是多引擎协同工作），直接用历史文章中的答案对来回答 核心的目的是计算一个输入问题和历史问题的相似度，这就需要一个问题和问题的匹配模型。那么采用如何的训练数据来得到这样一个匹配模型。像百度知道这样来抽取训练数据，页面中会有一堆其他类似问题，即重新定向到历史问题。有这样的问题和相似问题的数据之后就可以训练一个模型，给一个分数来表明两个问题的相似度。 计算相似度是一个语义理解的问题，这是nlp中的最核心问题。概念：有一些方式根据相似数据做模型，如一个词缀对齐。 第二类是同义词和近似词表，另外海量的翻译数据，对翻译数据进行对其之后，一个现象：如果源语言端的两个词或者短语对应的反义词是相同的，则这两个源语言是近似的。同义词的信息抽取 第三类是有了相似问题的海量数据之后，可以用深度学习或者是神经网络模型来做一个模型。概要性解释：把每一个问题转化成一个n维或者是1000维向量，每一个维度都是实值，人觉得是一千个数字，但是机器觉得1000维数字来对问题来编码来保证语意相同的两个句子在向量空间距离上是非常近的。给工具包，看说明 一些工具包中文分词词向量句子向量语言理解 最后的是去年的评测比赛提供的两个的基准的问答系统的实例。虽然系统非常简单，但可以以此为基础进行扩展 如何建立面向任务的自然语言理解模型人工智能助理（AI Bot）的一个重要部分是自然语言理解模型。本课程将介绍面向任务的自然语言理解模型，以及如何利用微软认知服务的自然语言理解智能服务（LUIS）快速开发应用合适的模型 什么是面向任务的自然语言理解模型三大特点：通过自然语言交互、完成特定的任务、具备一定知识和推理的能力 举例：通过自然语言交互：首先制定会议对应一个功能，team包含一组人，时间地点。慢慢拆解每个句子部分 转入到meeting的功能：四个会议的要素 myteam：office graph 相对时间： 根据测算 地点： 对于面向任务的自然语言bot，他们的模型是两大步 第一步：叫做intent，就是理解用户需要我们做什么。倒过来想，能帮用户做什么。 第二步叫entity，在那句话中给了我们哪些参数。或者说需要哪些参数 两部分解决方法基于规则：像正则表达式，去纂写规则。当手头上并没有用户数据（冷启动）。成长困难，用户负责性变多，管理这些规则是很难的，尤其是协作、管理数据、标定数据。 基于数据：应用机器学习算法，用数据来学习对应的模型，更可持续 机器学习算法用于自然理解模型中可用于两部分 intent：文本分类问题。线性模型、逻辑回归、cnn、lstm entity：从一句话中找到相应段落。然后给它一个语义标签。可以变成一个序列标注问题。条件随机场，dl中crf的算法 但是我们今天讲的是基于数据的解决方案 LUIS 介绍下：这是个平台化的机器学习解决方案，简答易上手 通过内建减少feature上花费的时间，高阶语言的表示。两个常用特征：entity的列表和正则表达式 基本工具：解析时间、标注器、很多领域的工具 suggestion：通过主动学习 demo 先登录，跳转到my apps，点new apps。dashboard，放了常见信息。 点开intent，增加新的:查考分 输入一些utterance：如计算机系的录取分数线是多少，然后save。所以同时想到要在entity那里加一个“院系”。然后回到utterance，把计算机系这几个字框住，然后标注为院系，save。就形成了一句标注的语句。 previous entity中有些时间年龄之类的內建。通过这种方式不停的去加训练数据。 再来看list feature，提供列表。给一个值之后有更多类似值推荐。 点train application就开始训练。然后就可以测试了。 下一步就是发布，endpoint key选luistest，然后点publish，拿到一个url。通过get拿到结果 add flag勾上，timezone可以解决相对时间passing的问题，在postman中测试 再看看一个正常的bot（可以直接import dataset） batch testing里面增加数据集 dashbroad里面的suggested utterance：通过发布的链接进来的所有的句子都会通过一个模块，推荐到这。选择一个模型，如capacity，然后相对应的句子加入到utterance单元中去通常这样做会得到一个更好的模型 经验总结1、兵无常势针对具体的任务选择规则还是数据或者是两者的结合 2、循序渐进 3、三思而行 框架中具体的细节决定成败：模型方面、数据方面、如何交互、如何提示，简单易用和足够聪明 1.介绍微软Bot Framework 的配置以及如何用Bot Framework SDK开发对话聊天机器人。2.介绍如何使用Bot Framework中的QnA Maker开发问答聊天机器人。3.介绍如何使用Bot Framework讲自己开发的聊天机器人发布到多个聊天工具中。4.用实际案例帮助大家熟悉Bot Framework的使用以及在开发过程中需要注意的问题。 Bot Framework开发框架，帮助开发者快速有效开发 三大重要部分 一、bot connector帮助开发者将聊天机器人发布到交流平台上，所以开发者只需要开发一个后端服务，节省通讯端的开发成本 二、bot builder sdks开源sdk，快速生成开发模板，提供开发工具。这次参赛选手主要是和bot builder打交道 三、bot dicrectory像一个bot的商店，可以看到并用其他人开发的bot 今天主要是以实际操作为主开发环境贮备和基本步骤实际操作来快速上手 准备今天用C#。其他语言可以看官方文档升级到最新版本 下载bot application template模板，然后拷到对应目录 引入bot bulider的sdk直接从github上下载bot builder源码 最后，需要安装bot的模拟器 实际操作例子官网注意看文档（包括如何连接人工智能的工具louis、关于图像和语音的api） 下载，解压之后放到vs的temple文件夹里面 新建一个工程 new bop 包的使用然后在browse搜寻bot builder，然后installupdate， messagecontroller是我们主要要面对的，用于收发用户的消息 dialogs就是对话的主要组成，今天主要来看在收发消息的时候怎么和用户进行交流，进入rootdialog 有个函数交messagerecieveasunc，收到消息返回的方法， 得到是result转化成activity，是一个类，回复是文字 看一下demo，问题和答案 用模拟器来调试 编译运行![](http://i4.buimg.com/588926/f4cfe0c8c5ed57c9.png会发布一个http本地服务，在模拟器中后缀http://localhost:3979/api/messages 另外一个多轮对话的demo 对于Louis新建一个app，新建两个entity，一个是教学楼一个是系别需要丰富train之后publish 返回json的结果，识别问句的意图，得到关键属性，然后bot框架 则帮我们包装，让我们更快的使用louis的结果 在luismodel中填入luis的id和key，节省开发时间，要定义两个常量即是entity里面的系别和教学楼具体看查询位置的方法，试图得到building的值，看json结构中有没有entity，然后回复位置在哪 前面这个例子讲解了bot框架怎么来和luis链接，来让luis处理nlp中的意图识别和实体抽取的问题，然后用bot框架来执行相应的逻辑，并和用户进行交流，最后再看个更加复杂的例子:表单。对于填表，提供了一套框架 先定义一个枚举类型：男还是女还有一些模板的定义，定义了四个关键的属性numeric对用户输入进行校验pattern快速写一个正则表达式 回调函数，然后看看formbuilder 有些关键词message、field、confirm、oncompletion、build]]></content>
  </entry>
  <entry>
    <title><![CDATA[计算机专业导论]]></title>
    <url>%2F2017%2F05%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%AF%BC%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[针对计算机课程自学话题，画出一张计算机专业流程图，并结合几个最重要的课程，具体讲学习方法 学习路线 课程用浪潮之巅起步 最开始两方面： 四个数学，四个编程语言都要会 数学学习的方法专业基础课程：最重要的四门考研课程数据结构算法、机组、操作系统、计网（另外还有编译原理、面向对象的方法、关系数据库理论、软件工程） 原则与方法：网络应用和这些基础课是相辅相成的，可相互促进理解 学习方法 学习动机和方式策略（《学习之道》）不要多件事情来回切换 基于组块的场景式地学习：确定学习目的，有选择性地学习，来形成知识组块，构建知识框架 必须循序渐进，并且多次反复（一环套一环，不能跳） 看遇到看不懂的地方，要停一下，思考一下，如果是因为基础不够，那么则需要先补基础，这就是反复学习。 高度可控，及时反馈，进度条 比如说游戏：阶段明确，第一关很简单，每一关有奖励 考试：考试大纲+刷题 技术：场景式组块学习法 从感性进入（插图）到之后的理性分析和实践的统一 对知识之网从哪里切入知识是有关联的，学习特定知识的时候要明白是一个子网。节点之间的联系 新旧知识之间是有关联的的，而不是孤立的。 所以就没有学不会，学不会的原因是第一与现有的知识之网的联结，第二是因为学的知识处在某一复杂的知识网络之中，特定的知识群没有了解 方式不好的方式 如中心开发：例如数学没有学好就开始学机器学习，外围节点没有掌握；广度遍历，距离现有的知识太远，四处出击； 好的方法 从现有的知识出发，深度遍历是更有效的学习方式，学习不要网状的形式跳来跳去来学习，而是应该以从树根到树叶的方式完整的路径，数据结构中的深度遍历法。一个分支走完之后再走另一个分支。再把这些分支组成一个网络。 n-1底层和n+1上层： 如http协议和web开发框架，但是这是相对的，一些细节不需要了解，只需要了解和自己相关的稍微顶层和稍微底层 计算机系统是分层的，每一层都有特定的工作岗位，依据想从事的，确定工作层次，来明确需要精通的知识集合 比较远的层次，可以不去理会 以互联网为老师，自主学习，终生学习 读书要自学就先得读书，首先是选书 阅读一本书若是超过自己能力，是没有意义的。选择具备读懂前提的书（即便再好的书，看懂才有意思）（看前言、或者几页就看不懂了） 分类阅读 ：科普书籍来入门，专业课本学基础，专业技术书籍来深入（如作者个人开发经验小结解决实际问题） 尽可能用英文书（写一个中英文术语对照表） 目的决定阅读方法：若是理解，就应该探究式；若是应付考试，考哪些，记住弄熟 多遍阅读法多遍阅读法 如系统方法和自顶向下两本书；确定先读什么后读什么，前言，目录和序；网络设计这方面，网络开发者方面 目录：精心组织来体现写作意图，目录是构建知识之网的手段，用对比目录来确定这个领域的知识网络图。重要的知识节点。 正文：多遍扫描，像编译器一样。书的第一章包括重要概念，要反复读。其余每章就是开头部分要重点读，其余就可以略读。看结论和观点。第一遍不要关注细节。 从而切实可行的读书计划就成型了 第二遍是挑出最感兴趣的，不懂得打个记号，重点去学。留在第三遍去攻克。哪些是重点哪些不是重点，你自己去判断。 第三遍只在干货上下功夫。第四遍啃硬骨头，去翻翻论文，去学一下需要用到的数学知识，在此基础上需要通读全书，构建整个知识网络。普通书籍一两遍就够了，只是那些非常经典的书籍和考试书籍才要三四遍或者更多遍。 阅读技术书籍或者很难专业书的障碍有三点：你不具备作者假设你已经掌握的知识（计算机体系结构的设计方法），书中满页的数学公式（机器学习）看到就怕，大牛的写作水平不怎么样很晦涩，如显而易见，易得…..（北大离散数学教程） 应付难书的解决方案：先阅读简单科普书，先弄明白是哪些公式，攻下那些公式，先看前面的书扫清知识障碍。没有任何捷径可走 断点法：看不懂了，多半有知识没弄断，果断放弃，扫清障碍 对照法：多看基本教材，互相补充，建立起知识的联系 教学视频法：很独特，程序的调试过程，算法设计过程，很动态的过程是书无法表示的 计算机专业课程详解数学数学在项目中并非决定成败，只需要知道如何调用，算法细节不需要了解。但是中高级程序员，一些特定的技术还是，特定的问题建模和设计算法，要扎实数学基础 有哪些数学分支是计算机必须学的：离散结构（高数、线性代数、离散数学、概率论） 吴军的数学之美：数学在计算机中的具体运用，开发框架来把数据可视化，python有很多数学的库 计算机的全局观：知识地图，所学知识在整个计算机体系中处于什么地位，随便逛逛，有哪些子领域，然后这就找到你自己的学习方向，关注作者视角，他是如何介绍计算机这个科学领域的。 如csapp，从使用者角度来讲解，把零散的知识组块化，再建立组块之间的联系。从而建立整体知识框架。再去阅读一些具体的书籍。 骨干课程数据结构和算法数据结构表示信息本身，算法表示信息的处理过程。但实际上是无法分开的。包容大量编程练习和acm，ai和大数据玩的就是算法 两大块：数据结构和算法的理论基础，和各种具体的数据结构和算法介绍。第一部分中，弄明白其中的概念和术语，怎么设计一个算法，怎么评估他的优劣。第二部分，只需要记住最常用的几个，不必要每个深挖。每个数据结构和算法都有特定的应用场景去解决特定问题。学习方式：先不看特定问题的书上解答，自己尝试去解决，实在解决不了，再看书。用自己掌握的语言去尝试实现。java中的数据结构库可以看源码。 流程：算法图解+算法基础+数据结构和算法java实现+算法导论（很多数学证明，但是可以完全不看）（算法是用java写的，很好） 算法知道体现的思想和应用优缺点就行了，开发框架中有现成组建。不必要知道细节，只需要调用函数。 如果在算法密集的领域工作：在算法上 下功夫很必要（数学基础得好） 操作系统操作系统：分为教材与专著（和软件开发和动手实践结合） 最重要的是理解操作系统的内部机理：推荐c语言。动手去完成实验，另外应用层的软件开发（安卓），安卓是多线程的，可以结合来理解进程线程。linux在上面下功夫绝对没错（linux内核深度解析）。但是没必要深究到操作系统源代码程度 计网学习计算机网络路线：用c、python、java去编写网络应用，操作系统来负责实现，基本上都是多线程的，网络中还有路由算法 计算机网络最重要的是分层，三本书分别是：自顶向下，自顶向下，横切 计网最重要三个问题：如何连接、数据如何传输、网络系统如何构建 网络协议是上层应用开发的必须，不懂得计网基本原理去写项目都是沙滩上盖楼 具体计网学习建议： 重点理解分层：每个层解决的主要问题和解决方案应用原理算法 动手：编程语言来实现两台计算机的信息互通，实现web server；重点学习http+面向对象，网络抓包工具， 最后以一个图结束 七月份有个程序员live九月份有个关于职业发展的live]]></content>
  </entry>
  <entry>
    <title><![CDATA[query入门]]></title>
    <url>%2F2017%2F05%2F16%2Fquery%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[主要内容：对通用搜索引擎的查询推荐技术的方法、评价进行了总结具体内容：“查询推荐”的不同英文叫法：Query Suggestion、Term Suggestion、Query Recommendation、Query Substitution、Query Rewriting 查询推荐的任务：找出和用户查询相似的query，以便更好地表达用户查询意图，供用户便捷输入 三种技术方法：1. 基于文档的方法：通过处理query搜索出来的文档，以此作为反馈，进一步理解用户意图，扩充query （1）全局文档分析：方法如题目 （2）局部文档分析：说一个更通用的名字，是伪相关反馈，用搜索结果的前N篇文章作为文档集合，扩展query，从伪相关文档中进行降维是一个难点，LCA等 （3）基于语言学资源的分析：说白了就是用wordnet或者hownet或者wiki来做同义词扩展 2. 基于日志的方法：日志作为用户的点击行为的记录，能够忠实的反映用户的真实意图。 （1）基于session的方法简单地说，就是用户搜了什么之后还搜索了什么。往往最后搜索的那个query是能够满足用户当时需求的query。 这个方法，session的判定是个难点 ###（2）基于click的方法：点击模型 ，这样的query和点击向量组成的数据结构，可以用一些距离函数（如：余弦距离）来计算query之间的相似度这种方法，用户click的url很多，造成向量的维度很多，降维仍然是一个问题，两种降维途径：1. 聚类；2. 矩阵分解 ###（3）基于时间分布的方法即统计query的时间分布，如：在情人节那天搜索“巧克力”的人会比较多。不过这个一般只能作为补充。 评价方法：基本思路还是人工建立评测集合（或者找国际标准评测集合，如trec），然后算准确率、召回率、NDCG等]]></content>
  </entry>
  <entry>
    <title><![CDATA[吴岸成《NN and DL》]]></title>
    <url>%2F2017%2F05%2F15%2F%E5%90%B4%E5%B2%B8%E6%88%90%E3%80%8ANN-and-DL%E3%80%8B%2F</url>
    <content type="text"><![CDATA[机器学习和神经网络入门 微软有一个图形化的深度学习在线工具–为了简化技术–技术的进步 历史geoffery hinton辛顿、lecun勒邱恩、bengio某个内向的德国人、李飞飞 自图灵提出机器和智能之后，两派，一个是自顶向下，用逻辑和符号系统（控制论）；另一派是字下而上，模拟生物学大脑，想要获得意识。 定义标准的机器学习问题机器学习算法是普通算法的优化 回归问题（多分类问题）： 选择一个橙子：training data 用一个表格来包括物理属性：feature和吃的时候的感觉：output variable 决策树算法，上述模型即规则库。如果用到香蕉，就是迁移学习。更多样本，增强学习。 神经网络（人工）神经网络是机器学习的分支。 神经元的特征：兴奋性（阈值）和传导性 学习过程：神经元之间的关系变迁（有自组织性） 构造神经网络构造一个神经元树突 — 信号处理（s=信号乘以强度）— 传递函数 感知机感知机：最简单的神经网络 感知机的学习：训练方法，就是监督学习。通过期望值不断修正权重。 用Java实现感知机–Neuroph：基于Java的神经网络框架一部分是用来创建神经网络的API，另一部分是图形工具–帮助构造多层神经网络。 第一部分的api分为三块：1、neuroph.core核心库，多个类库 输入层、隐藏层、输出层：每层由神经元组成 训练规则包含一个训练集，训练集由单个训练元素组成。 类neuron，表示单个神经元的构造 类connection，表示神经元的连接 代码实现实践 构造神经网络迭代几次才能使网络给出正确输出是不确定的 and要5次，or要12次，那为什么异或几十万也无法实现：线性不可分问题 如何解决:多层神经网络，如具有第一隐层，第二隐层。利用多层感知机。 更多层，会更耗时，但会更更精确 实际问题解决输入层和输出层一般按照数据集和需求确定，隐层的神经元个数需大于输入层，需求是精度高还是运行速度快 分类和特征识别问题：疫情、比赛、股票 深度学习深度学习是神经网络的一个大的分支，深度学习的基本结构是深度神经网络 深度神经网络大部分是至少有一个隐层的神经网络，其余小部分是递归神经网络和卷积神经网络。 机器学习的三种方式机器学习：通过算法，从大量数据中学习规矩，从而对新的样本做预测 监督学习：训练数据：输入数据（包括正确的训练集和错误的训练集），不断地与人工标注过的数据库作比较，然后不断调整模型 非监督学习：模型自己去做聚类学习，常见算法：Apriori算法、K-Means算法 强化学习：连续决策的过程。与监督学习的区别是，当不知道标注是什么的时候，给一个回报函数，回报函数决定当前状态得到什么样的结果，数学本质是一个马尔科夫决策过程。最终的目的是决策过程中整体的回报函数期望最优。 或者说，通过一个过程来回的调整所谓的“标注数据” 但是，当遇到高维数据的时候（游戏），处理数据的速度跟不上画面速度。此时要求助于深度学习 通过强化学习引入深度学习 深度学习是神经网络的一个大的分支，深度学习的基本结构是深度神经网络。用韦恩图来表示 特征特征是机器学习的原材料 特征粒度：我们在一个什么粒度上表示特征，才能发挥作用？ 任何事物都可以划分成粒度合适的浅层特征，而这个浅层特征一般就是我们的第二层输入 结构性特征具有明显的层级概念，从较小粒度划分，再用划分的基本特征组成上层特征，以此类推，可以展示特征的结构性。 从2006年开始深度学习爆火，为什么？几篇划时代性的论文： 1、a fast learning algorithm for deep belief nets 2.advances in neural information processing systems 3.advances in neural information processing systems 具体而言，1、非监督学习被用来训练各个层 2、每个层次学习的内容作为下一个层的输入 3、用监督学习来调整层与层之间的权重 如何训练有多个隐层的感知机：bp算法由信号正向传播和误差反向传播 核心思想：在反向传播上将所得误差分摊给各层所有的单元，修正权值 一般在5层以内的神经网络中使用 深度学习的常用方法深度学习是一个算法，但是特征的抽取过程是一个抽象的问题 以前如k-means等聚类都是对一个问题有解，但是深度学习不同 如何选取和如何训练？ 卷积神经网络cnn一个模型架构：一种特殊的对图像识别的方式，属于带有前向反馈的网络 循环神经网络杂项]]></content>
  </entry>
  <entry>
    <title><![CDATA[nlp入门笔记]]></title>
    <url>%2F2017%2F05%2F14%2Fnlp%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[nlp入门学习 概述基础理论自动机 形式逻辑 统计机器学习汉语语言学 形式语法理论 语言资源语料库 词典 关键技术汉字编码词法分析 句法分析 语义分析 文本生成 语音识别 应用系统文本分类和聚类 信息检索和过滤 信息抽取问答系统拼音汉字转换系统 机器翻译 新信息检测 技术数据稀疏与平滑技术大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题，符合经典的zip’f定律。如IBM, Brown：366M英语语料训练trigram，在测试语料中，有14.7%的trigram和2.2%的bigram在训练语料中未出现。 数据稀疏问题定义：“The problem of data sparseness, alsoknown as the zero-frequency problem ariseswhen analyses contain configurations thatnever occurred in the training corpus. Then it isnot possible to estimate probabilities from observedfrequencies, and some other estimation schemethat can generalize (that configurations) from thetraining data has to be used. —— Dagan”。 人们为理论模型实用化而进行了众多尝试与努力，诞生了一系列经典的平滑技术，它们的基本思想是“降低已出现n-gram条件概率分布，以使未出现的n-gram条件概率分布非零”，且经数据平滑后一定保证概率和为1，详细如下： Add-one（Laplace） Smoothing加一平滑法，又称拉普拉斯定律，其保证每个n-gram在训练语料中至少出现1次，以bigram为例，公式如图： 其中，V是所有bigram的个数。 Good-Turing Smoothing其基本思想是利用频率的类别信息对频率进行平滑。调整出现频率为c的n-gram频率为c*：直接的改进策略就是“对出现次数超过某个阈值的gram，不进行平滑，阈值一般取8~10”，其他方法请参见“Simple Good-Turing”。 InterpolationSmoothing不管是Add-one，还是Good Turing平滑技术，对于未出现的n-gram都一视同仁，难免存在不合理（事件发生概率存在差别），所以这里再介绍一种线性插值平滑技术，其基本思想是将高阶模型和低阶模型作线性组合，利用低元n-gram模型对高元n-gram模型进行线性插值。因为在没有足够的数据对高元n-gram模型进行概率估计时，低元n-gram模型通常可以提供有用的信息。公式如下如下 扩展方式（上下文相关）为如右图 λs可以通过EM算法来估计，具体步骤如下: 首先，确定三种数据：Training data、Held-out data和Test data；然后，根据Training data构造初始的语言模型，并确定初始的λs（如均为1）；最后，基于EM算法迭代地优化λs，使得Held-out data概率（如下式）最大化。 处理工具OpenNLPOpenNLP是一个基于Java机器学习工具包，用于处理自然语言文本。支持大多数常用的 NLP 任务，例如：标识化、句子切分、部分词性标注、名称抽取、组块、解析等。 FudanNLPFudanNLP主要是为中文自然语言处理而开发的工具包，也包含为实现这些任务的机器学习算法和数据集。本工具包及其包含数据集使用LGPL3.0许可证。开发语言为Java。功能： 文本分类 新闻聚类 中文分词 词性标注 实体名识别 关键词抽取 依存句法分析 时间短语识别 结构化学习 在线学习 层次分类 聚类 精确推理语言技术平台(LTP) 语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心历时十年开发的一整套中文语言处理系统。LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口，可视化工具，并且能够以网络服务（Web Service）的形式进行使用。 自然语言处理技术难点单词 的边界界定在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。词义的消歧 许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。 句法 的模糊性自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析(Parse)出多棵剖析树(Parse Tree)，而我们必须要仰赖语意及前后文的信息才能在其中选择一棵最为适合的剖析树。有瑕疵的或不规范的输入 例如语音处理时遇到外国口音或地方口音,或者在文本的处理中处理拼写,语法或者光学字符识别(OCR)的错误。 语言行为与计划句子常常并不只是字面上的意思；例如，“你能把盐递过来吗”，一个好的回答应当是把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程上一年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Distant supervision]]></title>
    <url>%2F2017%2F05%2F13%2FDistant-supervision%2F</url>
    <content type="text"><![CDATA[远程监督：使用未标注语料做关系抽取 1. 背景：关系抽取（某个人是否属于某个组织等） 关系抽取中使用的3种方法： a) 监督学习优点：准确率很高 缺点：1.手工标注金标语料代价昂贵，时间金钱上需要很大的开销，并且数量受限，得不到大量的训练数据; 2.领域受限，标注都是在一个特定的语料中，训练的系统受限于那个领域 b) 无监督学习。 优点：可以使用大规模的数据，抽取出大量的关系缺点：抽取的结果往往比较难映射到特定的知识库 c) Bootstrap learning，往往有低准确率的问题。d) 远程监督 使用知识库（freebase）来获取weekly labeled training data。、特点：相比监督学习，使用知识库提供训练数据来取代人工标注获取训练数据，没有过拟合的问题和领域依赖的问题；比起无监督，不用解决聚类结果到关系的映射问题，并且使用大规模的训练数据可以得到丰富的特征。2. 方法介绍训练阶段使用 named entity tagge标注 persons organizations 和 locations；对在freebase中出现的实体对提取特征，构造训练数据；训练多类别逻辑斯特回归模型。 测试阶段：使用 named entity tagge标注 persons organizations 和 locations在句子中出现的每对实体都被考虑做为一个潜在的关系实例，作为测试数据使用训练后的模型对实体对分类。 3. 特征选择3.1. 词汇特征：a) 两个实体中间的词序列b) 这些词的词性标记 c) 标志位表示哪个实体出现在前面 d) 大小为k的左窗口 e) 大小为k的右窗口。 3.2. 句法特征：a) 两个实体之间的最短依存路径；b) 两个实体的左右窗口。 3.3. 命名实体tag特征：人名、地名、组织名和其他 4. 其他连接特征来丢进多类逻辑斯特回归模型。 负例构造：随机选取不在freebase中的实体对（有错误的可能） 5.原文及我的全文翻译可向我索要]]></content>
  </entry>
  <entry>
    <title><![CDATA[昆曲]]></title>
    <url>%2F2017%2F05%2F13%2F%E6%98%86%E6%9B%B2%2F</url>
    <content type="text"><![CDATA[薪尽火传的昆曲 综述昆曲传奇的人文之美体现美好人生的寄托和希望，昆腔传奇显现了永恒的人性之美，表现忠贞爱情题材的作品和人物形象。魏良辅提出的“曲有三绝”，指的是字清#腔纯#板正 。汤显祖在牡丹亭《题词》中有言：“如杜丽娘者，乃可谓之有情人耳。情不知所起，一往而深。生者可以死，死亦可生。生而不可与死，死而不可复生者，皆非情之至也。”阅读《牡丹亭》，享受文字的飨宴，穿越时空的生死之恋，不必借助现代科技，缠绵秾丽，至情弘贯苍茫人世，逶迤而来。 恋恋昆曲，前世今生昆曲的前世起源于江苏昆山，也叫做昆山腔。明代魏良辅等人改革昆山腔，南曲北曲融合，水磨昆山调。魏良辅所做：调理腔调和语音的关系，完善和提升曲调的音乐性，兼容并蓄融合南北曲为一炉，伴奏场面和乐队编制的完善 。昆曲盛世，明清传奇的光辉，万历年间。民国初年被京剧的大盛代替，强调多元，兼容并蓄。如梅兰芳。1923年最后一个昆曲戏班解散，昆曲即将死亡。但是，苏州昆曲传习所的传字辈艺人。忍受寂寞到新中国成立后，《十五贯》拯救整个剧种，田汉周恩来毛泽东。 昆曲的今生苏昆上昆等大剧团，艺以人传。等等老艺术家，一言蔽之，情向前生种，人逢今世缘。 昆曲的文化既然生活了就会去追寻，文化的雅俗争胜，不同的品味在不同的时代显示群体的爱好。俗是指自然的东西，雅的东西是追求层面的东西。无论艺术的发展还是文明的发展都是源于生活而高于生活。南戏的声调、腔调与地方有关，但是韵是规定的。腔调演变：海盐到昆山，水磨调的昆山腔，还是说道魏良辅。而梁辰鱼则是创作剧本，《浣溪沙》，汤显祖对其评价也颇高。戏曲的故事不重要，重要的是表演，去体会，怎么用艺术的形式展示悲欢离合的角色。而生离死别是每个人都会经历的，剧场的气场，所以真正演出是可以把你引进去，引发你自己的情感世界，这是与碟片大大不同的。 汤显祖临川四梦《紫钗记》#《牡丹亭》#《南柯记》#《邯郸梦》：生与死的关系，情感的追求，人生的意思也意味着思想的解放和人性的思考，生死问题的触碰。他受达观和尚的影响，对情的纠缠。 牡丹亭的意义：文学性和水磨调的完美配合昆曲的复兴和汤显祖的重新发现，汤显祖的昆剧是一种高雅的艺术。而高雅是一种追求，高雅是人类文明的追求，甚至说对人类文化发展都是有意义的。 民间艺术元代的特殊性。元杂剧作家身份的双重性，科举取消后三个走向：隐居；全真教；勾栏瓦舍（关汉卿）。从而实现了雅俗共赏。昆曲、文人和民间的融合，民间对昆曲的吸收，采纳，交流不断加深。明代汉族传统重建时，对戏曲身份的接受和认可问题就出现了。皇室承认了昆剧，自此，才出现万历以后昆曲200年的辉煌。昆曲文学创作，传承大于创新。昆曲扶持工程，抢救保护扶持。 魏良辅开始用乐器，武场：打击乐器（鼓板、小锣），文场：三弦、提琴。融南北于一炉，文化皆有南北不同。北曲以遒劲为主，重弦索，建立在北方方言上（入派三声，平分阴阳），只有四个音调。词来自于马上之歌，七声音阶。宜阔口家门。南曲建立在吴方言上（平上去入，各分阴阳），重鼓板，以婉转为主。词来自于里巷歌谣，歌，五声音阶（无凡和以），适宜于生旦家门。《懒画眉》为例，拖得很长，缠绵。昆腔集大成。立昆腔为正声，规矩很多。不只是魏良辅的贡献，叶堂、徐大椿、沈宠绥等 演唱曲牌宫调腔格口法四大要素曲牌最基本有两千多个，来源历史很广。牌有定腔，句有定式，字有定声。宫调上南北有所不同北曲宫调比较完善，有十七宫调，元人赋予不同宫调不同情感色彩和表现方式，全部划分。南曲不太规范，特性和作用和其他因素也有关，比较难以捉摸，只标管色，不论宫调。腔格，以汉语的音韵系统来保证曲子的表意。方音和官话有所不同，所以要”一转“。平上去入各有不同的腔格。口法，运气发声技巧来保障歌唱的准确动听。俞振飞十六法。比较繁琐。度曲：曲乐活动的选牌联套、填词订谱、擪笛唱曲全过程。大处依腔填词，小处订谱就字。]]></content>
  </entry>
  <entry>
    <title><![CDATA[算法]]></title>
    <url>%2F2017%2F05%2F04%2F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[北大版算法读书笔记 分治策略和递归是什么二分查找和二分归并排序： 二分查找：从中位数开始判断，累加或者累减二分归并排序：规模减半的子问题 时间复杂度时间复杂度要求解递归方程：使划分均匀 MORE 两类递归方程 T(n)第一类：如hanoi。迭代、递归树、尝试 第二类：如二分检索、二分归并排序。迭代、、递归树、主定理 改进减少子问题个数：寻找子问题之间的依赖关系 减少递归里面的东西作为预处理 实例快速排序首个元素作为划分标准，从后往前找一个小的j,再从前往后找一个大的i，i&lt;j交换它俩。重复直到ij相遇 选择问题就是排序 n-1次多项式动态规划]]></content>
  </entry>
  <entry>
    <title><![CDATA[读书笔记《cpp类与数据结构》]]></title>
    <url>%2F2017%2F05%2F02%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%8Acpp%E7%B1%BB%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%8B%2F</url>
    <content type="text"><![CDATA[读书笔记《cpp类与数据结构》比大话数据结构更适合数据结构的入门 什么是类结构体和类结构体和数组最大的区别就是数据成员可以拥有不同的类型，结构体一个是可以储存信息记录，另一个就是用来为数据结构本身创造构建块。数组不能赋值给数组但是结构体的对象可以复制给同一结构体的另一个对象 类一个类生成很多个对象（私有：数据成员、公用：函数成员）封装encapsulation：对象内部的数据只能由对象本身访问类接口class interface：对象的共用部分当做程序和对象数据之间的接口类接口好处：修改量减少 类的实现implementation类包含两个文件：说明文件和实现文件 说明文件名称：“类名.h”class 类名{public：……private：……}； 实现文件名称：“类名.cpp” #inlude “类名.h”void 类名::函数名（）{}；……两点：从对象的函数返回的时，对象将保留它的数据成员的当前值类的所有函数都可以使用类定义中声明的数据成员。主程序不能使用私有数据成员。 要记得测试类为每个类编写测试驱动程序注：把函数定义放到类中结构和类的区别：结构不指明的话默认为公用，类则默认为私有 重载运算符和类模板函数的名称必须使用关键字operator，后面跟被重载运算符的符号当左操作数是结构的一个对象的时候，就会为这个对象调用该函数一般不用==测试两个结构体是否相等当左操作数不是对象，右操作数是对象时，最好将函数定义直接放在结构体定义的下方，但是与结构体中函数定义两点不同一、这个函数定义不属于对象二、不认为结构体拥有该函数（访问方式） 当左操作数为类的对象，函数原型位于类定义的公用部分；当左操作数不是而右操作数是类的对象，函数原型位于类定义的下面 类模板：生成类的蓝图template const限定符和构造函数使用const有多个方面的好处参数传递方式概括：函数中的对象需要修改的时，用传址方式，对象的改变映射到调用函数中；用传值方式则不会映射到调用函数中；函数中的对象不需要改变就用传常址构造函数通常用来初始化私有类的数据成员 指针和动态数组array类组合数据结构的方法（数组、链表）栈和队列时间复杂度链表散列表、双向链表优先级队列（树、堆）递归排序算法二叉树和图]]></content>
  </entry>
  <entry>
    <title><![CDATA[编程语言之美]]></title>
    <url>%2F2017%2F05%2F02%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E4%B9%8B%E7%BE%8E%2F</url>
    <content type="text"><![CDATA[绘制图案方法1、所见即所得的绘制方法,缺点：不灵活、不易修改,优点：直观、简单2、使用控制结构绘制图形,在知道需要输出的规格后，按行打印输出如简单对比 爱心说明：用的是Visual Studio 2015开发环境 普通爱心 带花纹的爱心图案 跳动的爱心跳动的爱心动画需要运行程序后才可以看到 打印楼梯，同时在楼梯上方打印两个笑脸 五角星 棋盘 递归出来的美丽分形世界世界上任何美丽的事物都可以用分形来展示出来 谢宾斯基三角 毕达哥拉斯树 科赫曲线1次迭代2次迭代：而我们可以改变“+”“-”所旋转角度，从而得到自由旋转的曲线还可以树杈的递归，三叉递归出一棵枝叶茂密的大树 一些非常好看分形壁纸，有关的算法就不清楚了 分形 重点来了：Tweetable Mathematical ArtTweetable Mathematical Art 具体地说，参赛者需要用 C++ 语言编写 RD 、 GR 、 BL 三个函数，每个函数都不能超过 140 个字符。每个函数都会接到 i 和 j 两个整型参数（0 ≤ i, j ≤ 1023），然后需要返回一个 0 到 255 之间的整数，表示位于 (i, j) 的像素点的颜色值。举个例子，如果 RD(0, 0) 和 GR(0, 0) 返回的都是 0 ，但 BL(0, 0) 返回的是 255 ，那么图像的最左上角那个像素就是蓝色。参赛者编写的代码会被插进下面这段程序当中（我做了一些细微的改动），最终会生成一个大小为 1024×1024 的图片 控制旋转角肥皂泡纹理 将图片转为ASCII字符画基本思想就是用不同“亮度”的字符（每个字符有自己对应的视觉亮度）替换图片中对应亮度的那些部分，最后形成和原图亮度分布差不多的“看起来很像”的字符画╭╮ ╭╮ ││ ││ ╭┴┴———————┴┴╮│ │ │ │ │ ● ● ││○ ╰┬┬┬╯ ○ ││ ╰—╯ │ ╰——┬Ｏ———Ｏ┬——╯ ╭╮ ╭╮ ╰┴————┴╯ 为什么要用java或者cpp画]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于git的几点]]></title>
    <url>%2F2017%2F04%2F26%2F%E5%85%B3%E4%BA%8Egit%E7%9A%84%E5%87%A0%E7%82%B9%2F</url>
    <content type="text"><![CDATA[关于git的几点使用fork 配置Configuring a remote for a fork 查看远程状态: git remote -v 添加一个将被同步给fork远程的上游仓库:git remote add upstreamhttps://github.com/tangxiangru/original_repository.git 再次查看状态确认是否配置成功: git remote -v 同步Syncing a fork 从上游仓库 fetch 分支和提交点，传送到本地，并会被存储在一个本地分支upstream/master: git fetch upstream 切换到本地主分支(如果不在): git checkout master 把 upstream/master分支合并到本地master上，这样就完成了同步，并且不会丢掉本地修改的内容: git merge upstream/master*如果想更新到GitHub的fork上: git push origin master Git 如何 clone 非 master 分支的代码 查看所有分支: git branch -a 查看远程分支:git branch -r 然后: git checkout dev-branch 另一种方法：在本地先建立一个分支(名称和远程的想要同步的分支名称一样），再切换到这个分支，然后git pull一下按提示来做：git branch --set-upstream-to=origin/devbranch，然后最后git pull]]></content>
  </entry>
  <entry>
    <title><![CDATA[读被伤害与侮辱的人们]]></title>
    <url>%2F2017%2F04%2F23%2F%E8%AF%BB%E8%A2%AB%E4%BC%A4%E5%AE%B3%E4%B8%8E%E4%BE%AE%E8%BE%B1%E7%9A%84%E4%BA%BA%E4%BB%AC%2F</url>
    <content type="text"><![CDATA[一九一零年秋天，在阿斯塔陂沃的小火车站，托尔斯泰临终前，打开陀斯托耶夫思基的小说《卡拉玛左夫兄弟》，翻开的一页上面分明写着：当爱再也不复存在，那可能便是地狱。 为何而读大一寒假开学第一个周末，因为偶然翻到手机中去武大图书馆拍的阅读之星发的推荐名著，就选择了这本书。没想到，寒假的一些永不会忘的经历让我对有些情节感同身受，哭到忘我。“活得不像是自己” 为何而写1849年4月23日他因牵涉反对沙皇的革命活动而被捕，并于11月16日执行死刑。在行刑之前的一刻才改判成了流放西伯利亚。1860年，陀思妥耶夫斯基在西伯利亚流放十年了之后，返回圣彼得堡，发表了第一部长篇《被侮辱与被损害的人》。这部作品可以被看作是他前后期的过渡作品，既有前期的对社会苦难人民的描写，又带有后期的宗教与哲学探讨。这段时间他文学上有所进展，但生活却连遭打击。1864年他的妻子和兄长相继逝世，他还需要照顾兄长的家人，这使得他濒临破产。他希望通过赌博来还清债务，却欠下更多债，整个人陷入消沉之中。在那流放的十年之中，我们完全相信苦难带给了他深刻的影响。恐怕正是那十年的思考奠定了他后来所走路线的基础。在挫折中他直面生活的黑暗与苦难，揭露人性中阴暗的一面，却以更多的笔触来告诉人们只有爱的光辉才是永远不灭的。相比他未来后期的作品，思想、哲学性并没有那么成熟。而且本书相对来说，思想性并不成体系，但是其中却闪动很多片段式的思想的火花。特别是他借人物之口进行的辩论，非常能启发人的思考。 人物万尼亚万尼亚对娜塔莎的爱，温暖深厚。他没有因为娜塔莎移情别恋而痛恨她。反而是同情她，帮助她，不求回报。这是善良让他心灵中充满了正面的力量。而这种爱，才是真正的爱，区别于那些得不到就要毁灭的邪恶之爱来说。 ###史密斯老人史密斯老人被女儿和她的情人弄得破了产，由富有变得穷困潦倒、孑然一身。他看上去深深地痛恨自己的女儿，可这恨中却又饱含着爱。最后在得知女儿要死的消息以后，他拖着病痛的身体在街道上狂奔，放下心结想去见她。谁知见到的却是她冰冷的尸体。他悔恨交加，昏倒在地上。在这一打击下，他性格变得更为古怪。他的内心被悔恨蚕食，痛恨自己没有早一点原谅女儿。曾经她也是他的掌上明珠，曾经他对她宠爱有加，但是爱恨情仇过后，只剩下一个父亲回到对女儿最初的爱上，为她奔跑。不论他看起来是多么古怪讨厌小气的一个老头，最后只是一个普通父亲对女儿的爱。 阿辽沙他最缺乏主观见解，最容易被人牵着鼻子走。可好死不死的，他偏偏是最被人牵着鼻子的那个：一生信任他那玩弄他于鼓掌之中的父亲，纵使直觉感受到了怀疑，也终究没有展开。更不用提他沉没在自己同时对两个女性的爱里。 娜塔莎的父亲伊赫梅涅夫这本是一个本分善良的老人。可是老实人却被“白眼狼”——他为之尊敬忠心的东家公爵陷害，蒙受了不白之冤，而且因此倾家荡产。可是他的爱女却偏偏在这节骨眼上爱上了他仇人的儿子，并与之私奔。他表面上虽然诅咒了女儿，表现出无比的恨，但骨子里还是爱她。他一边咒骂她，一边却又关心她。最后终于忍不住去找回了她，含泪痛骂自己，差点失去女儿。是涅莉用她外公史密斯和自己母亲的故事，让他清醒，使得这一段父女之情终于获得了一个美好的结局。宽容是对真爱最好的注解。 阿廖沙阿廖沙是一个本性或许不坏的人，但他缺乏主见、思想极端不成熟。所以他容易爱上比自己成熟、意志力强的女性。比如母亲般的娜塔莎，比如活泼可爱却强势的卡佳，因为她们都能带领他。 公爵公爵毫不掩饰自己的无耻，向着世界张牙舞爪，他蔑视所有的人，以他的肮脏与丑陋为荣，这是一颗彻底黑暗的，让人痛恨的心灵。然而，不能否认的是，这个世界存在着许多这样的心，但他们却伪装的很好，很少能让人看到这么肆无忌惮的表露，给人带来非常大的冲击。如此一颗丑陋的灵魂令人震撼。 感受读完整体感受这本书里动辄长达两三页的独白或者心理描写让我享受。感叹居然有这么伟大的作者，能够写出这么细致入微的内心世界。书中人物那些激烈的个性、疯狂的行为、暴躁的情绪，还有书中所弥漫的无尽的悲怆、无边的苦痛、无助的愤怒、无底的绝望，以及隐藏在书背后作者宁静的注视、巨大的悲悯。在这本书里没有峰回路转也没有皆大欢喜，从头到尾都只能看到善良的被欺压、纯洁的被侮辱、尊严的被践踏，恶者愈恶而无所制约，弱者愈弱而无可反抗。 悲悯之心我还认为，一个伟大的作家要写出真正打动人心的作品，必然要有一副对人类的悲悯之心。陀思妥耶夫斯基正是拥有这一点。 ###关于爱情诚然，爱情都是自私的，可是，成年人的爱，总要不断地调整自己，需要为对方设身处地着想，甚至为对方的幸福做出适当的牺牲。只有儿童的感情，才会完全只考虑自己的喜恶，高兴的时候，便不顾一切对你好，不高兴的时候，便使出种种招数来招惹你。如果孩子气保持到成年，当爱上某人的时候，就容易变成一种“唯我之爱”。不能说一个人不爱、或者不够爱另一个人，但他这样的爱情是不成熟的，是纯粹从自身需要出发的爱情。这样的爱情往往不顾一切，炽烈的时候让人感动，痛苦的时候让人心碎。 倘若爱上这样一个人，则在爱情中感受到的快乐与痛苦都会加倍。所以更需要有足够的耐心去理解和宽容对方，需要能忍受对方不时迸发出的孩子气。相应地，可以从对方那里得到的，则是一份充满赤子之心的感情。这种感情，是无法从历经沧桑的人身上得到的，但是往往，只凭这一份感情，便可以让一个人，明明知道对方是怀着可以用自私来形容的“唯我之爱”，仍然不由自主地泥足深陷、无法自拔。（娜达莎就是这样不由自主地抛弃青梅竹马的朋友瓦尼亚而喜欢上阿辽沙的） 相比之下，《被伤害与侮辱的人们》中的瓦尼亚，虽然并不是这样一种唯我之爱，但总让人觉得缺乏真实感。明明是自己心爱的对象喜欢上了别人，自己却丝毫没有显出难过的样子，反而极力为对方的幸福奔走忙碌。是瓦尼亚傻，是他不够爱娜达莎，还是实在爱她太深了呢？我想，答案应该是最后一个。瓦尼亚并非不伤心难过，而是他知道，娜达莎已经背负着太多的压力了，他一点点也不愿意，让自己的痛苦与伤心为对方增添哪怕一丝的负担。即使在最痛苦的时候，他依然全心全意为娜达莎着想，充当娜达莎与父母、与阿辽沙之间沟通的桥梁，竭尽全力解决他们之间的问题，为了娜达莎的幸福，付出自己的一切。在娜达莎幸福快乐的时候，他小心翼翼地不去打扰，而在她遇到问题，在窗台上点起那支召唤的蜡烛时，瓦尼亚总是在第一时间及时出现，为她排忧解难，安慰她、照顾她。即使在她心情糟糕、冷眼相向的时候，也一如既往，毫不介怀。我想，爱一个人，只有爱到极致，爱到骨髓，才能爱到如此忘我。 以娜达莎的聪明与善良，她从一开始就知道瓦尼亚的心情。我想，对瓦尼亚来说，自己的心意不用说就可以传达到对方心中，其实已经是幸福到极致的事了，单是娜达莎的这份体贴与温柔，就足以，让他付出100倍的努力来回报。娜达莎这样的姑娘，的确是值得让人这样去爱的。而事实上，瓦尼亚也并非没有得到回报，虽然娜达莎无法给予他最珍贵的爱情，可是，在爱情之外，她把最亲密的友谊与信赖给了瓦尼亚。而瓦尼亚也从这样的友谊与信赖中，得到了快乐与幸福。 因为，对怀着这样一种爱的瓦尼亚来说，娜达莎的一个微笑，一次回眸，都是额外的恩赐，更何况，她给予他的，要远比一个微笑多得多。所以直到最后，穷困、潦倒，病体弥留的瓦尼亚，在床上写下这段故事的时候，他依然幸福的觉得，在故事结束时，娜达莎的眼神是在告诉他：“我们本来是可以幸福地共度一生的”。原来，爱一个人，真的可以爱到如此忘我，如此极致。 爱到”无私忘我“的境界是非常难的。我敬佩这样的爱情，但可能我永远做不到。我感到很抱歉，我大一寒假就是因为这个原因从而break up。所以正是这个愿意，看这本书的时候我哭了很久一会。 回味“卑鄙是卑鄙者的通行证，高尚是高尚者的墓志铭” 但是我却不痛恨公爵，这只是人性，他们那种人都是没有道德底线的，为了利益无所不为。而善良的人们带着道德的枷锁，活在自己的世界中，不去抗争，妄图用心灵上的优势来进行道德上的谴责，但这样是毫无意义的。 —-那么我们究竟该如何存在？]]></content>
  </entry>
  <entry>
    <title><![CDATA[cmu_bomb_lab]]></title>
    <url>%2F2017%2F04%2F23%2Fcmu-bomb-lab-1%2F</url>
    <content type="text"><![CDATA[题目Writeup/Handout.tar Ubuntu16.04 amd64GDB：GNU Project debugger, Ubuntu下sudo apt-get install gdb安装，用来调试objdump: 用来做反编译器 常用gdb命令i rx/sx/anii register （终端计算器bc） 游戏开始前设断点b explode_bombb phase_1b phase_2b phase_3b phase_4b phase_5b phase_6 逐个解析phase_1:发现有两行都是callq ：分别作用是调用 strings_not_equal 和 explode_bomb 这两个函数而这里 %esi 对应的是第二个参数，第一个参数呢？当然就是我们拆弹时需要输入的字符串了。之后的 test 是用来判断函数的返回值 %eax 是否为 0， 如果为 0 则进行跳转，否则炸弹爆炸，所以我们实际上要做的，就是看看 $0x4027f0 这个地址里对应存放的是什么字符串 x/s 0x402400得到答案Border relations with Canada have never been better 把答案写入answer.txtset args answer.txt phase_2:石泽远的解析 add指针移四个字节 最重要的add %eax,%eax所以是乘以2 从cmpl $0x1, (%rsp) 看出第一个数字一定是 1，然后跳转到 +24 的位置，然后把 1 移动到 %ebx 中，跳转到 +57 的位置，然后和 5 进行比较，因为 1 比 5 小，所以会跳转到 +31 的位置。 接着是 movslq 语句，这个语句是带符号地把第一个寄存器扩展并复制到第二个寄存器中，所以现在 %rdx 的值也是 1。lea 之后 %eax 等于 0，然后用 cltq 扩展到 64 位（也就是 %rax 等于 0），接着的语句相当于是 %eax = (%rsp) + 4 * %rax 即 %eax 等于 1。然后与自己相加等于乘以 2，现在 %eax 等于 2，然后等于是判断第二个参数((%rsp, %rdx, 4))和 2 是否相等，所以第二个数字是 2。 然后进行循环的累加并返回到 +31 的位置，继续循环。接着就是类似的操作了，最后分析可以得到每次增大一倍，答案就是 1 2 4 8 16 32。 phase_3：考查跳转指令控制语句switch $0x4025cf输入的东西存的地方从第一个 jmpq 开始，发现是switch语句，case0到case7，一一找出来就好了。 八个答案，输入时记得转化为十进制可能的答案为： 0 2071 3112 7073 2564 3895 2066 6827 327 phase_4比较2和输入的个数：所以是两个输入用到逻辑右移和算术右移由它cmpl number1和0的大小，然后相等就je，所以第二个数字 number1 = 0。而第一个数字 number0，要满足 func4(number0, 0, 14) 的返回值为0，来看func4 func4中有个if和else if，而且还callq 400fce，递归。 注意：如果第一个参数number0取得是7，即取 number0 = 7，使 func4(number0, 0, 14) 返回0，就直接跳过那两个大于7或者小于7的判断递归直接到结果 事实上func4函数可以不看，直接设断点枚举输入找需要的输出 phase_5：要常备ASCII码表phase_5中if else中else里面有个for循环mov %fs:0x28,%rax保护模式 xor %eax,%eax抑或 movzbl (%rbx,%rax,1),%ecx 0扩展两次初始化操作后最后是要比较两个字符串，有一个在0x40245e中即是f：”flyers”，最后答案就是相与之后变成ionefg 从”maduiersnfotvbyl”中得到”flyers”，可以取下标9 15 14 5 6 7，对应十六进制分别为 0x9 0xf 0xe 0x5 0x6 0x7，该输入由input字符串中每个字符的低8位给出。使用man ascii 查看 ASCII 表，对照后可知可以输入 ionefg phase_6汇编代码巨长压栈，读取6个数字到 int numbers[6]第一个循环读取的第一个数&lt;= 5，就goto L1 输一个序列后按照用7减过的数字再另外开辟一块空间存进去，在此新的空间比较&gt; =63004492每个加4然后print出来如：print *6304500用数组存的，指针只是优化，最后指针便为0 根据输入的数字逆序重排的链表value值但带递增（要求链表结点的value值严格单调递增），而原来结点的值按顺序依次为0x14c 0xa8 0x39c 0x2b3 0x1dd 0x1bb 结构体如下 12345struct Node &#123;int anInt;int seven_minus_input;Node *next;&#125; 0x14c 0xa8 0x39c 0x2b3 0x1dd 0x1bb正确顺序应为3 4 5 6 1 2由此推断4 3 2 1 6 5 secret_phase：secret_phase是存在的，objdump -t一下能看见这个符号发现它出现在phase_defused函数里面read_line和bomb_defused这两个函数吧 查看0x402619处字符串为”%d %d %s” 返回值不是0的时候就跳到401635set args s cmp $0x3e8,%eax 比较这两个，所以输入的数字要小于318：即是1000 print *0x6030f0得到36地址6030f0是个二叉树看fun7函数递归的反编译：test %rdi,%rdi %rdi地址是null 注意：64位程序前六个参数原来是存在寄存器里 比较edx就是我们输入的数和edx即36比较如果大于36就到28行，小于等于就返回值乘以二，第一个返回fun7rdi的后八位放到rdi里面（为什么是八位？因为64操作系统里指针是八位的）大于等于就到57第二个返回fun7：lea 0x1(%rax,%rax,1),%eax乘以二再加一 不断print 6303984 依次加8判断条件可以看出这里的每个结点有2个子结点点，子结点的值分别存放在当前结点地址后8位和后16位的位置,看刘聪学长博客即是二叉树 来看看什么是csapp csapp如何阅读-知乎 csapp介绍-知乎 超详细bomb lab解析 提前写好：b explode_bombb pharse_1b pharse_2b pharse_3b pharse_4b pharse_5b pharse_6 感谢前人经验bomb lab文档 Quick Intro to gdb逆向工程——二进制炸弹(CSAPP Project) 作为最先做完前五题的石泽远的bomb-lab解析 gdb写脚本 gdb详细用法 gdb视频介绍 bomb lab官方文档 gdb快速指南 小土刀介绍csapp第三章 小土刀介绍bomb lab，但是我们做的不是小土刀版本简析phase1-5寄存器指令语句、操作数和寻址如何使用gdb]]></content>
  </entry>
  <entry>
    <title><![CDATA[更舒服的开发环境]]></title>
    <url>%2F2017%2F03%2F24%2F%E7%BB%88%E7%AB%AF%E7%BF%BB%E5%A2%99%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[怎么终端翻墙 Step1 安装 Command Line Tools配置开发环境的第一步是安装系统软件包管理器 Homebrew，但是首先 Homebrew 需要依赖 Command Line Tools。这个软件包为Terminal命令行环境安装了进行 Unix 风格开发所必需的工具，同时包含 OS X SDK 及相关的头文件.如果你装了 Xcode，那么Command Line Tools 已经集成在了 Xcode 的软件包里（可能需要在偏好设置里确认安装，如果你不需要做 iOS 开发的话，Xcode的体积过大，所以可以下载单独的Command Line Tools安装包 Step2 安装 HomebrewHomebrew 是安装一系列软件包的工具，我们应该尽可能避免下载 -&gt; 安装这样的工作，而是用命令行完成软件安装和环境配置工作。Homebrew 的安装过程很简单，只需要在终端执行如下命令： 1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 这条命令需要兼容 bash 的 shell 环境但最终我是用 iTerm2 取代默认的终端bash Step3 完善科学上网先安装 ShadowSocks客户端解决操作系统层面下的上网问题，但是 Shadowsocks 的代理无法直接设置给终端使用。没关系，我们有PolipoHomebrew 可以直接安装 Polipo，但是需要配置 Shadowsocks。首先 Polipo 作为终端下的服务是需要启动／停止的，这里有两个选择：一）手动；二）开机自动。 for手动使用 Homebrew Services 来简化操作，执行 brew tap homebrew/services 之后 brew services start|stop|restart SERVICE_NAME 来操作一切终端服务 for开机自启动brew info polipo ，唯一的问题是需要 Polipo 在启动的时候要设定 socksParentProxy 选项。网上很多教程都是让去修改 /usr/local/opt/polipo/homebrew.mxcl.polipo.plist 文件。推荐另一种，在 ~/.polipo 文件里加一句 socksParentProxy = “localhost:1080”就好了。 最后在 ~/.bash_profile 里加两句： 12export http_proxy=localhost:8123export ALL_PROXY=$http_proxy]]></content>
  </entry>
</search>
